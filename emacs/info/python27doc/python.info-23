This is python.info, produced by makeinfo version 4.8 from python.texi.

Generated by Sphinx 1.6.3.
INFO-DIR-SECTION Python
START-INFO-DIR-ENTRY
* Python: (python.info). The Python reference manual.
END-INFO-DIR-ENTRY

     Python 2.7.13, July 15, 2017

     Copyright (C) 1990-2017, Python Software Foundation


File: python.info,  Node: Porting Extension Modules to Python 3,  Next: Curses Programming with Python,  Prev: Porting Python 2 Code to Python 3,  Up: Python HOWTOs

10.2 Porting Extension Modules to Python 3
==========================================

author: Benjamin Peterson

Abstract
........

Although changing the C-API was not one of Python 3’s objectives, the
many Python-level changes made leaving Python 2’s API intact
impossible.  In fact, some changes such as *Note int(): 1f2. and *Note
long(): 1f3. unification are more obvious on the C level.  This
document endeavors to document incompatibilities and how they can be
worked around.

* Menu:

* Conditional compilation::
* Changes to Object APIs::
* Module initialization and state::
* CObject replaced with Capsule::
* Other options::


File: python.info,  Node: Conditional compilation,  Next: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.1 Conditional compilation
------------------------------

The easiest way to compile only some code for Python 3 is to check if
`PY_MAJOR_VERSION' is greater than or equal to 3.

    #if PY_MAJOR_VERSION >= 3
    #define IS_PY3K
    #endif

API functions that are not present can be aliased to their equivalents
within conditional blocks.


File: python.info,  Node: Changes to Object APIs,  Next: Module initialization and state,  Prev: Conditional compilation,  Up: Porting Extension Modules to Python 3

10.2.2 Changes to Object APIs
-----------------------------

Python 3 merged together some types with similar functions while cleanly
separating others.

* Menu:

* str/unicode Unification::
* long/int Unification::


File: python.info,  Node: str/unicode Unification,  Next: long/int Unification,  Up: Changes to Object APIs

10.2.2.1 str/unicode Unification
................................

Python 3’s *Note str(): 1ea. type is equivalent to Python 2’s *Note
unicode(): 1f5.; the C functions are called `PyUnicode_*' for both.
The old 8-bit string type has become `bytes()', with C functions called
`PyBytes_*'.  Python 2.6 and later provide a compatibility header,
`bytesobject.h', mapping `PyBytes' names to `PyString' ones.  For best
compatibility with Python 3, `PyUnicode' should be used for textual
data and `PyBytes' for binary data.  It’s also important to remember
that `PyBytes' and `PyUnicode' in Python 3 are not interchangeable like
`PyString' and `PyUnicode' are in Python 2.  The following example
shows best practices with regards to `PyUnicode', `PyString', and
`PyBytes'.

    #include "stdlib.h"
    #include "Python.h"
    #include "bytesobject.h"

    /* text example */
    static PyObject *
    say_hello(PyObject *self, PyObject *args) {
        PyObject *name, *result;

        if (!PyArg_ParseTuple(args, "U:say_hello", &name))
            return NULL;

        result = PyUnicode_FromFormat("Hello, %S!", name);
        return result;
    }

    /* just a forward */
    static char * do_encode(PyObject *);

    /* bytes example */
    static PyObject *
    encode_object(PyObject *self, PyObject *args) {
        char *encoded;
        PyObject *result, *myobj;

        if (!PyArg_ParseTuple(args, "O:encode_object", &myobj))
            return NULL;

        encoded = do_encode(myobj);
        if (encoded == NULL)
            return NULL;
        result = PyBytes_FromString(encoded);
        free(encoded);
        return result;
    }


File: python.info,  Node: long/int Unification,  Prev: str/unicode Unification,  Up: Changes to Object APIs

10.2.2.2 long/int Unification
.............................

Python 3 has only one integer type, *Note int(): 1f2.  But it actually
corresponds to Python 2’s *Note long(): 1f3. type—the *Note int():
1f2. type used in Python 2 was removed.  In the C-API, `PyInt_*'
functions are replaced by their `PyLong_*' equivalents.


File: python.info,  Node: Module initialization and state,  Next: CObject replaced with Capsule,  Prev: Changes to Object APIs,  Up: Porting Extension Modules to Python 3

10.2.3 Module initialization and state
--------------------------------------

Python 3 has a revamped extension module initialization system.  (See PEP
3121(1).)  Instead of storing module state in globals, they should be
stored in an interpreter specific structure.  Creating modules that act
correctly in both Python 2 and Python 3 is tricky.  The following
simple example demonstrates how.

    #include "Python.h"

    struct module_state {
        PyObject *error;
    };

    #if PY_MAJOR_VERSION >= 3
    #define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))
    #else
    #define GETSTATE(m) (&_state)
    static struct module_state _state;
    #endif

    static PyObject *
    error_out(PyObject *m) {
        struct module_state *st = GETSTATE(m);
        PyErr_SetString(st->error, "something bad happened");
        return NULL;
    }

    static PyMethodDef myextension_methods[] = {
        {"error_out", (PyCFunction)error_out, METH_NOARGS, NULL},
        {NULL, NULL}
    };

    #if PY_MAJOR_VERSION >= 3

    static int myextension_traverse(PyObject *m, visitproc visit, void *arg) {
        Py_VISIT(GETSTATE(m)->error);
        return 0;
    }

    static int myextension_clear(PyObject *m) {
        Py_CLEAR(GETSTATE(m)->error);
        return 0;
    }


    static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "myextension",
            NULL,
            sizeof(struct module_state),
            myextension_methods,
            NULL,
            myextension_traverse,
            myextension_clear,
            NULL
    };

    #define INITERROR return NULL

    PyMODINIT_FUNC
    PyInit_myextension(void)

    #else
    #define INITERROR return

    void
    initmyextension(void)
    #endif
    {
    #if PY_MAJOR_VERSION >= 3
        PyObject *module = PyModule_Create(&moduledef);
    #else
        PyObject *module = Py_InitModule("myextension", myextension_methods);
    #endif

        if (module == NULL)
            INITERROR;
        struct module_state *st = GETSTATE(module);

        st->error = PyErr_NewException("myextension.Error", NULL, NULL);
        if (st->error == NULL) {
            Py_DECREF(module);
            INITERROR;
        }

    #if PY_MAJOR_VERSION >= 3
        return module;
    #endif
    }

---------- Footnotes ----------

(1) https://www.python.org/dev/peps/pep-3121


File: python.info,  Node: CObject replaced with Capsule,  Next: Other options,  Prev: Module initialization and state,  Up: Porting Extension Modules to Python 3

10.2.4 CObject replaced with Capsule
------------------------------------

The `Capsule' object was introduced in Python 3.1 and 2.7 to replace
`CObject'.  CObjects were useful, but the `CObject' API was
problematic: it didn’t permit distinguishing between valid CObjects,
which allowed mismatched CObjects to crash the interpreter, and some of
its APIs relied on undefined behavior in C.  (For further reading on
the rationale behind Capsules, please see issue 5630(1).)

If you’re currently using CObjects, and you want to migrate to 3.1 or
newer, you’ll need to switch to Capsules.  `CObject' was deprecated
in 3.1 and 2.7 and completely removed in Python 3.2.  If you only
support 2.7, or 3.1 and above, you can simply switch to `Capsule'.  If
you need to support Python 3.0, or versions of Python earlier than 2.7,
you’ll have to support both CObjects and Capsules.  (Note that Python
3.0 is no longer supported, and it is not recommended for production
use.)

The following example header file `capsulethunk.h' may solve the
problem for you.  Simply write your code against the `Capsule' API and
include this header file after `Python.h'.  Your code will
automatically use Capsules in versions of Python with Capsules, and
switch to CObjects when Capsules are unavailable.

`capsulethunk.h' simulates Capsules using CObjects.  However, `CObject'
provides no place to store the capsule’s “name”.  As a result the
simulated `Capsule' objects created by `capsulethunk.h' behave slightly
differently from real Capsules.  Specifically:

        * The name parameter passed in to *Note PyCapsule_New(): 2a98.
          is ignored.

        * The name parameter passed in to *Note PyCapsule_IsValid():
          2cb. and *Note PyCapsule_GetPointer(): 2e38. is ignored, and
          no error checking of the name is performed.

        * *Note PyCapsule_GetName(): 2e3b. always returns NULL.

        * *Note PyCapsule_SetName(): 2e3e. always raises an exception
          and returns failure.  (Since there’s no way to store a name
          in a CObject, noisy failure of *Note PyCapsule_SetName():
          2e3e.  was deemed preferable to silent failure here.  If this
          is inconvenient, feel free to modify your local copy as you
          see fit.)

You can find `capsulethunk.h' in the Python source distribution as
Doc/includes/capsulethunk.h(2).  We also include it here for your
convenience:

    #ifndef __CAPSULETHUNK_H
    #define __CAPSULETHUNK_H

    #if (    (PY_VERSION_HEX <  0x02070000) \
         || ((PY_VERSION_HEX >= 0x03000000) \
          && (PY_VERSION_HEX <  0x03010000)) )

    #define __PyCapsule_GetField(capsule, field, default_value) \
        ( PyCapsule_CheckExact(capsule) \
            ? (((PyCObject *)capsule)->field) \
            : (default_value) \
        ) \

    #define __PyCapsule_SetField(capsule, field, value) \
        ( PyCapsule_CheckExact(capsule) \
            ? (((PyCObject *)capsule)->field = value), 1 \
            : 0 \
        ) \


    #define PyCapsule_Type PyCObject_Type

    #define PyCapsule_CheckExact(capsule) (PyCObject_Check(capsule))
    #define PyCapsule_IsValid(capsule, name) (PyCObject_Check(capsule))


    #define PyCapsule_New(pointer, name, destructor) \
        (PyCObject_FromVoidPtr(pointer, destructor))


    #define PyCapsule_GetPointer(capsule, name) \
        (PyCObject_AsVoidPtr(capsule))

    /* Don't call PyCObject_SetPointer here, it fails if there's a destructor */
    #define PyCapsule_SetPointer(capsule, pointer) \
        __PyCapsule_SetField(capsule, cobject, pointer)


    #define PyCapsule_GetDestructor(capsule) \
        __PyCapsule_GetField(capsule, destructor)

    #define PyCapsule_SetDestructor(capsule, dtor) \
        __PyCapsule_SetField(capsule, destructor, dtor)


    /*
     * Sorry, there's simply no place
     * to store a Capsule "name" in a CObject.
     */
    #define PyCapsule_GetName(capsule) NULL

    static int
    PyCapsule_SetName(PyObject *capsule, const char *unused)
    {
        unused = unused;
        PyErr_SetString(PyExc_NotImplementedError,
            "can't use PyCapsule_SetName with CObjects");
        return 1;
    }



    #define PyCapsule_GetContext(capsule) \
        __PyCapsule_GetField(capsule, descr)

    #define PyCapsule_SetContext(capsule, context) \
        __PyCapsule_SetField(capsule, descr, context)


    static void *
    PyCapsule_Import(const char *name, int no_block)
    {
        PyObject *object = NULL;
        void *return_value = NULL;
        char *trace;
        size_t name_length = (strlen(name) + 1) * sizeof(char);
        char *name_dup = (char *)PyMem_MALLOC(name_length);

        if (!name_dup) {
            return NULL;
        }

        memcpy(name_dup, name, name_length);

        trace = name_dup;
        while (trace) {
            char *dot = strchr(trace, '.');
            if (dot) {
                *dot++ = '\0';
            }

            if (object == NULL) {
                if (no_block) {
                    object = PyImport_ImportModuleNoBlock(trace);
                } else {
                    object = PyImport_ImportModule(trace);
                    if (!object) {
                        PyErr_Format(PyExc_ImportError,
                            "PyCapsule_Import could not "
                            "import module \"%s\"", trace);
                    }
                }
            } else {
                PyObject *object2 = PyObject_GetAttrString(object, trace);
                Py_DECREF(object);
                object = object2;
            }
            if (!object) {
                goto EXIT;
            }

            trace = dot;
        }

        if (PyCObject_Check(object)) {
            PyCObject *cobject = (PyCObject *)object;
            return_value = cobject->cobject;
        } else {
            PyErr_Format(PyExc_AttributeError,
                "PyCapsule_Import \"%s\" is not valid",
                name);
        }

    EXIT:
        Py_XDECREF(object);
        if (name_dup) {
            PyMem_FREE(name_dup);
        }
        return return_value;
    }

    #endif /* #if PY_VERSION_HEX < 0x02070000 */

    #endif /* __CAPSULETHUNK_H */

---------- Footnotes ----------

(1) https://bugs.python.org/issue5630

(2) https://hg.python.org/cpython/file/2.7/Doc/includes/capsulethunk.h


File: python.info,  Node: Other options,  Prev: CObject replaced with Capsule,  Up: Porting Extension Modules to Python 3

10.2.5 Other options
--------------------

If you are writing a new extension module, you might consider
Cython(1).  It translates a Python-like language to C.  The extension
modules it creates are compatible with Python 3 and Python 2.

---------- Footnotes ----------

(1) http://cython.org/


File: python.info,  Node: Curses Programming with Python,  Next: Descriptor HowTo Guide,  Prev: Porting Extension Modules to Python 3,  Up: Python HOWTOs

10.3 Curses Programming with Python
===================================

Author: A.M. Kuchling, Eric S. Raymond

Release: 2.03

Abstract
........

This document describes how to write text-mode programs with Python
2.x, using the *Note curses: 79. extension module to control the
display.

* Menu:

* What is curses?::
* Starting and ending a curses application::
* Windows and Pads::
* Displaying Text::
* User Input::
* For More Information::


File: python.info,  Node: What is curses?,  Next: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.1 What is curses?
----------------------

The curses library supplies a terminal-independent screen-painting and
keyboard-handling facility for text-based terminals; such terminals
include VT100s, the Linux console, and the simulated terminal provided
by X11 programs such as xterm and rxvt.  Display terminals support
various control codes to perform common operations such as moving the
cursor, scrolling the screen, and erasing areas.  Different terminals
use widely differing codes, and often have their own minor quirks.

In a world of X displays, one might ask “why bother”?  It’s true
that character-cell display terminals are an obsolete technology, but
there are niches in which being able to do fancy things with them are
still valuable.  One is on small-footprint or embedded Unixes that
don’t carry an X server.  Another is for tools like OS installers and
kernel configurators that may have to run before X is available.

The curses library hides all the details of different terminals, and
provides the programmer with an abstraction of a display, containing
multiple non-overlapping windows.  The contents of a window can be
changed in various ways—adding text, erasing it, changing its
appearance—and the curses library will automagically figure out what
control codes need to be sent to the terminal to produce the right
output.

The curses library was originally written for BSD Unix; the later
System V versions of Unix from AT&T added many enhancements and new
functions. BSD curses is no longer maintained, having been replaced by
ncurses, which is an open-source implementation of the AT&T interface.
If you’re using an open-source Unix such as Linux or FreeBSD, your
system almost certainly uses ncurses.  Since most current commercial
Unix versions are based on System V code, all the functions described
here will probably be available.  The older versions of curses carried
by some proprietary Unixes may not support everything, though.

No one has made a Windows port of the curses module.  On a Windows
platform, try the Console module written by Fredrik Lundh.  The Console
module provides cursor-addressable text output, plus full support for
mouse and keyboard input, and is available from
<http://effbot.org/zone/console-index.htm>.

* Menu:

* The Python curses module::


File: python.info,  Node: The Python curses module,  Up: What is curses?

10.3.1.1 The Python curses module
.................................

Thy Python module is a fairly simple wrapper over the C functions
provided by curses; if you’re already familiar with curses
programming in C, it’s really easy to transfer that knowledge to
Python.  The biggest difference is that the Python interface makes
things simpler, by merging different C functions such as `addstr()',
`mvaddstr()', `mvwaddstr()', into a single `addstr()' method.  You’ll
see this covered in more detail later.

This HOWTO is simply an introduction to writing text-mode programs with
curses and Python. It doesn’t attempt to be a complete guide to the
curses API; for that, see the Python library guide’s section on
ncurses, and the C manual pages for ncurses.  It will, however, give
you the basic ideas.


File: python.info,  Node: Starting and ending a curses application,  Next: Windows and Pads,  Prev: What is curses?,  Up: Curses Programming with Python

10.3.2 Starting and ending a curses application
-----------------------------------------------

Before doing anything, curses must be initialized.  This is done by
calling the `initscr()' function, which will determine the terminal
type, send any required setup codes to the terminal, and create various
internal data structures.  If successful, `initscr()' returns a window
object representing the entire screen; this is usually called `stdscr',
after the name of the corresponding C variable.

    import curses
    stdscr = curses.initscr()

Usually curses applications turn off automatic echoing of keys to the
screen, in order to be able to read keys and only display them under
certain circumstances.  This requires calling the `noecho()' function.

    curses.noecho()

Applications will also commonly need to react to keys instantly, without
requiring the Enter key to be pressed; this is called cbreak mode, as
opposed to the usual buffered input mode.

    curses.cbreak()

Terminals usually return special keys, such as the cursor keys or
navigation keys such as Page Up and Home, as a multibyte escape
sequence.  While you could write your application to expect such
sequences and process them accordingly, curses can do it for you,
returning a special value such as `curses.KEY_LEFT'.  To get curses to
do the job, you’ll have to enable keypad mode.

    stdscr.keypad(1)

Terminating a curses application is much easier than starting one.
You’ll need to call

    curses.nocbreak(); stdscr.keypad(0); curses.echo()

to reverse the curses-friendly terminal settings. Then call the
`endwin()' function to restore the terminal to its original operating
mode.

    curses.endwin()

A common problem when debugging a curses application is to get your
terminal messed up when the application dies without restoring the
terminal to its previous state.  In Python this commonly happens when
your code is buggy and raises an uncaught exception.  Keys are no
longer echoed to the screen when you type them, for example, which
makes using the shell difficult.

In Python you can avoid these complications and make debugging much
easier by importing the *Note curses.wrapper(): 140a. function.  It
takes a callable and does the initializations described above, also
initializing colors if color support is present.  It then runs your
provided callable and finally deinitializes appropriately.  The
callable is called inside a try-catch clause which catches exceptions,
performs curses deinitialization, and then passes the exception
upwards.  Thus, your terminal won’t be left in a funny state on
exception.


File: python.info,  Node: Windows and Pads,  Next: Displaying Text,  Prev: Starting and ending a curses application,  Up: Curses Programming with Python

10.3.3 Windows and Pads
-----------------------

Windows are the basic abstraction in curses.  A window object
represents a rectangular area of the screen, and supports various
methods to display text, erase it, allow the user to input strings, and
so forth.

The `stdscr' object returned by the `initscr()' function is a window
object that covers the entire screen.  Many programs may need only this
single window, but you might wish to divide the screen into smaller
windows, in order to redraw or clear them separately. The `newwin()'
function creates a new window of a given size, returning the new window
object.

    begin_x = 20; begin_y = 7
    height = 5; width = 40
    win = curses.newwin(height, width, begin_y, begin_x)

A word about the coordinate system used in curses: coordinates are
always passed in the order `y,x', and the top-left corner of a window
is coordinate (0,0).  This breaks a common convention for handling
coordinates, where the `x' coordinate usually comes first.  This is an
unfortunate difference from most other computer applications, but
it’s been part of curses since it was first written, and it’s too
late to change things now.

When you call a method to display or erase text, the effect doesn’t
immediately show up on the display.  This is because curses was
originally written with slow 300-baud terminal connections in mind;
with these terminals, minimizing the time required to redraw the screen
is very important.  This lets curses accumulate changes to the screen,
and display them in the most efficient manner.  For example, if your
program displays some characters in a window, and then clears the
window, there’s no need to send the original characters because
they’d never be visible.

Accordingly, curses requires that you explicitly tell it to redraw
windows, using the `refresh()' method of window objects.  In practice,
this doesn’t really complicate programming with curses much. Most
programs go into a flurry of activity, and then pause waiting for a
keypress or some other action on the part of the user.  All you have to
do is to be sure that the screen has been redrawn before pausing to
wait for user input, by simply calling `stdscr.refresh()' or the
`refresh()' method of some other relevant window.

A pad is a special case of a window; it can be larger than the actual
display screen, and only a portion of it displayed at a time. Creating
a pad simply requires the pad’s height and width, while refreshing a
pad requires giving the coordinates of the on-screen area where a
subsection of the pad will be displayed.

    pad = curses.newpad(100, 100)
    #  These loops fill the pad with letters; this is
    # explained in the next section
    for y in range(0, 100):
        for x in range(0, 100):
            try:
                pad.addch(y,x, ord('a') + (x*x+y*y) % 26)
            except curses.error:
                pass

    #  Displays a section of the pad in the middle of the screen
    pad.refresh(0,0, 5,5, 20,75)

The `refresh()' call displays a section of the pad in the rectangle
extending from coordinate (5,5) to coordinate (20,75) on the screen;
the upper left corner of the displayed section is coordinate (0,0) on
the pad.  Beyond that difference, pads are exactly like ordinary
windows and support the same methods.

If you have multiple windows and pads on screen there is a more
efficient way to go, which will prevent annoying screen flicker at
refresh time.  Use the `noutrefresh()' method of each window to update
the data structure representing the desired state of the screen; then
change the physical screen to match the desired state in one go with
the function `doupdate()'.  The normal `refresh()' method calls
`doupdate()' as its last act.


File: python.info,  Node: Displaying Text,  Next: User Input,  Prev: Windows and Pads,  Up: Curses Programming with Python

10.3.4 Displaying Text
----------------------

From a C programmer’s point of view, curses may sometimes look like a
twisty maze of functions, all subtly different.  For example,
`addstr()' displays a string at the current cursor location in the
`stdscr' window, while `mvaddstr()' moves to a given y,x coordinate
first before displaying the string. `waddstr()' is just like
`addstr()', but allows specifying a window to use, instead of using
`stdscr' by default. `mvwaddstr()' follows similarly.

Fortunately the Python interface hides all these details; `stdscr' is a
window object like any other, and methods like `addstr()' accept
multiple argument forms.  Usually there are four different forms.

Form                                  Description
------------------------------------------------------------------------------------------ 
`str' or `ch'                         Display the string `str' or character `ch' at the
                                      current position
`str' or `ch', `attr'                 Display the string `str' or character `ch', using
                                      attribute `attr' at the current position
`y', `x', `str' or `ch'               Move to position `y,x' within the window, and
                                      display `str' or `ch'
`y', `x', `str' or `ch', `attr'       Move to position `y,x' within the window, and
                                      display `str' or `ch', using attribute `attr'

Attributes allow displaying text in highlighted forms, such as in
boldface, underline, reverse code, or in color.  They’ll be explained
in more detail in the next subsection.

The `addstr()' function takes a Python string as the value to be
displayed, while the `addch()' functions take a character, which can be
either a Python string of length 1 or an integer.  If it’s a string,
you’re limited to displaying characters between 0 and 255.  SVr4
curses provides constants for extension characters; these constants are
integers greater than 255.  For example, `ACS_PLMINUS' is a +/- symbol,
and `ACS_ULCORNER' is the upper left corner of a box (handy for drawing
borders).

Windows remember where the cursor was left after the last operation, so
if you leave out the `y,x' coordinates, the string or character will be
displayed wherever the last operation left off.  You can also move the
cursor with the `move(y,x)' method.  Because some terminals always
display a flashing cursor, you may want to ensure that the cursor is
positioned in some location where it won’t be distracting; it can be
confusing to have the cursor blinking at some apparently random
location.

If your application doesn’t need a blinking cursor at all, you can
call `curs_set(0)' to make it invisible.  Equivalently, and for
compatibility with older curses versions, there’s a `leaveok(bool)'
function.  When `bool' is true, the curses library will attempt to
suppress the flashing cursor, and you won’t need to worry about
leaving it in odd locations.

* Menu:

* Attributes and Color::


File: python.info,  Node: Attributes and Color,  Up: Displaying Text

10.3.4.1 Attributes and Color
.............................

Characters can be displayed in different ways.  Status lines in a
text-based application are commonly shown in reverse video; a text
viewer may need to highlight certain words.  curses supports this by
allowing you to specify an attribute for each cell on the screen.

An attribute is an integer, each bit representing a different
attribute.  You can try to display text with multiple attribute bits
set, but curses doesn’t guarantee that all the possible combinations
are available, or that they’re all visually distinct.  That depends
on the ability of the terminal being used, so it’s safest to stick to
the most commonly available attributes, listed here.

Attribute                  Description
---------------------------------------------------------------------- 
`A_BLINK'                  Blinking text
`A_BOLD'                   Extra bright or bold text
`A_DIM'                    Half bright text
`A_REVERSE'                Reverse-video text
`A_STANDOUT'               The best highlighting mode available
`A_UNDERLINE'              Underlined text

So, to display a reverse-video status line on the top line of the
screen, you could code:

    stdscr.addstr(0, 0, "Current mode: Typing mode",
                  curses.A_REVERSE)
    stdscr.refresh()

The curses library also supports color on those terminals that provide
it. The most common such terminal is probably the Linux console,
followed by color xterms.

To use color, you must call the `start_color()' function soon after
calling `initscr()', to initialize the default color set (the
`curses.wrapper.wrapper()' function does this automatically).  Once
that’s done, the `has_colors()' function returns TRUE if the terminal
in use can actually display color.  (Note: curses uses the American
spelling ‘color’, instead of the Canadian/British spelling
‘colour’.  If you’re used to the British spelling, you’ll have
to resign yourself to misspelling it for the sake of these functions.)

The curses library maintains a finite number of color pairs, containing
a foreground (or text) color and a background color.  You can get the
attribute value corresponding to a color pair with the `color_pair()'
function; this can be bitwise-OR’ed with other attributes such as
`A_REVERSE', but again, such combinations are not guaranteed to work on
all terminals.

An example, which displays a line of text using color pair 1:

    stdscr.addstr("Pretty text", curses.color_pair(1))
    stdscr.refresh()

As I said before, a color pair consists of a foreground and background
color.  `start_color()' initializes 8 basic colors when it activates
color mode.  They are: 0:black, 1:red, 2:green, 3:yellow, 4:blue,
5:magenta, 6:cyan, and 7:white.  The curses module defines named
constants for each of these colors: `curses.COLOR_BLACK',
`curses.COLOR_RED', and so forth.

The `init_pair(n, f, b)' function changes the definition of color pair
`n', to foreground color f and background color b.  Color pair 0 is
hard-wired to white on black, and cannot be changed.

Let’s put all this together. To change color 1 to red text on a white
background, you would call:

    curses.init_pair(1, curses.COLOR_RED, curses.COLOR_WHITE)

When you change a color pair, any text already displayed using that
color pair will change to the new colors.  You can also display new
text in this color with:

    stdscr.addstr(0,0, "RED ALERT!", curses.color_pair(1))

Very fancy terminals can change the definitions of the actual colors to
a given RGB value.  This lets you change color 1, which is usually red,
to purple or blue or any other color you like.  Unfortunately, the
Linux console doesn’t support this, so I’m unable to try it out,
and can’t provide any examples.  You can check if your terminal can
do this by calling `can_change_color()', which returns TRUE if the
capability is there.  If you’re lucky enough to have such a talented
terminal, consult your system’s man pages for more information.


File: python.info,  Node: User Input,  Next: For More Information,  Prev: Displaying Text,  Up: Curses Programming with Python

10.3.5 User Input
-----------------

The curses library itself offers only very simple input mechanisms.
Python’s support adds a text-input widget that makes up some of the
lack.

The most common way to get input to a window is to use its `getch()'
method.  `getch()' pauses and waits for the user to hit a key,
displaying it if `echo()' has been called earlier.  You can optionally
specify a coordinate to which the cursor should be moved before pausing.

It’s possible to change this behavior with the method `nodelay()'.
After `nodelay(1)', `getch()' for the window becomes non-blocking and
returns `curses.ERR' (a value of -1) when no input is ready.  There’s
also a `halfdelay()' function, which can be used to (in effect) set a
timer on each `getch()'; if no input becomes available within a
specified delay (measured in tenths of a second), curses raises an
exception.

The `getch()' method returns an integer; if it’s between 0 and 255, it
represents the ASCII code of the key pressed.  Values greater than 255
are special keys such as Page Up, Home, or the cursor keys. You can
compare the value returned to constants such as `curses.KEY_PPAGE',
`curses.KEY_HOME', or `curses.KEY_LEFT'.  Usually the main loop of your
program will look something like this:

    while 1:
        c = stdscr.getch()
        if c == ord('p'):
            PrintDocument()
        elif c == ord('q'):
            break  # Exit the while()
        elif c == curses.KEY_HOME:
            x = y = 0

The *Note curses.ascii: 7a. module supplies ASCII class membership
functions that take either integer or 1-character-string arguments;
these may be useful in writing more readable tests for your command
interpreters.  It also supplies conversion functions  that take either
integer or 1-character-string arguments and return the same type.  For
example, *Note curses.ascii.ctrl(): 1475. returns the control character
corresponding to its argument.

There’s also a method to retrieve an entire string, `getstr()'.  It
isn’t used very often, because its functionality is quite limited;
the only editing keys available are the backspace key and the Enter
key, which terminates the string.  It can optionally be limited to a
fixed number of characters.

    curses.echo()            # Enable echoing of characters

    # Get a 15-character string, with the cursor on the top line
    s = stdscr.getstr(0,0, 15)

The Python *Note curses.textpad: 7c. module supplies something better.
With it, you can turn a window into a text box that supports an
Emacs-like set of keybindings.  Various methods of `Textbox' class
support editing with input validation and gathering the edit results
either with or without trailing spaces.   See the library documentation
on *Note curses.textpad: 7c. for the details.


File: python.info,  Node: For More Information,  Prev: User Input,  Up: Curses Programming with Python

10.3.6 For More Information
---------------------------

This HOWTO didn’t cover some advanced topics, such as screen-scraping
or capturing mouse events from an xterm instance.  But the Python
library page for the curses modules is now pretty complete.  You should
browse it next.

If you’re in doubt about the detailed behavior of any of the ncurses
entry points, consult the manual pages for your curses implementation,
whether it’s ncurses or a proprietary Unix vendor’s.  The manual
pages will document any quirks, and provide complete lists of all the
functions, attributes, and `ACS_*' characters available to you.

Because the curses API is so large, some functions aren’t supported
in the Python interface, not because they’re difficult to implement,
but because no one has needed them yet.  Feel free to add them and then
submit a patch.  Also, we don’t yet have support for the menu library
associated with ncurses; feel free to add that.

If you write an interesting little program, feel free to contribute it
as another demo.  We can always use more of them!

The ncurses FAQ: <http://invisible-island.net/ncurses/ncurses.faq.html>


File: python.info,  Node: Descriptor HowTo Guide,  Next: Idioms and Anti-Idioms in Python,  Prev: Curses Programming with Python,  Up: Python HOWTOs

10.4 Descriptor HowTo Guide
===========================

Author: Raymond Hettinger

Contact: <python at rcn dot com>

* Menu:

* Abstract::
* Definition and Introduction::
* Descriptor Protocol::
* Invoking Descriptors: Invoking Descriptors<2>.
* Descriptor Example::
* Properties::
* Functions and Methods::
* Static Methods and Class Methods::


File: python.info,  Node: Abstract,  Next: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.1 Abstract
---------------

Defines descriptors, summarizes the protocol, and shows how descriptors
are called.  Examines a custom descriptor and several built-in python
descriptors including functions, properties, static methods, and class
methods.  Shows how each works by giving a pure Python equivalent and a
sample application.

Learning about descriptors not only provides access to a larger
toolset, it creates a deeper understanding of how Python works and an
appreciation for the elegance of its design.


File: python.info,  Node: Definition and Introduction,  Next: Descriptor Protocol,  Prev: Abstract,  Up: Descriptor HowTo Guide

10.4.2 Definition and Introduction
----------------------------------

In general, a descriptor is an object attribute with “binding
behavior”, one whose attribute access has been overridden by methods
in the descriptor protocol.  Those methods are *Note __get__(): 73b,
*Note __set__(): 73c, and *Note __delete__(): 73d.  If any of those
methods are defined for an object, it is said to be a descriptor.

The default behavior for attribute access is to get, set, or delete the
attribute from an object’s dictionary.  For instance, `a.x' has a
lookup chain starting with `a.__dict__['x']', then
`type(a).__dict__['x']', and continuing through the base classes of
`type(a)' excluding metaclasses. If the looked-up value is an object
defining one of the descriptor methods, then Python may override the
default behavior and invoke the descriptor method instead.  Where this
occurs in the precedence chain depends on which descriptor methods were
defined.  Note that descriptors are only invoked for new style objects
or classes (a class is new style if it inherits from *Note object: 1f1.
or *Note type: 4ac.).

Descriptors are a powerful, general purpose protocol.  They are the
mechanism behind properties, methods, static methods, class methods,
and *Note super(): 395.  They are used throughout Python itself to
implement the new style classes introduced in version 2.2.  Descriptors
simplify the underlying C-code and offer a flexible set of new tools
for everyday Python programs.


File: python.info,  Node: Descriptor Protocol,  Next: Invoking Descriptors<2>,  Prev: Definition and Introduction,  Up: Descriptor HowTo Guide

10.4.3 Descriptor Protocol
--------------------------

`descr.__get__(self, obj, type=None) --> value'

`descr.__set__(self, obj, value) --> None'

`descr.__delete__(self, obj) --> None'

That is all there is to it.  Define any of these methods and an object
is considered a descriptor and can override default behavior upon being
looked up as an attribute.

If an object defines both *Note __get__(): 73b. and *Note __set__():
73c, it is considered a data descriptor.  Descriptors that only define
*Note __get__(): 73b. are called non-data descriptors (they are
typically used for methods but other uses are possible).

Data and non-data descriptors differ in how overrides are calculated
with respect to entries in an instance’s dictionary.  If an
instance’s dictionary has an entry with the same name as a data
descriptor, the data descriptor takes precedence.  If an instance’s
dictionary has an entry with the same name as a non-data descriptor,
the dictionary entry takes precedence.

To make a read-only data descriptor, define both *Note __get__(): 73b.
and *Note __set__(): 73c. with the *Note __set__(): 73c. raising an
*Note AttributeError: 1f8. when called.  Defining the *Note __set__():
73c. method with an exception raising placeholder is enough to make it
a data descriptor.


File: python.info,  Node: Invoking Descriptors<2>,  Next: Descriptor Example,  Prev: Descriptor Protocol,  Up: Descriptor HowTo Guide

10.4.4 Invoking Descriptors
---------------------------

A descriptor can be called directly by its method name.  For example,
`d.__get__(obj)'.

Alternatively, it is more common for a descriptor to be invoked
automatically upon attribute access.  For example, `obj.d' looks up `d'
in the dictionary of `obj'.  If `d' defines the method *Note __get__():
73b, then `d.__get__(obj)' is invoked according to the precedence rules
listed below.

The details of invocation depend on whether `obj' is an object or a
class.  Either way, descriptors only work for new style objects and
classes.  A class is new style if it is a subclass of *Note object: 1f1.

For objects, the machinery is in *Note object.__getattribute__(): 34f.
which transforms `b.x' into `type(b).__dict__['x'].__get__(b,
type(b))'.  The implementation works through a precedence chain that
gives data descriptors priority over instance variables, instance
variables priority over non-data descriptors, and assigns lowest
priority to *Note __getattr__(): 345. if provided.  The full C
implementation can be found in *Note PyObject_GenericGetAttr(): 2bec. in
Objects/object.c(1).

For classes, the machinery is in `type.__getattribute__()' which
transforms `B.x' into `B.__dict__['x'].__get__(None, B)'.  In pure
Python, it looks like:

    def __getattribute__(self, key):
        "Emulate type_getattro() in Objects/typeobject.c"
        v = object.__getattribute__(self, key)
        if hasattr(v, '__get__'):
            return v.__get__(None, self)
        return v

The important points to remember are:

   * descriptors are invoked by the *Note __getattribute__(): 34f.
     method

   * overriding *Note __getattribute__(): 34f. prevents automatic
     descriptor calls

   * *Note __getattribute__(): 34f. is only available with new style
     classes and objects

   * *Note object.__getattribute__(): 34f. and
     `type.__getattribute__()' make different calls to *Note __get__():
     73b.

   * data descriptors always override instance dictionaries.

   * non-data descriptors may be overridden by instance dictionaries.

The object returned by `super()' also has a custom *Note
__getattribute__(): 34f.  method for invoking descriptors.  The call
`super(B, obj).m()' searches `obj.__class__.__mro__' for the base class
`A' immediately following `B' and then returns
`A.__dict__['m'].__get__(obj, B)'.  If not a descriptor, `m' is
returned unchanged.  If not in the dictionary, `m' reverts to a search
using *Note object.__getattribute__(): 34f.

Note, in Python 2.2, `super(B, obj).m()' would only invoke *Note
__get__(): 73b. if `m' was a data descriptor.  In Python 2.3, non-data
descriptors also get invoked unless an old-style class is involved.
The implementation details are in `super_getattro()' in
Objects/typeobject.c(2).

The details above show that the mechanism for descriptors is embedded
in the *Note __getattribute__(): 34f. methods for *Note object: 1f1,
*Note type: 4ac, and *Note super(): 395.  Classes inherit this
machinery when they derive from *Note object: 1f1. or if they have a
meta-class providing similar functionality.  Likewise, classes can
turn-off descriptor invocation by overriding *Note __getattribute__():
34f.

---------- Footnotes ----------

(1) https://hg.python.org/cpython/file/2.7/Objects/object.c

(2) https://hg.python.org/cpython/file/2.7/Objects/typeobject.c


File: python.info,  Node: Descriptor Example,  Next: Properties,  Prev: Invoking Descriptors<2>,  Up: Descriptor HowTo Guide

10.4.5 Descriptor Example
-------------------------

The following code creates a class whose objects are data descriptors
which print a message for each get or set.  Overriding *Note
__getattribute__(): 34f. is alternate approach that could do this for
every attribute.  However, this descriptor is useful for monitoring
just a few chosen attributes:

    class RevealAccess(object):
        """A data descriptor that sets and returns values
           normally and prints a message logging their access.
        """

        def __init__(self, initval=None, name='var'):
            self.val = initval
            self.name = name

        def __get__(self, obj, objtype):
            print 'Retrieving', self.name
            return self.val

        def __set__(self, obj, val):
            print 'Updating', self.name
            self.val = val

    >>> class MyClass(object):
    ...     x = RevealAccess(10, 'var "x"')
    ...     y = 5
    ...
    >>> m = MyClass()
    >>> m.x
    Retrieving var "x"
    10
    >>> m.x = 20
    Updating var "x"
    >>> m.x
    Retrieving var "x"
    20
    >>> m.y
    5

The protocol is simple and offers exciting possibilities.  Several use
cases are so common that they have been packaged into individual
function calls.  Properties, bound and unbound methods, static methods,
and class methods are all based on the descriptor protocol.


File: python.info,  Node: Properties,  Next: Functions and Methods,  Prev: Descriptor Example,  Up: Descriptor HowTo Guide

10.4.6 Properties
-----------------

Calling *Note property(): 4a4. is a succinct way of building a data
descriptor that triggers function calls upon access to an attribute.
Its signature is:

    property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute `x':

    class C(object):
        def getx(self): return self.__x
        def setx(self, value): self.__x = value
        def delx(self): del self.__x
        x = property(getx, setx, delx, "I'm the 'x' property.")

To see how *Note property(): 4a4. is implemented in terms of the
descriptor protocol, here is a pure Python equivalent:

    class Property(object):
        "Emulate PyProperty_Type() in Objects/descrobject.c"

        def __init__(self, fget=None, fset=None, fdel=None, doc=None):
            self.fget = fget
            self.fset = fset
            self.fdel = fdel
            if doc is None and fget is not None:
                doc = fget.__doc__
            self.__doc__ = doc

        def __get__(self, obj, objtype=None):
            if obj is None:
                return self
            if self.fget is None:
                raise AttributeError("unreadable attribute")
            return self.fget(obj)

        def __set__(self, obj, value):
            if self.fset is None:
                raise AttributeError("can't set attribute")
            self.fset(obj, value)

        def __delete__(self, obj):
            if self.fdel is None:
                raise AttributeError("can't delete attribute")
            self.fdel(obj)

        def getter(self, fget):
            return type(self)(fget, self.fset, self.fdel, self.__doc__)

        def setter(self, fset):
            return type(self)(self.fget, fset, self.fdel, self.__doc__)

        def deleter(self, fdel):
            return type(self)(self.fget, self.fset, fdel, self.__doc__)

The *Note property(): 4a4. builtin helps whenever a user interface has
granted attribute access and then subsequent changes require the
intervention of a method.

For instance, a spreadsheet class may grant access to a cell value
through `Cell('b10').value'. Subsequent improvements to the program
require the cell to be recalculated on every access; however, the
programmer does not want to affect existing client code accessing the
attribute directly.  The solution is to wrap access to the value
attribute in a property data descriptor:

    class Cell(object):
        . . .
        def getvalue(self, obj):
            "Recalculate cell before returning value"
            self.recalc()
            return obj._value
        value = property(getvalue)


File: python.info,  Node: Functions and Methods,  Next: Static Methods and Class Methods,  Prev: Properties,  Up: Descriptor HowTo Guide

10.4.7 Functions and Methods
----------------------------

Python’s object oriented features are built upon a function based
environment.  Using non-data descriptors, the two are merged seamlessly.

Class dictionaries store methods as functions.  In a class definition,
methods are written using *Note def: 40d. and *Note lambda: 41c, the
usual tools for creating functions.  The only difference from regular
functions is that the first argument is reserved for the object
instance.  By Python convention, the instance reference is called
`self' but may be called `this' or any other variable name.

To support method calls, functions include the *Note __get__(): 73b.
method for binding methods during attribute access.  This means that
all functions are non-data descriptors which return bound or unbound
methods depending whether they are invoked from an object or a class.
In pure python, it works like this:

    class Function(object):
        . . .
        def __get__(self, obj, objtype=None):
            "Simulate func_descr_get() in Objects/funcobject.c"
            return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in
practice:

    >>> class D(object):
    ...     def f(self, x):
    ...         return x
    ...
    >>> d = D()
    >>> D.__dict__['f']  # Stored internally as a function
    <function f at 0x00C45070>
    >>> D.f              # Get from a class becomes an unbound method
    <unbound method D.f>
    >>> d.f              # Get from an instance becomes a bound method
    <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different
types.  While they could have been implemented that way, the actual C
implementation of *Note PyMethod_Type: 2de3. in
Objects/classobject.c(1) is a single object with two different
representations depending on whether the `im_self' field is set or is
`NULL' (the C equivalent of `None').

Likewise, the effects of calling a method object depend on the `im_self'
field. If set (meaning bound), the original function (stored in the
`im_func' field) is called as expected with the first argument set to
the instance.  If unbound, all of the arguments are passed unchanged to
the original function. The actual C implementation of
`instancemethod_call()' is only slightly more complex in that it
includes some type checking.

---------- Footnotes ----------

(1) https://hg.python.org/cpython/file/2.7/Objects/classobject.c


File: python.info,  Node: Static Methods and Class Methods,  Prev: Functions and Methods,  Up: Descriptor HowTo Guide

10.4.8 Static Methods and Class Methods
---------------------------------------

Non-data descriptors provide a simple mechanism for variations on the
usual patterns of binding functions into methods.

To recap, functions have a *Note __get__(): 73b. method so that they
can be converted to a method when accessed as attributes.  The non-data
descriptor transforms an `obj.f(*args)' call into `f(obj, *args)'.
Calling `klass.f(*args)' becomes `f(*args)'.

This chart summarizes the binding and its two most useful variants:

      Transformation        Called from an Object      Called from a Class
     ------------------------------------------------------------------------ 
     function              f(obj, *args)              f(*args)
     staticmethod          f(*args)                   f(*args)
     classmethod           f(type(obj), *args)        f(klass, *args)


Static methods return the underlying function without changes.  Calling
either `c.f' or `C.f' is the equivalent of a direct lookup into
`object.__getattribute__(c, "f")' or `object.__getattribute__(C, "f")'.
As a result, the function becomes identically accessible from either an
object or a class.

Good candidates for static methods are methods that do not reference the
`self' variable.

For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the
average, mean, median, and other descriptive statistics that depend on
the data. However, there may be useful functions which are conceptually
related but do not depend on the data.  For instance, `erf(x)' is handy
conversion routine that comes up in statistical work but does not
directly depend on a particular dataset.  It can be called either from
an object or the class:  `s.erf(1.5) --> .9332' or `Sample.erf(1.5) -->
.9332'.

Since staticmethods return the underlying function with no changes, the
example calls are unexciting:

    >>> class E(object):
    ...     def f(x):
    ...         print x
    ...     f = staticmethod(f)
    ...
    >>> print E.f(3)
    3
    >>> print E().f(3)
    3

Using the non-data descriptor protocol, a pure Python version of *Note
staticmethod(): 40e. would look like this:

    class StaticMethod(object):
        "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

        def __init__(self, f):
            self.f = f

        def __get__(self, obj, objtype=None):
            return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same for
whether the caller is an object or a class:

    >>> class E(object):
    ...     def f(klass, x):
    ...          return klass.__name__, x
    ...     f = classmethod(f)
    ...
    >>> print E.f(3)
    ('E', 3)
    >>> print E().f(3)
    ('E', 3)

This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for
classmethods is to create alternate class constructors.  In Python 2.3,
the classmethod *Note dict.fromkeys(): 926. creates a new dictionary
from a list of keys.  The pure Python equivalent is:

    class Dict(object):
        . . .
        def fromkeys(klass, iterable, value=None):
            "Emulate dict_fromkeys() in Objects/dictobject.c"
            d = klass()
            for key in iterable:
                d[key] = value
            return d
        fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this:

    >>> Dict.fromkeys('abracadabra')
    {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of *Note
classmethod(): 40f. would look like this:

    class ClassMethod(object):
        "Emulate PyClassMethod_Type() in Objects/funcobject.c"

        def __init__(self, f):
            self.f = f

        def __get__(self, obj, klass=None):
            if klass is None:
                klass = type(obj)
            def newfunc(*args):
                return self.f(klass, *args)
            return newfunc


File: python.info,  Node: Idioms and Anti-Idioms in Python,  Next: Functional Programming HOWTO,  Prev: Descriptor HowTo Guide,  Up: Python HOWTOs

10.5 Idioms and Anti-Idioms in Python
=====================================

Author: Moshe Zadka

This document is placed in the public domain.

Abstract
........

This document can be considered a companion to the tutorial. It shows
how to use Python, and even more importantly, how `not' to use Python.

* Menu:

* Language Constructs You Should Not Use::
* Exceptions: Exceptions<8>.
* Using the Batteries::
* Using Backslash to Continue Statements::


File: python.info,  Node: Language Constructs You Should Not Use,  Next: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.1 Language Constructs You Should Not Use
---------------------------------------------

While Python has relatively few gotchas compared to other languages, it
still has some constructs which are only useful in corner cases, or are
plain dangerous.

* Menu:

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.


File: python.info,  Node: from module import *,  Next: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.1 from module import *
.............................

* Menu:

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: Inside Function Definitions,  Next: At Module Level,  Up: from module import *

10.5.1.2 Inside Function Definitions
....................................

`from module import *' is `invalid' inside function definitions. While
many versions of Python do not check for the invalidity, it does not
make it more valid, no more than having a smart lawyer makes a man
innocent. Do not use it like that ever. Even in versions where it was
accepted, it made the function execution slower, because the compiler
could not be certain which names were local and which were global. In
Python 2.1 this construct causes warnings, and sometimes even errors.


File: python.info,  Node: At Module Level,  Next: When It Is Just Fine,  Prev: Inside Function Definitions,  Up: from module import *

10.5.1.3 At Module Level
........................

While it is valid to use `from module import *' at module level it is
usually a bad idea. For one, this loses an important property Python
otherwise has — you can know where each toplevel name is defined by a
simple “search” function in your favourite editor. You also open
yourself to trouble in the future, if some module grows additional
functions or classes.

One of the most awful questions asked on the newsgroup is why this code:

    f = open("www")
    f.read()

does not work. Of course, it works just fine (assuming you have a file
called “www”.) But it does not work if somewhere in the module, the
statement `from os import *' is present. The *Note os: 129. module has
a function called *Note open(): 2d9. which returns an integer. While it
is very useful, shadowing a builtin is one of its least useful
properties.

Remember, you can never know for sure what names a module exports, so
either take what you need — `from module import name1, name2', or
keep them in the module and access on a per-need basis —  `import
module;print module.name'.


File: python.info,  Node: When It Is Just Fine,  Prev: At Module Level,  Up: from module import *

10.5.1.4 When It Is Just Fine
.............................

There are situations in which `from module import *' is just fine:

   * The interactive prompt. For example, `from math import *' makes
     Python an amazing scientific calculator.

   * When extending a module in C with a module in Python.

   * When the module advertises itself as `from import *' safe.


File: python.info,  Node: Unadorned exec execfile and friends,  Next: from module import name1 name2,  Prev: from module import *,  Up: Language Constructs You Should Not Use

10.5.1.5 Unadorned `exec', `execfile()' and friends
...................................................

The word “unadorned” refers to the use without an explicit
dictionary, in which case those constructs evaluate code in the
`current' environment. This is dangerous for the same reasons `from
import *' is dangerous — it might step over variables you are
counting on and mess up things for the rest of your code.  Simply do
not do that.

Bad examples:

    >>> for name in sys.argv[1:]:
    >>>     exec "%s=1" % name
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         exec "s.%s=val" % var  # invalid!
    >>> execfile("handler.py")
    >>> handle()

Good examples:

    >>> d = {}
    >>> for name in sys.argv[1:]:
    >>>     d[name] = 1
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         setattr(s, var, val)
    >>> d={}
    >>> execfile("handle.py", d, d)
    >>> handle = d['handle']
    >>> handle()


File: python.info,  Node: from module import name1 name2,  Next: except,  Prev: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

10.5.1.6 from module import name1, name2
........................................

This is a “don’t” which is much weaker than the previous
“don’t”s but is still something you should not do if you don’t
have good reasons to do that. The reason it is usually a bad idea is
because you suddenly have an object which lives in two separate
namespaces. When the binding in one namespace changes, the binding in
the other will not, so there will be a discrepancy between them. This
happens when, for example, one module is reloaded, or changes the
definition of a function at runtime.

Bad example:

    # foo.py
    a = 1

    # bar.py
    from foo import a
    if something():
        a = 2 # danger: foo.a != a

Good example:

    # foo.py
    a = 1

    # bar.py
    import foo
    if something():
        foo.a = 2


File: python.info,  Node: except,  Prev: from module import name1 name2,  Up: Language Constructs You Should Not Use

10.5.1.7 except:
................

Python has the `except:' clause, which catches all exceptions. Since
`every' error in Python raises an exception, using `except:' can make
many programming errors look like runtime problems, which hinders the
debugging process.

The following code shows a great example of why this is bad:

    try:
        foo = opne("file") # misspelled "open"
    except:
        sys.exit("could not open file!")

The second line triggers a *Note NameError: 3bb, which is caught by the
except clause. The program will exit, and the error message the program
prints will make you think the problem is the readability of `"file"'
when in fact the real error has nothing to do with `"file"'.

A better way to write the above is

    try:
        foo = opne("file")
    except IOError:
        sys.exit("could not open file")

When this is run, Python will produce a traceback showing the *Note
NameError: 3bb, and it will be immediately apparent what needs to be
fixed.

Because `except:' catches `all' exceptions, including *Note SystemExit:
346, *Note KeyboardInterrupt: 251, and *Note GeneratorExit: 34b. (which
is not an error and should not normally be caught by user code), using
a bare `except:' is almost never a good idea.  In situations where you
need to catch all “normal” errors, such as in a framework that runs
callbacks, you can catch the base class for all normal exceptions,
*Note Exception: 34d.  Unfortunately in Python 2.x it is possible for
third-party code to raise exceptions that do not inherit from *Note
Exception: 34d, so in Python 2.x there are some cases where you may
have to use a bare `except:' and manually re-raise the exceptions you
don’t want to catch.


File: python.info,  Node: Exceptions<8>,  Next: Using the Batteries,  Prev: Language Constructs You Should Not Use,  Up: Idioms and Anti-Idioms in Python

10.5.2 Exceptions
-----------------

Exceptions are a useful feature of Python. You should learn to raise
them whenever something unexpected occurs, and catch them only where
you can do something about them.

The following is a very popular anti-idiom

    def get_status(file):
        if not os.path.exists(file):
            print "file not found"
            sys.exit(1)
        return open(file).readline()

Consider the case where the file gets deleted between the time the call
to *Note os.path.exists(): e26. is made and the time *Note open(): 2d9.
is called. In that case the last line will raise an *Note IOError: 1fa.
The same thing would happen if `file' exists but has no read
permission.  Since testing this on a normal machine on existent and
non-existent files makes it seem bugless, the test results will seem
fine, and the code will get shipped.  Later an unhandled *Note IOError:
1fa. (or perhaps some other *Note EnvironmentError: 976.) escapes to the
user, who gets to watch the ugly traceback.

Here is a somewhat better way to do it.

    def get_status(file):
        try:
            return open(file).readline()
        except EnvironmentError as err:
            print "Unable to open file: {}".format(err)
            sys.exit(1)

In this version, `either' the file gets opened and the line is read (so
it works even on flaky NFS or SMB connections), or an error message is
printed that provides all the available information on why the open
failed, and the application is aborted.

However, even this version of `get_status()' makes too many assumptions
— that it will only be used in a short running script, and not, say,
in a long running server. Sure, the caller could do something like

    try:
        status = get_status(log)
    except SystemExit:
        status = None

But there is a better way.  You should try to use as few `except'
clauses in your code as you can — the ones you do use will usually be
inside calls which should always succeed, or a catch-all in a main
function.

So, an even better version of `get_status()' is probably

    def get_status(file):
        return open(file).readline()

The caller can deal with the exception if it wants (for example, if it
tries several files in a loop), or just let the exception filter
upwards to `its' caller.

But the last version still has a serious problem — due to
implementation details in CPython, the file would not be closed when an
exception is raised until the exception handler finishes; and, worse,
in other implementations (e.g., Jython) it might not be closed at all
regardless of whether or not an exception is raised.

The best version of this function uses the `open()' call as a context
manager, which will ensure that the file gets closed as soon as the
function returns:

    def get_status(file):
        with open(file) as fp:
            return fp.readline()


File: python.info,  Node: Using the Batteries,  Next: Using Backslash to Continue Statements,  Prev: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

10.5.3 Using the Batteries
--------------------------

Every so often, people seem to be writing stuff in the Python library
again, usually poorly. While the occasional module has a poor
interface, it is usually much better to use the rich standard library
and data types that come with Python than inventing your own.

A useful module very few people know about is *Note os.path: 12a. It
always has the correct path arithmetic for your operating system, and
will usually be much better than whatever you come up with yourself.

Compare:

    # ugh!
    return dir+"/"+file
    # better
    return os.path.join(dir, file)

More useful functions in *Note os.path: 12a.: `basename()',
`dirname()' and `splitext()'.

There are also many useful built-in functions people seem not to be
aware of for some reason: *Note min(): 224. and *Note max(): 225. can
find the minimum/maximum of any sequence with comparable semantics, for
example, yet many people write their own *Note max(): 225./*Note min():
224. Another highly useful function is *Note reduce(): 2fc. which can
be used to repeatly apply a binary operation to a sequence, reducing it
to a single value.  For example, compute a factorial with a series of
multiply operations:

    >>> n = 4
    >>> import operator
    >>> reduce(operator.mul, range(1, n+1))
    24

When it comes to parsing numbers, note that *Note float(): 1eb, *Note
int(): 1f2. and *Note long(): 1f3. all accept string arguments and will
reject ill-formed strings by raising an *Note ValueError: 236.


File: python.info,  Node: Using Backslash to Continue Statements,  Prev: Using the Batteries,  Up: Idioms and Anti-Idioms in Python

10.5.4 Using Backslash to Continue Statements
---------------------------------------------

Since Python treats a newline as a statement terminator, and since
statements are often more than is comfortable to put in one line, many
people do:

    if foo.bar()['first'][0] == baz.quux(1, 2)[5:9] and \
       calculate_number(10, 20) != forbulate(500, 360):
          pass

You should realize that this is dangerous: a stray space after the `\'
would make this line wrong, and stray spaces are notoriously hard to
see in editors.  In this case, at least it would be a syntax error, but
if the code was:

    value = foo.bar()['first'][0]*baz.quux(1, 2)[5:9] \
            + calculate_number(10, 20)*forbulate(500, 360)

then it would just be subtly wrong.

It is usually much better to use the implicit continuation inside
parenthesis:

This version is bulletproof:

    value = (foo.bar()['first'][0]*baz.quux(1, 2)[5:9]
            + calculate_number(10, 20)*forbulate(500, 360))


File: python.info,  Node: Functional Programming HOWTO,  Next: Logging HOWTO,  Prev: Idioms and Anti-Idioms in Python,  Up: Python HOWTOs

10.6 Functional Programming HOWTO
=================================

Author: A. M. Kuchling

Release: 0.31

In this document, we’ll take a tour of Python’s features suitable
for implementing programs in a functional style.  After an introduction
to the concepts of functional programming, we’ll look at language
features such as *Note iterator: 8a8.s and *Note generator: 5f7.s and
relevant library modules such as *Note itertools: fb. and *Note
functools: da.

* Menu:

* Introduction: Introduction<11>.
* Iterators: Iterators<2>.
* Generator expressions and list comprehensions::
* Generators: Generators<2>.
* Built-in functions::
* Small functions and the lambda expression::
* The itertools module::
* The functools module::
* Revision History and Acknowledgements::
* References::


File: python.info,  Node: Introduction<11>,  Next: Iterators<2>,  Up: Functional Programming HOWTO

10.6.1 Introduction
-------------------

This section explains the basic concept of functional programming; if
you’re just interested in learning about Python language features,
skip to the next section.

Programming languages support decomposing problems in several different
ways:

   * Most programming languages are `procedural': programs are lists of
     instructions that tell the computer what to do with the
     program’s input.  C, Pascal, and even Unix shells are procedural
     languages.

   * In `declarative' languages, you write a specification that
     describes the problem to be solved, and the language
     implementation figures out how to perform the computation
     efficiently.  SQL is the declarative language you’re most likely
     to be familiar with; a SQL query describes the data set you want
     to retrieve, and the SQL engine decides whether to scan tables or
     use indexes, which subclauses should be performed first, etc.

   * `Object-oriented' programs manipulate collections of objects.
     Objects have internal state and support methods that query or
     modify this internal state in some way. Smalltalk and Java are
     object-oriented languages.  C++ and Python are languages that
     support object-oriented programming, but don’t force the use of
     object-oriented features.

   * `Functional' programming decomposes a problem into a set of
     functions.  Ideally, functions only take inputs and produce
     outputs, and don’t have any internal state that affects the
     output produced for a given input.  Well-known functional
     languages include the ML family (Standard ML, OCaml, and other
     variants) and Haskell.

The designers of some computer languages choose to emphasize one
particular approach to programming.  This often makes it difficult to
write programs that use a different approach.  Other languages are
multi-paradigm languages that support several different approaches.
Lisp, C++, and Python are multi-paradigm; you can write programs or
libraries that are largely procedural, object-oriented, or functional
in all of these languages.  In a large program, different sections
might be written using different approaches; the GUI might be
object-oriented while the processing logic is procedural or functional,
for example.

In a functional program, input flows through a set of functions. Each
function operates on its input and produces some output.  Functional
style discourages functions with side effects that modify internal
state or make other changes that aren’t visible in the function’s
return value.  Functions that have no side effects at all are called
`purely functional'.  Avoiding side effects means not using data
structures that get updated as a program runs; every function’s
output must only depend on its input.

Some languages are very strict about purity and don’t even have
assignment statements such as `a=3' or `c = a + b', but it’s
difficult to avoid all side effects.  Printing to the screen or writing
to a disk file are side effects, for example.  For example, in Python a
`print' statement or a `time.sleep(1)' both return no useful value;
they’re only called for their side effects of sending some text to
the screen or pausing execution for a second.

Python programs written in functional style usually won’t go to the
extreme of avoiding all I/O or all assignments; instead, they’ll
provide a functional-appearing interface but will use non-functional
features internally.  For example, the implementation of a function
will still use assignments to local variables, but won’t modify
global variables or have other side effects.

Functional programming can be considered the opposite of object-oriented
programming.  Objects are little capsules containing some internal
state along with a collection of method calls that let you modify this
state, and programs consist of making the right set of state changes.
Functional programming wants to avoid state changes as much as possible
and works with data flowing between functions.  In Python you might
combine the two approaches by writing functions that take and return
instances representing objects in your application (e-mail messages,
transactions, etc.).

Functional design may seem like an odd constraint to work under.  Why
should you avoid objects and side effects?  There are theoretical and
practical advantages to the functional style:

   * Formal provability.

   * Modularity.

   * Composability.

   * Ease of debugging and testing.

* Menu:

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::


File: python.info,  Node: Formal provability,  Next: Modularity,  Up: Introduction<11>

10.6.1.1 Formal provability
...........................

A theoretical benefit is that it’s easier to construct a mathematical
proof that a functional program is correct.

For a long time researchers have been interested in finding ways to
mathematically prove programs correct.  This is different from testing
a program on numerous inputs and concluding that its output is usually
correct, or reading a program’s source code and concluding that the
code looks right; the goal is instead a rigorous proof that a program
produces the right result for all possible inputs.

The technique used to prove programs correct is to write down
`invariants', properties of the input data and of the program’s
variables that are always true.  For each line of code, you then show
that if invariants X and Y are true `before' the line is executed, the
slightly different invariants X’ and Y’ are true `after' the line
is executed.  This continues until you reach the end of the program, at
which point the invariants should match the desired conditions on the
program’s output.

Functional programming’s avoidance of assignments arose because
assignments are difficult to handle with this technique; assignments
can break invariants that were true before the assignment without
producing any new invariants that can be propagated onward.

Unfortunately, proving programs correct is largely impractical and not
relevant to Python software. Even trivial programs require proofs that
are several pages long; the proof of correctness for a moderately
complicated program would be enormous, and few or none of the programs
you use daily (the Python interpreter, your XML parser, your web
browser) could be proven correct.  Even if you wrote down or generated
a proof, there would then be the question of verifying the proof; maybe
there’s an error in it, and you wrongly believe you’ve proved the
program correct.


File: python.info,  Node: Modularity,  Next: Ease of debugging and testing,  Prev: Formal provability,  Up: Introduction<11>

10.6.1.2 Modularity
...................

A more practical benefit of functional programming is that it forces
you to break apart your problem into small pieces.  Programs are more
modular as a result.  It’s easier to specify and write a small
function that does one thing than a large function that performs a
complicated transformation.  Small functions are also easier to read
and to check for errors.


File: python.info,  Node: Ease of debugging and testing,  Next: Composability,  Prev: Modularity,  Up: Introduction<11>

10.6.1.3 Ease of debugging and testing
......................................

Testing and debugging a functional-style program is easier.

Debugging is simplified because functions are generally small and
clearly specified.  When a program doesn’t work, each function is an
interface point where you can check that the data are correct.  You can
look at the intermediate inputs and outputs to quickly isolate the
function that’s responsible for a bug.

Testing is easier because each function is a potential subject for a
unit test.  Functions don’t depend on system state that needs to be
replicated before running a test; instead you only have to synthesize
the right input and then check that the output matches expectations.


File: python.info,  Node: Composability,  Prev: Ease of debugging and testing,  Up: Introduction<11>

10.6.1.4 Composability
......................

As you work on a functional-style program, you’ll write a number of
functions with varying inputs and outputs.  Some of these functions
will be unavoidably specialized to a particular application, but others
will be useful in a wide variety of programs.  For example, a function
that takes a directory path and returns all the XML files in the
directory, or a function that takes a filename and returns its
contents, can be applied to many different situations.

Over time you’ll form a personal library of utilities.  Often
you’ll assemble new programs by arranging existing functions in a new
configuration and writing a few functions specialized for the current
task.


File: python.info,  Node: Iterators<2>,  Next: Generator expressions and list comprehensions,  Prev: Introduction<11>,  Up: Functional Programming HOWTO

10.6.2 Iterators
----------------

I’ll start by looking at a Python language feature that’s an
important foundation for writing functional-style programs: iterators.

An iterator is an object representing a stream of data; this object
returns the data one element at a time.  A Python iterator must support
a method called `next()' that takes no arguments and always returns the
next element of the stream.  If there are no more elements in the
stream, `next()' must raise the `StopIteration' exception.  Iterators
don’t have to be finite, though; it’s perfectly reasonable to write
an iterator that produces an infinite stream of data.

The built-in *Note iter(): 334. function takes an arbitrary object and
tries to return an iterator that will return the object’s contents or
elements, raising *Note TypeError: 218. if the object doesn’t support
iteration.  Several of Python’s built-in data types support
iteration, the most common being lists and dictionaries.  An object is
called an `iterable' object if you can get an iterator for it.

You can experiment with the iteration interface manually:

    >>> L = [1,2,3]
    >>> it = iter(L)
    >>> print it
    <...iterator object at ...>
    >>> it.next()
    1
    >>> it.next()
    2
    >>> it.next()
    3
    >>> it.next()
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    StopIteration
    >>>

Python expects iterable objects in several different contexts, the most
important being the `for' statement.  In the statement `for X in Y', Y
must be an iterator or some object for which `iter()' can create an
iterator.  These two statements are equivalent:

    for i in iter(obj):
        print i

    for i in obj:
        print i

Iterators can be materialized as lists or tuples by using the *Note
list(): 3d6. or *Note tuple(): 421. constructor functions:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> t = tuple(iterator)
    >>> t
    (1, 2, 3)

Sequence unpacking also supports iterators: if you know an iterator
will return N elements, you can unpack them into an N-tuple:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> a,b,c = iterator
    >>> a,b,c
    (1, 2, 3)

Built-in functions such as *Note max(): 225. and *Note min(): 224. can
take a single iterator argument and will return the largest or smallest
element.  The `"in"' and `"not in"' operators also support iterators:
`X in iterator' is true if X is found in the stream returned by the
iterator.  You’ll run into obvious problems if the iterator is
infinite; `max()', `min()' will never return, and if the element X
never appears in the stream, the `"in"' and `"not in"' operators
won’t return either.

Note that you can only go forward in an iterator; there’s no way to
get the previous element, reset the iterator, or make a copy of it.
Iterator objects can optionally provide these additional capabilities,
but the iterator protocol only specifies the `next()' method.
Functions may therefore consume all of the iterator’s output, and if
you need to do something different with the same stream, you’ll have
to create a new iterator.

* Menu:

* Data Types That Support Iterators::


File: python.info,  Node: Data Types That Support Iterators,  Up: Iterators<2>

10.6.2.1 Data Types That Support Iterators
..........................................

We’ve already seen how lists and tuples support iterators.  In fact,
any Python sequence type, such as strings, will automatically support
creation of an iterator.

Calling *Note iter(): 334. on a dictionary returns an iterator that
will loop over the dictionary’s keys:

    >>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
    ...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
    >>> for key in m:
    ...     print key, m[key]
    Mar 3
    Feb 2
    Aug 8
    Sep 9
    Apr 4
    Jun 6
    Jul 7
    Jan 1
    May 5
    Nov 11
    Dec 12
    Oct 10

Note that the order is essentially random, because it’s based on the
hash ordering of the objects in the dictionary.

Applying `iter()' to a dictionary always loops over the keys, but
dictionaries have methods that return other iterators.  If you want to
iterate over keys, values, or key/value pairs, you can explicitly call
the `iterkeys()', `itervalues()', or `iteritems()' methods to get an
appropriate iterator.

The *Note dict(): 319. constructor can accept an iterator that returns
a finite stream of `(key, value)' tuples:

    >>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
    >>> dict(iter(L))
    {'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

Files also support iteration by calling the `readline()' method until
there are no more lines in the file.  This means you can read each line
of a file like this:

    for line in file:
        # do something for each line
        ...

Sets can take their contents from an iterable and let you iterate over
the set’s elements:

    S = set((2, 3, 5, 7, 11, 13))
    for i in S:
        print i


File: python.info,  Node: Generator expressions and list comprehensions,  Next: Generators<2>,  Prev: Iterators<2>,  Up: Functional Programming HOWTO

10.6.3 Generator expressions and list comprehensions
----------------------------------------------------

Two common operations on an iterator’s output are 1) performing some
operation for every element, 2) selecting a subset of elements that
meet some condition.  For example, given a list of strings, you might
want to strip off trailing whitespace from each line or extract all the
strings containing a given substring.

List comprehensions and generator expressions (short form:
“listcomps” and “genexps”) are a concise notation for such
operations, borrowed from the functional programming language Haskell
(<https://www.haskell.org/>).  You can strip all the whitespace from a
stream of strings with the following code:

    line_list = ['  line 1\n', 'line 2  \n', ...]

    # Generator expression -- returns iterator
    stripped_iter = (line.strip() for line in line_list)

    # List comprehension -- returns list
    stripped_list = [line.strip() for line in line_list]

You can select only certain elements by adding an `"if"' condition:

    stripped_list = [line.strip() for line in line_list
                     if line != ""]

With a list comprehension, you get back a Python list; `stripped_list'
is a list containing the resulting lines, not an iterator.  Generator
expressions return an iterator that computes the values as necessary,
not needing to materialize all the values at once.  This means that
list comprehensions aren’t useful if you’re working with iterators
that return an infinite stream or a very large amount of data.
Generator expressions are preferable in these situations.

Generator expressions are surrounded by parentheses (“()”) and list
comprehensions are surrounded by square brackets (“[]”).  Generator
expressions have the form:

    ( expression for expr in sequence1
                 if condition1
                 for expr2 in sequence2
                 if condition2
                 for expr3 in sequence3 ...
                 if condition3
                 for exprN in sequenceN
                 if conditionN )

Again, for a list comprehension only the outside brackets are different
(square brackets instead of parentheses).

The elements of the generated output will be the successive values of
`expression'.  The `if' clauses are all optional; if present,
`expression' is only evaluated and added to the result when `condition'
is true.

Generator expressions always have to be written inside parentheses, but
the parentheses signalling a function call also count.  If you want to
create an iterator that will be immediately passed to a function you
can write:

    obj_total = sum(obj.count for obj in list_all_objects())

The `for...in' clauses contain the sequences to be iterated over.  The
sequences do not have to be the same length, because they are iterated
over from left to right, `not' in parallel.  For each element in
`sequence1', `sequence2' is looped over from the beginning.
`sequence3' is then looped over for each resulting pair of elements
from `sequence1' and `sequence2'.

To put it another way, a list comprehension or generator expression is
equivalent to the following Python code:

    for expr1 in sequence1:
        if not (condition1):
            continue   # Skip this element
        for expr2 in sequence2:
            if not (condition2):
                continue   # Skip this element
            ...
            for exprN in sequenceN:
                if not (conditionN):
                    continue   # Skip this element

                # Output the value of
                # the expression.

This means that when there are multiple `for...in' clauses but no `if'
clauses, the length of the resulting output will be equal to the
product of the lengths of all the sequences.  If you have two lists of
length 3, the output list is 9 elements long:

    >>> seq1 = 'abc'
    >>> seq2 = (1,2,3)
    >>> [(x,y) for x in seq1 for y in seq2]
    [('a', 1), ('a', 2), ('a', 3),
     ('b', 1), ('b', 2), ('b', 3),
     ('c', 1), ('c', 2), ('c', 3)]

To avoid introducing an ambiguity into Python’s grammar, if
`expression' is creating a tuple, it must be surrounded with
parentheses.  The first list comprehension below is a syntax error,
while the second one is correct:

    # Syntax error
    [ x,y for x in seq1 for y in seq2]
    # Correct
    [ (x,y) for x in seq1 for y in seq2]


File: python.info,  Node: Generators<2>,  Next: Built-in functions,  Prev: Generator expressions and list comprehensions,  Up: Functional Programming HOWTO

10.6.4 Generators
-----------------

Generators are a special class of functions that simplify the task of
writing iterators.  Regular functions compute a value and return it,
but generators return an iterator that returns a stream of values.

You’re doubtless familiar with how regular function calls work in
Python or C.  When you call a function, it gets a private namespace
where its local variables are created.  When the function reaches a
`return' statement, the local variables are destroyed and the value is
returned to the caller.  A later call to the same function creates a
new private namespace and a fresh set of local variables. But, what if
the local variables weren’t thrown away on exiting a function?  What
if you could later resume the function where it left off?  This is what
generators provide; they can be thought of as resumable functions.

Here’s the simplest example of a generator function:

    def generate_ints(N):
        for i in range(N):
            yield i

Any function containing a `yield' keyword is a generator function; this
is detected by Python’s *Note bytecode: 59e. compiler which compiles
the function specially as a result.

When you call a generator function, it doesn’t return a single value;
instead it returns a generator object that supports the iterator
protocol.  On executing the `yield' expression, the generator outputs
the value of `i', similar to a `return' statement.  The big difference
between `yield' and a `return' statement is that on reaching a `yield'
the generator’s state of execution is suspended and local variables
are preserved.  On the next call to the generator’s `.next()' method,
the function will resume executing.

Here’s a sample usage of the `generate_ints()' generator:

    >>> gen = generate_ints(3)
    >>> gen
    <generator object generate_ints at ...>
    >>> gen.next()
    0
    >>> gen.next()
    1
    >>> gen.next()
    2
    >>> gen.next()
    Traceback (most recent call last):
      File "stdin", line 1, in ?
      File "stdin", line 2, in generate_ints
    StopIteration

You could equally write `for i in generate_ints(5)', or `a,b,c =
generate_ints(3)'.

Inside a generator function, the `return' statement can only be used
without a value, and signals the end of the procession of values; after
executing a `return' the generator cannot return any further values.
`return' with a value, such as `return 5', is a syntax error inside a
generator function.  The end of the generator’s results can also be
indicated by raising `StopIteration' manually, or by just letting the
flow of execution fall off the bottom of the function.

You could achieve the effect of generators manually by writing your own
class and storing all the local variables of the generator as instance
variables.  For example, returning a list of integers could be done by
setting `self.count' to 0, and having the `next()' method increment
`self.count' and return it.  However, for a moderately complicated
generator, writing a corresponding class can be much messier.

The test suite included with Python’s library, `test_generators.py',
contains a number of more interesting examples.  Here’s one generator
that implements an in-order traversal of a tree using generators
recursively.

    # A recursive generator that generates Tree leaves in in-order.
    def inorder(t):
        if t:
            for x in inorder(t.left):
                yield x

            yield t.label

            for x in inorder(t.right):
                yield x

Two other examples in `test_generators.py' produce solutions for the
N-Queens problem (placing N queens on an NxN chess board so that no
queen threatens another) and the Knight’s Tour (finding a route that
takes a knight to every square of an NxN chessboard without visiting
any square twice).

* Menu:

* Passing values into a generator::


File: python.info,  Node: Passing values into a generator,  Up: Generators<2>

10.6.4.1 Passing values into a generator
........................................

In Python 2.4 and earlier, generators only produced output.  Once a
generator’s code was invoked to create an iterator, there was no way
to pass any new information into the function when its execution is
resumed.  You could hack together this ability by making the generator
look at a global variable or by passing in some mutable object that
callers then modify, but these approaches are messy.

In Python 2.5 there’s a simple way to pass values into a generator.
*Note yield: 30a. became an expression, returning a value that can be
assigned to a variable or otherwise operated on:

    val = (yield i)

I recommend that you `always' put parentheses around a `yield'
expression when you’re doing something with the returned value, as in
the above example.  The parentheses aren’t always necessary, but
it’s easier to always add them instead of having to remember when
they’re needed.

(PEP 342 explains the exact rules, which are that a `yield'-expression
must always be parenthesized except when it occurs at the top-level
expression on the right-hand side of an assignment.  This means you can
write `val = yield i' but have to use parentheses when there’s an
operation, as in `val = (yield i) + 12'.)

Values are sent into a generator by calling its `send(value)' method.
This method resumes the generator’s code and the `yield' expression
returns the specified value.  If the regular `next()' method is called,
the `yield' returns `None'.

Here’s a simple counter that increments by 1 and allows changing the
value of the internal counter.

    def counter (maximum):
        i = 0
        while i < maximum:
            val = (yield i)
            # If value provided, change counter
            if val is not None:
                i = val
            else:
                i += 1

And here’s an example of changing the counter:

    >>> it = counter(10)
    >>> print it.next()
    0
    >>> print it.next()
    1
    >>> print it.send(8)
    8
    >>> print it.next()
    9
    >>> print it.next()
    Traceback (most recent call last):
      File "t.py", line 15, in ?
        print it.next()
    StopIteration

Because `yield' will often be returning `None', you should always check
for this case.  Don’t just use its value in expressions unless
you’re sure that the `send()' method will be the only method used to
resume your generator function.

In addition to `send()', there are two other new methods on generators:

   * `throw(type, value=None, traceback=None)' is used to raise an
     exception inside the generator; the exception is raised by the
     `yield' expression where the generator’s execution is paused.

   * `close()' raises a *Note GeneratorExit: 34b. exception inside the
     generator to terminate the iteration.  On receiving this
     exception, the generator’s code must either raise *Note
     GeneratorExit: 34b. or *Note StopIteration: 347.; catching the
     exception and doing anything else is illegal and will trigger a
     *Note RuntimeError: 3b3.  `close()' will also be called by
     Python’s garbage collector when the generator is
     garbage-collected.

     If you need to run cleanup code when a *Note GeneratorExit: 34b.
     occurs, I suggest using a `try: ... finally:' suite instead of
     catching *Note GeneratorExit: 34b.

The cumulative effect of these changes is to turn generators from
one-way producers of information into both producers and consumers.

Generators also become `coroutines', a more generalized form of
subroutines.  Subroutines are entered at one point and exited at
another point (the top of the function, and a `return' statement), but
coroutines can be entered, exited, and resumed at many different points
(the `yield' statements).


File: python.info,  Node: Built-in functions,  Next: Small functions and the lambda expression,  Prev: Generators<2>,  Up: Functional Programming HOWTO

10.6.5 Built-in functions
-------------------------

Let’s look in more detail at built-in functions often used with
iterators.

Two of Python’s built-in functions, *Note map(): 318. and *Note
filter(): 422, are somewhat obsolete; they duplicate the features of
list comprehensions but return actual lists instead of iterators.

`map(f, iterA, iterB, ...)' returns a list containing `f(iterA[0],
iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ...'.

    >>> def upper(s):
    ...     return s.upper()

    >>> map(upper, ['sentence', 'fragment'])
    ['SENTENCE', 'FRAGMENT']

    >>> [upper(s) for s in ['sentence', 'fragment']]
    ['SENTENCE', 'FRAGMENT']

As shown above, you can achieve the same effect with a list
comprehension.  The *Note itertools.imap(): d8d. function does the same
thing but can handle infinite iterators; it’ll be discussed later, in
the section on the *Note itertools: fb. module.

`filter(predicate, iter)' returns a list that contains all the sequence
elements that meet a certain condition, and is similarly duplicated by
list comprehensions.  A `predicate' is a function that returns the
truth value of some condition; for use with *Note filter(): 422, the
predicate must take a single value.

    >>> def is_even(x):
    ...     return (x % 2) == 0

    >>> filter(is_even, range(10))
    [0, 2, 4, 6, 8]

This can also be written as a list comprehension:

    >>> [x for x in range(10) if is_even(x)]
    [0, 2, 4, 6, 8]

*Note filter(): 422. also has a counterpart in the *Note itertools: fb.
module, *Note itertools.ifilter(): 8aa, that returns an iterator and
can therefore handle infinite sequences just as *Note itertools.imap():
d8d. can.

`reduce(func, iter, [initial_value])' doesn’t have a counterpart in
the *Note itertools: fb. module because it cumulatively performs an
operation on all the iterable’s elements and therefore can’t be
applied to infinite iterables.  `func' must be a function that takes
two elements and returns a single value.  *Note reduce(): 2fc. takes
the first two elements A and B returned by the iterator and calculates
`func(A, B)'.  It then requests the third element, C, calculates
`func(func(A, B), C)', combines this result with the fourth element
returned, and continues until the iterable is exhausted.  If the
iterable returns no values at all, a *Note TypeError: 218. exception is
raised.  If the initial value is supplied, it’s used as a starting
point and `func(initial_value, A)' is the first calculation.

    >>> import operator
    >>> reduce(operator.concat, ['A', 'BB', 'C'])
    'ABBC'
    >>> reduce(operator.concat, [])
    Traceback (most recent call last):
      ...
    TypeError: reduce() of empty sequence with no initial value
    >>> reduce(operator.mul, [1,2,3], 1)
    6
    >>> reduce(operator.mul, [], 1)
    1

If you use *Note operator.add(): dbc. with *Note reduce(): 2fc,
you’ll add up all the elements of the iterable.  This case is so
common that there’s a special built-in called *Note sum(): 43f. to
compute it:

    >>> reduce(operator.add, [1,2,3,4], 0)
    10
    >>> sum([1,2,3,4])
    10
    >>> sum([])
    0

For many uses of *Note reduce(): 2fc, though, it can be clearer to just
write the obvious *Note for: 303. loop:

    # Instead of:
    product = reduce(operator.mul, [1,2,3], 1)

    # You can write:
    product = 1
    for i in [1,2,3]:
        product *= i

`enumerate(iter)' counts off the elements in the iterable, returning
2-tuples containing the count and each element.

    >>> for item in enumerate(['subject', 'verb', 'object']):
    ...     print item
    (0, 'subject')
    (1, 'verb')
    (2, 'object')

*Note enumerate(): 440. is often used when looping through a list and
recording the indexes at which certain conditions are met:

    f = open('data.txt', 'r')
    for i, line in enumerate(f):
        if line.strip() == '':
            print 'Blank line at line #%i' % i

`sorted(iterable, [cmp=None], [key=None], [reverse=False])' collects
all the elements of the iterable into a list, sorts the list, and
returns the sorted result.  The `cmp', `key', and `reverse' arguments
are passed through to the constructed list’s `.sort()' method.

    >>> import random
    >>> # Generate 8 random numbers between [0, 10000)
    >>> rand_list = random.sample(range(10000), 8)
    >>> rand_list
    [769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
    >>> sorted(rand_list)
    [769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
    >>> sorted(rand_list, reverse=True)
    [9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

(For a more detailed discussion of sorting, see the Sorting mini-HOWTO
in the Python wiki at <https://wiki.python.org/moin/HowTo/Sorting>.)

The `any(iter)' and `all(iter)' built-ins look at the truth values of an
iterable’s contents.  *Note any(): 3c6. returns `True' if any element
in the iterable is a true value, and *Note all(): 3c7. returns `True'
if all of the elements are true values:

    >>> any([0,1,0])
    True
    >>> any([0,0,0])
    False
    >>> any([1,1,1])
    True
    >>> all([0,1,0])
    False
    >>> all([0,0,0])
    False
    >>> all([1,1,1])
    True


File: python.info,  Node: Small functions and the lambda expression,  Next: The itertools module,  Prev: Built-in functions,  Up: Functional Programming HOWTO

10.6.6 Small functions and the lambda expression
------------------------------------------------

When writing functional-style programs, you’ll often need little
functions that act as predicates or that combine elements in some way.

If there’s a Python built-in or a module function that’s suitable,
you don’t need to define a new function at all:

    stripped_lines = [line.strip() for line in lines]
    existing_files = filter(os.path.exists, file_list)

If the function you need doesn’t exist, you need to write it.  One
way to write small functions is to use the `lambda' statement.
`lambda' takes a number of parameters and an expression combining these
parameters, and creates a small function that returns the value of the
expression:

    lowercase = lambda x: x.lower()

    print_assign = lambda name, value: name + '=' + str(value)

    adder = lambda x, y: x+y

An alternative is to just use the `def' statement and define a function
in the usual way:

    def lowercase(x):
        return x.lower()

    def print_assign(name, value):
        return name + '=' + str(value)

    def adder(x,y):
        return x + y

Which alternative is preferable?  That’s a style question; my usual
course is to avoid using `lambda'.

One reason for my preference is that `lambda' is quite limited in the
functions it can define.  The result has to be computable as a single
expression, which means you can’t have multiway `if... elif... else'
comparisons or `try... except' statements.  If you try to do too much
in a `lambda' statement, you’ll end up with an overly complicated
expression that’s hard to read.  Quick, what’s the following code
doing?

    total = reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

You can figure it out, but it takes time to disentangle the expression
to figure out what’s going on.  Using a short nested `def' statements
makes things a little bit better:

    def combine (a, b):
        return 0, a[1] + b[1]

    total = reduce(combine, items)[1]

But it would be best of all if I had simply used a `for' loop:

    total = 0
    for a, b in items:
        total += b

Or the *Note sum(): 43f. built-in and a generator expression:

    total = sum(b for a,b in items)

Many uses of *Note reduce(): 2fc. are clearer when written as `for'
loops.

Fredrik Lundh once suggested the following set of rules for refactoring
uses of `lambda':

  1. Write a lambda function.

  2. Write a comment explaining what the heck that lambda does.

  3. Study the comment for a while, and think of a name that captures
     the essence of the comment.

  4. Convert the lambda to a def statement, using that name.

  5. Remove the comment.

I really like these rules, but you’re free to disagree about whether
this lambda-free style is better.


File: python.info,  Node: The itertools module,  Next: The functools module,  Prev: Small functions and the lambda expression,  Up: Functional Programming HOWTO

10.6.7 The itertools module
---------------------------

The *Note itertools: fb. module contains a number of commonly-used
iterators as well as functions for combining several iterators.  This
section will introduce the module’s contents by showing small
examples.

The module’s functions fall into a few broad classes:

   * Functions that create a new iterator based on an existing iterator.

   * Functions for treating an iterator’s elements as function
     arguments.

   * Functions for selecting portions of an iterator’s output.

   * A function for grouping an iterator’s output.

* Menu:

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::


File: python.info,  Node: Creating new iterators,  Next: Calling functions on elements,  Up: The itertools module

10.6.7.1 Creating new iterators
...............................

`itertools.count(n)' returns an infinite stream of integers, increasing
by 1 each time.  You can optionally supply the starting number, which
defaults to 0:

    itertools.count() =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
    itertools.count(10) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

`itertools.cycle(iter)' saves a copy of the contents of a provided
iterable and returns a new iterator that returns its elements from
first to last.  The new iterator will repeat these elements infinitely.

    itertools.cycle([1,2,3,4,5]) =>
      1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

`itertools.repeat(elem, [n])' returns the provided element `n' times, or
returns the element endlessly if `n' is not provided.

    itertools.repeat('abc') =>
      abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
    itertools.repeat('abc', 5) =>
      abc, abc, abc, abc, abc

`itertools.chain(iterA, iterB, ...)' takes an arbitrary number of
iterables as input, and returns all the elements of the first iterator,
then all the elements of the second, and so on, until all of the
iterables have been exhausted.

    itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
      a, b, c, 1, 2, 3

`itertools.izip(iterA, iterB, ...)' takes one element from each
iterable and returns them in a tuple:

    itertools.izip(['a', 'b', 'c'], (1, 2, 3)) =>
      ('a', 1), ('b', 2), ('c', 3)

It’s similar to the built-in *Note zip(): 41e. function, but
doesn’t construct an in-memory list and exhaust all the input
iterators before returning; instead tuples are constructed and returned
only if they’re requested.  (The technical term for this behaviour is
lazy evaluation(1).)

This iterator is intended to be used with iterables that are all of the
same length.  If the iterables are of different lengths, the resulting
stream will be the same length as the shortest iterable.

    itertools.izip(['a', 'b'], (1, 2, 3)) =>
      ('a', 1), ('b', 2)

You should avoid doing this, though, because an element may be taken
from the longer iterators and discarded.  This means you can’t go on
to use the iterators further because you risk skipping a discarded
element.

`itertools.islice(iter, [start], stop, [step])' returns a stream
that’s a slice of the iterator.  With a single `stop' argument, it
will return the first `stop' elements.  If you supply a starting index,
you’ll get `stop-start' elements, and if you supply a value for
`step', elements will be skipped accordingly.  Unlike Python’s string
and list slicing, you can’t use negative values for `start', `stop',
or `step'.

    itertools.islice(range(10), 8) =>
      0, 1, 2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8) =>
      2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8, 2) =>
      2, 4, 6

`itertools.tee(iter, [n])' replicates an iterator; it returns `n'
independent iterators that will all return the contents of the source
iterator.  If you don’t supply a value for `n', the default is 2.
Replicating iterators requires saving some of the contents of the
source iterator, so this can consume significant memory if the iterator
is large and one of the new iterators is consumed more than the others.

    itertools.tee( itertools.count() ) =>
       iterA, iterB

    where iterA ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

    and   iterB ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

---------- Footnotes ----------

(1) http://en.wikipedia.org/wiki/Lazy_evaluation


File: python.info,  Node: Calling functions on elements,  Next: Selecting elements,  Prev: Creating new iterators,  Up: The itertools module

10.6.7.2 Calling functions on elements
......................................

Two functions are used for calling other functions on the contents of an
iterable.

`itertools.imap(f, iterA, iterB, ...)' returns a stream containing
`f(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]),
...':

    itertools.imap(operator.add, [5, 6, 5], [1, 2, 3]) =>
      6, 8, 8

The `operator' module contains a set of functions corresponding to
Python’s operators.  Some examples are `operator.add(a, b)' (adds two
values), `operator.ne(a, b)' (same as `a!=b'), and
`operator.attrgetter('id')' (returns a callable that fetches the `"id"'
attribute).

`itertools.starmap(func, iter)' assumes that the iterable will return a
stream of tuples, and calls `f()' using these tuples as the arguments:

    itertools.starmap(os.path.join,
                      [('/usr', 'bin', 'java'), ('/bin', 'python'),
                       ('/usr', 'bin', 'perl'),('/usr', 'bin', 'ruby')])
    =>
      /usr/bin/java, /bin/python, /usr/bin/perl, /usr/bin/ruby


File: python.info,  Node: Selecting elements,  Next: Grouping elements,  Prev: Calling functions on elements,  Up: The itertools module

10.6.7.3 Selecting elements
...........................

Another group of functions chooses a subset of an iterator’s elements
based on a predicate.

`itertools.ifilter(predicate, iter)' returns all the elements for which
the predicate returns true:

    def is_even(x):
        return (x % 2) == 0

    itertools.ifilter(is_even, itertools.count()) =>
      0, 2, 4, 6, 8, 10, 12, 14, ...

`itertools.ifilterfalse(predicate, iter)' is the opposite, returning all
elements for which the predicate returns false:

    itertools.ifilterfalse(is_even, itertools.count()) =>
      1, 3, 5, 7, 9, 11, 13, 15, ...

`itertools.takewhile(predicate, iter)' returns elements for as long as
the predicate returns true.  Once the predicate returns false, the
iterator will signal the end of its results.

    def less_than_10(x):
        return (x < 10)

    itertools.takewhile(less_than_10, itertools.count()) =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9

    itertools.takewhile(is_even, itertools.count()) =>
      0

`itertools.dropwhile(predicate, iter)' discards elements while the
predicate returns true, and then returns the rest of the iterable’s
results.

    itertools.dropwhile(less_than_10, itertools.count()) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

    itertools.dropwhile(is_even, itertools.count()) =>
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...


File: python.info,  Node: Grouping elements,  Prev: Selecting elements,  Up: The itertools module

10.6.7.4 Grouping elements
..........................

The last function I’ll discuss, `itertools.groupby(iter,
key_func=None)', is the most complicated.  `key_func(elem)' is a
function that can compute a key value for each element returned by the
iterable.  If you don’t supply a key function, the key is simply each
element itself.

`groupby()' collects all the consecutive elements from the underlying
iterable that have the same key value, and returns a stream of 2-tuples
containing a key value and an iterator for the elements with that key.

    city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
                 ('Anchorage', 'AK'), ('Nome', 'AK'),
                 ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
                 ...
                ]

    def get_state ((city, state)):
        return state

    itertools.groupby(city_list, get_state) =>
      ('AL', iterator-1),
      ('AK', iterator-2),
      ('AZ', iterator-3), ...

    where
    iterator-1 =>
      ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
    iterator-2 =>
      ('Anchorage', 'AK'), ('Nome', 'AK')
    iterator-3 =>
      ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

`groupby()' assumes that the underlying iterable’s contents will
already be sorted based on the key.  Note that the returned iterators
also use the underlying iterable, so you have to consume the results of
iterator-1 before requesting iterator-2 and its corresponding key.


File: python.info,  Node: The functools module,  Next: Revision History and Acknowledgements,  Prev: The itertools module,  Up: Functional Programming HOWTO

10.6.8 The functools module
---------------------------

The *Note functools: da. module in Python 2.5 contains some
higher-order functions.  A `higher-order function' takes one or more
functions as input and returns a new function.  The most useful tool in
this module is the *Note functools.partial(): d9f. function.

For programs written in a functional style, you’ll sometimes want to
construct variants of existing functions that have some of the
parameters filled in.  Consider a Python function `f(a, b, c)'; you may
wish to create a new function `g(b, c)' that’s equivalent to `f(1, b,
c)'; you’re filling in a value for one of `f()'’s parameters.  This
is called “partial function application”.

The constructor for `partial' takes the arguments `(function, arg1,
arg2, ... kwarg1=value1, kwarg2=value2)'.  The resulting object is
callable, so you can just call it to invoke `function' with the
filled-in arguments.

Here’s a small but realistic example:

    import functools

    def log (message, subsystem):
        "Write the contents of 'message' to the specified subsystem."
        print '%s: %s' % (subsystem, message)
        ...

    server_log = functools.partial(log, subsystem='server')
    server_log('Unable to open socket')

* Menu:

* The operator module::


File: python.info,  Node: The operator module,  Up: The functools module

10.6.8.1 The operator module
............................

The *Note operator: 127. module was mentioned earlier.  It contains a
set of functions corresponding to Python’s operators.  These
functions are often useful in functional-style code because they save
you from writing trivial functions that perform a single operation.

Some of the functions in this module are:

   * Math operations: `add()', `sub()', `mul()', `div()', `floordiv()',
     `abs()', …

   * Logical operations: `not_()', `truth()'.

   * Bitwise operations: `and_()', `or_()', `invert()'.

   * Comparisons: `eq()', `ne()', `lt()', `le()', `gt()', and `ge()'.

   * Object identity: `is_()', `is_not()'.

Consult the operator module’s documentation for a complete list.


File: python.info,  Node: Revision History and Acknowledgements,  Next: References,  Prev: The functools module,  Up: Functional Programming HOWTO

10.6.9 Revision History and Acknowledgements
--------------------------------------------

The author would like to thank the following people for offering
suggestions, corrections and assistance with various drafts of this
article: Ian Bicking, Nick Coghlan, Nick Efford, Raymond Hettinger, Jim
Jewett, Mike Krell, Leandro Lameiro, Jussi Salmela, Collin Winter,
Blake Winton.

Version 0.1: posted June 30 2006.

Version 0.11: posted July 1 2006.  Typo fixes.

Version 0.2: posted July 10 2006.  Merged genexp and listcomp sections
into one.  Typo fixes.

Version 0.21: Added more references suggested on the tutor mailing list.

Version 0.30: Adds a section on the `functional' module written by
Collin Winter; adds short section on the operator module; a few other
edits.


File: python.info,  Node: References,  Prev: Revision History and Acknowledgements,  Up: Functional Programming HOWTO

10.6.10 References
------------------

* Menu:

* General::
* Python-specific::
* Python documentation::


File: python.info,  Node: General,  Next: Python-specific,  Up: References

10.6.10.1 General
.................

`Structure and Interpretation of Computer Programs', by Harold Abelson
and Gerald Jay Sussman with Julie Sussman.  Full text at
<https://mitpress.mit.edu/sicp/>.  In this classic textbook of computer
science, chapters 2 and 3 discuss the use of sequences and streams to
organize the data flow inside a program.  The book uses Scheme for its
examples, but many of the design approaches described in these chapters
are applicable to functional-style Python code.

<http://www.defmacro.org/ramblings/fp.html>: A general introduction to
functional programming that uses Java examples and has a lengthy
historical introduction.

<https://en.wikipedia.org/wiki/Functional_programming>: General
Wikipedia entry describing functional programming.

<https://en.wikipedia.org/wiki/Coroutine>: Entry for coroutines.

<https://en.wikipedia.org/wiki/Currying>: Entry for the concept of
currying.


File: python.info,  Node: Python-specific,  Next: Python documentation,  Prev: General,  Up: References

10.6.10.2 Python-specific
.........................

<http://gnosis.cx/TPiP/>: The first chapter of David Mertz’s book
`Text Processing in Python' discusses functional programming for text
processing, in the section titled “Utilizing Higher-Order Functions in
Text Processing”.

Mertz also wrote a 3-part series of articles on functional programming
for IBM’s DeveloperWorks site; see

part 1(1), part 2(2), and part 3(3),

---------- Footnotes ----------

(1) https://www.ibm.com/developerworks/linux/library/l-prog/index.html

(2) https://www.ibm.com/developerworks/linux/library/l-prog2/index.html

(3) https://www.ibm.com/developerworks/linux/library/l-prog3/index.html


File: python.info,  Node: Python documentation,  Prev: Python-specific,  Up: References

10.6.10.3 Python documentation
..............................

Documentation for the *Note itertools: fb. module.

Documentation for the *Note operator: 127. module.

PEP 289(1): “Generator Expressions”

PEP 342(2): “Coroutines via Enhanced Generators” describes the new
generator features in Python 2.5.

---------- Footnotes ----------

(1) https://www.python.org/dev/peps/pep-0289

(2) https://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Logging HOWTO,  Next: Logging Cookbook,  Prev: Functional Programming HOWTO,  Up: Python HOWTOs

10.7 Logging HOWTO
==================

Author: Vinay Sajip <vinay_sajip at red-dove dot com>

* Menu:

* Basic Logging Tutorial::
* Advanced Logging Tutorial::
* Logging Levels: Logging Levels<2>.
* Useful Handlers::
* Exceptions raised during logging::
* Using arbitrary objects as messages::
* Optimization::


File: python.info,  Node: Basic Logging Tutorial,  Next: Advanced Logging Tutorial,  Up: Logging HOWTO

10.7.1 Basic Logging Tutorial
-----------------------------

Logging is a means of tracking events that happen when some software
runs. The software’s developer adds logging calls to their code to
indicate that certain events have occurred. An event is described by a
descriptive message which can optionally contain variable data (i.e.
data that is potentially different for each occurrence of the event).
Events also have an importance which the developer ascribes to the
event; the importance can also be called the `level' or `severity'.

* Menu:

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::


File: python.info,  Node: When to use logging,  Next: A simple example,  Up: Basic Logging Tutorial

10.7.1.1 When to use logging
............................

Logging provides a set of convenience functions for simple logging
usage. These are *Note debug(): 12fd, *Note info(): 132a, *Note
warning(): 133a, *Note error(): 133b. and *Note critical(): 133c. To
determine when to use logging, see the table below, which states, for
each of a set of common tasks, the best tool to use for it.

Task you want to perform                  The best tool for the task
------------------------------------------------------------------------------------- 
Display console output for ordinary       *Note print(): 31f.
usage of a command line script or program 
Report events that occur during normal    *Note logging.info(): 132a. (or *Note
operation of a program (e.g.  for status  logging.debug(): 12fd. for very detailed
monitoring or fault investigation)        output for diagnostic purposes)
Issue a warning regarding a particular    *Note warnings.warn(): 4dc. in library
runtime event                             code if the issue is avoidable and the
                                          client application should be modified to
                                          eliminate the warning
                                          
                                          *Note logging.warning(): 133a. if there is
                                          nothing the client application can do
                                          about the situation, but the event should
                                          still be noted
Report an error regarding a particular    Raise an exception
runtime event                             
Report suppression of an error without    *Note logging.error(): 133b, *Note
raising an exception (e.g.  error         logging.exception(): 133d. or *Note
handler in a long-running server process) logging.critical(): 133c. as appropriate
                                          for the specific error and application
                                          domain

The logging functions are named after the level or severity of the
events they are used to track. The standard levels and their
applicability are described below (in increasing order of severity):

Level              When it’s used
--------------------------------------------------------------------- 
`DEBUG'            Detailed information, typically of interest only
                   when diagnosing problems.
`INFO'             Confirmation that things are working as expected.
`WARNING'          An indication that something unexpected
                   happened, or indicative of some problem in the
                   near future (e.g. ‘disk space low’).  The
                   software is still working as expected.
`ERROR'            Due to a more serious problem, the software has
                   not been able to perform some function.
`CRITICAL'         A serious error, indicating that the program
                   itself may be unable to continue running.

The default level is `WARNING', which means that only events of this
level and above will be tracked, unless the logging package is
configured to do otherwise.

Events that are tracked can be handled in different ways. The simplest
way of handling tracked events is to print them to the console. Another
common way is to write them to a disk file.


File: python.info,  Node: A simple example,  Next: Logging to a file,  Prev: When to use logging,  Up: Basic Logging Tutorial

10.7.1.2 A simple example
.........................

A very simple example is:

    import logging
    logging.warning('Watch out!')  # will print a message to the console
    logging.info('I told you so')  # will not print anything

If you type these lines into a script and run it, you’ll see:

    WARNING:root:Watch out!

printed out on the console. The `INFO' message doesn’t appear because
the default level is `WARNING'. The printed message includes the
indication of the level and the description of the event provided in
the logging call, i.e.  ‘Watch out!’. Don’t worry about the
‘root’ part for now: it will be explained later. The actual output
can be formatted quite flexibly if you need that; formatting options
will also be explained later.


File: python.info,  Node: Logging to a file,  Next: Logging from multiple modules,  Prev: A simple example,  Up: Basic Logging Tutorial

10.7.1.3 Logging to a file
..........................

A very common situation is that of recording logging events in a file,
so let’s look at that next. Be sure to try the following in a
newly-started Python interpreter, and don’t just continue from the
session described above:

    import logging
    logging.basicConfig(filename='example.log',level=logging.DEBUG)
    logging.debug('This message should go to the log file')
    logging.info('So should this')
    logging.warning('And this, too')

And now if we open the file and look at what we have, we should find
the log messages:

    DEBUG:root:This message should go to the log file
    INFO:root:So should this
    WARNING:root:And this, too

This example also shows how you can set the logging level which acts as
the threshold for tracking. In this case, because we set the threshold
to `DEBUG', all of the messages were printed.

If you want to set the logging level from a command-line option such as:

    --log=INFO

and you have the value of the parameter passed for `--log' in some
variable `loglevel', you can use:

    getattr(logging, loglevel.upper())

to get the value which you’ll pass to *Note basicConfig(): 133f. via
the `level' argument. You may want to error check any user input value,
perhaps as in the following example:

    # assuming loglevel is bound to the string value obtained from the
    # command line argument. Convert to upper case to allow the user to
    # specify --log=DEBUG or --log=debug
    numeric_level = getattr(logging, loglevel.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % loglevel)
    logging.basicConfig(level=numeric_level, ...)

The call to *Note basicConfig(): 133f. should come `before' any calls
to *Note debug(): 12fd, *Note info(): 132a. etc. As it’s intended as
a one-off simple configuration facility, only the first call will
actually do anything: subsequent calls are effectively no-ops.

If you run the above script several times, the messages from successive
runs are appended to the file `example.log'. If you want each run to
start afresh, not remembering the messages from earlier runs, you can
specify the `filemode' argument, by changing the call in the above
example to:

    logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)

The output will be the same as before, but the log file is no longer
appended to, so the messages from earlier runs are lost.


File: python.info,  Node: Logging from multiple modules,  Next: Logging variable data,  Prev: Logging to a file,  Up: Basic Logging Tutorial

10.7.1.4 Logging from multiple modules
......................................

If your program consists of multiple modules, here’s an example of
how you could organize logging in it:

    # myapp.py
    import logging
    import mylib

    def main():
        logging.basicConfig(filename='myapp.log', level=logging.INFO)
        logging.info('Started')
        mylib.do_something()
        logging.info('Finished')

    if __name__ == '__main__':
        main()

    # mylib.py
    import logging

    def do_something():
        logging.info('Doing something')

If you run `myapp.py', you should see this in `myapp.log':

    INFO:root:Started
    INFO:root:Doing something
    INFO:root:Finished

which is hopefully what you were expecting to see. You can generalize
this to multiple modules, using the pattern in `mylib.py'. Note that
for this simple usage pattern, you won’t know, by looking in the log
file, `where' in your application your messages came from, apart from
looking at the event description. If you want to track the location of
your messages, you’ll need to refer to the documentation beyond the
tutorial level – see *Note Advanced Logging Tutorial: 12f0.


File: python.info,  Node: Logging variable data,  Next: Changing the format of displayed messages,  Prev: Logging from multiple modules,  Up: Basic Logging Tutorial

10.7.1.5 Logging variable data
..............................

To log variable data, use a format string for the event description
message and append the variable data as arguments. For example:

    import logging
    logging.warning('%s before you %s', 'Look', 'leap!')

will display:

    WARNING:root:Look before you leap!

As you can see, merging of variable data into the event description
message uses the old, %-style of string formatting. This is for
backwards compatibility: the logging package pre-dates newer formatting
options such as *Note str.format(): 1d4. and *Note string.Template:
5ae. These newer formatting options `are' supported, but exploring them
is outside the scope of this tutorial.


File: python.info,  Node: Changing the format of displayed messages,  Next: Displaying the date/time in messages,  Prev: Logging variable data,  Up: Basic Logging Tutorial

10.7.1.6 Changing the format of displayed messages
..................................................

To change the format which is used to display messages, you need to
specify the format you want to use:

    import logging
    logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
    logging.debug('This message should appear on the console')
    logging.info('So should this')
    logging.warning('And this, too')

which would print:

    DEBUG:This message should appear on the console
    INFO:So should this
    WARNING:And this, too

Notice that the ‘root’ which appeared in earlier examples has
disappeared. For a full set of things that can appear in format
strings, you can refer to the documentation for *Note LogRecord
attributes: 1321, but for simple usage, you just need the `levelname'
(severity), `message' (event description, including variable data) and
perhaps to display when the event occurred. This is described in the
next section.


File: python.info,  Node: Displaying the date/time in messages,  Next: Next Steps,  Prev: Changing the format of displayed messages,  Up: Basic Logging Tutorial

10.7.1.7 Displaying the date/time in messages
.............................................

To display the date and time of an event, you would place
‘%(asctime)s’ in your format string:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s')
    logging.warning('is when this event was logged.')

which should print something like this:

    2010-12-12 11:41:42,612 is when this event was logged.

The default format for date/time display (shown above) is ISO8601. If
you need more control over the formatting of the date/time, provide a
`datefmt' argument to `basicConfig', as in this example:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
    logging.warning('is when this event was logged.')

which would display something like this:

    12/12/2010 11:46:36 AM is when this event was logged.

The format of the `datefmt' argument is the same as supported by *Note
time.strftime(): 3d8.


File: python.info,  Node: Next Steps,  Prev: Displaying the date/time in messages,  Up: Basic Logging Tutorial

10.7.1.8 Next Steps
...................

That concludes the basic tutorial. It should be enough to get you up and
running with logging. There’s a lot more that the logging package
offers, but to get the best out of it, you’ll need to invest a little
more of your time in reading the following sections. If you’re ready
for that, grab some of your favourite beverage and carry on.

If your logging needs are simple, then use the above examples to
incorporate logging into your own scripts, and if you run into problems
or don’t understand something, please post a question on the
comp.lang.python Usenet group (available at
<https://groups.google.com/group/comp.lang.python>) and you should
receive help before too long.

Still here? You can carry on reading the next few sections, which
provide a slightly more advanced/in-depth tutorial than the basic one
above. After that, you can take a look at the *Note Logging Cookbook:
12f1.


File: python.info,  Node: Advanced Logging Tutorial,  Next: Logging Levels<2>,  Prev: Basic Logging Tutorial,  Up: Logging HOWTO

10.7.2 Advanced Logging Tutorial
--------------------------------

The logging library takes a modular approach and offers several
categories of components: loggers, handlers, filters, and formatters.

   * Loggers expose the interface that application code directly uses.

   * Handlers send the log records (created by loggers) to the
     appropriate destination.

   * Filters provide a finer grained facility for determining which log
     records to output.

   * Formatters specify the layout of log records in the final output.

Log event information is passed between loggers, handlers, filters and
formatters in a *Note LogRecord: 130b. instance.

Logging is performed by calling methods on instances of the *Note
Logger: 1dd.  class (hereafter called `loggers'). Each instance has a
name, and they are conceptually arranged in a namespace hierarchy using
dots (periods) as separators. For example, a logger named ‘scan’ is
the parent of loggers ‘scan.text’, ‘scan.html’ and
‘scan.pdf’. Logger names can be anything you want, and indicate the
area of an application in which a logged message originates.

A good convention to use when naming loggers is to use a module-level
logger, in each module which uses logging, named as follows:

    logger = logging.getLogger(__name__)

This means that logger names track the package/module hierarchy, and
it’s intuitively obvious where events are logged just from the logger
name.

The root of the hierarchy of loggers is called the root logger.
That’s the logger used by the functions *Note debug(): 12fd, *Note
info(): 132a, *Note warning(): 133a, *Note error(): 133b. and *Note
critical(): 133c, which just call the same-named method of the root
logger. The functions and the methods have the same signatures. The
root logger’s name is printed as ‘root’ in the logged output.

It is, of course, possible to log messages to different destinations.
Support is included in the package for writing log messages to files,
HTTP GET/POST locations, email via SMTP, generic sockets, or
OS-specific logging mechanisms such as syslog or the Windows NT event
log. Destinations are served by `handler' classes. You can create your
own log destination class if you have special requirements not met by
any of the built-in handler classes.

By default, no destination is set for any logging messages. You can
specify a destination (such as console or file) by using *Note
basicConfig(): 133f. as in the tutorial examples. If you call the
functions  *Note debug(): 12fd, *Note info(): 132a, *Note warning():
133a, *Note error(): 133b. and *Note critical(): 133c, they will check
to see if no destination is set; and if one is not set, they will set a
destination of the console (`sys.stderr') and a default format for the
displayed message before delegating to the root logger to do the actual
message output.

The default format set by *Note basicConfig(): 133f. for messages is:

    severity:logger name:message

You can change this by passing a format string to *Note basicConfig():
133f. with the `format' keyword argument. For all options regarding how
a format string is constructed, see *Note Formatter Objects: 131f.

* Menu:

* Logging Flow::
* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::


File: python.info,  Node: Logging Flow,  Next: Loggers,  Up: Advanced Logging Tutorial

10.7.2.1 Logging Flow
.....................

The flow of log event information in loggers and handlers is
illustrated in the following diagram.


File: python.info,  Node: Loggers,  Next: Handlers,  Prev: Logging Flow,  Up: Advanced Logging Tutorial

10.7.2.2 Loggers
................

*Note Logger: 1dd. objects have a threefold job.  First, they expose
several methods to application code so that applications can log
messages at runtime.  Second, logger objects determine which log
messages to act upon based upon severity (the default filtering
facility) or filter objects.  Third, logger objects pass along relevant
log messages to all interested log handlers.

The most widely used methods on logger objects fall into two categories:
configuration and message sending.

These are the most common configuration methods:

   * *Note Logger.setLevel(): 12f6. specifies the lowest-severity log
     message a logger will handle, where debug is the lowest built-in
     severity level and critical is the highest built-in severity.  For
     example, if the severity level is INFO, the logger will handle
     only INFO, WARNING, ERROR, and CRITICAL messages and will ignore
     DEBUG messages.

   * *Note Logger.addHandler(): 1306. and *Note Logger.removeHandler():
     1307. add and remove handler objects from the logger object.
     Handlers are covered in more detail in *Note Handlers: 2fbc.

   * *Note Logger.addFilter(): 1303. and *Note Logger.removeFilter():
     1304. add and remove filter objects from the logger object.
     Filters are covered in more detail in *Note Filter Objects: 1326.

You don’t need to always call these methods on every logger you
create. See the last two paragraphs in this section.

With the logger object configured, the following methods create log
messages:

   * *Note Logger.debug(): 12fa, *Note Logger.info(): 12fc, *Note
     Logger.warning(): 12fe, *Note Logger.error(): 12ff, and *Note
     Logger.critical(): 1300. all create log records with a message and
     a level that corresponds to their respective method names. The
     message is actually a format string, which may contain the
     standard string substitution syntax of `%s', `%d', `%f', and so
     on.  The rest of their arguments is a list of objects that
     correspond with the substitution fields in the message.  With
     regard to `**kwargs', the logging methods care only about a
     keyword of `exc_info' and use it to determine whether to log
     exception information.

   * *Note Logger.exception(): 1302. creates a log message similar to
     *Note Logger.error(): 12ff.  The difference is that *Note
     Logger.exception(): 1302. dumps a stack trace along with it.  Call
     this method only from an exception handler.

   * *Note Logger.log(): 1301. takes a log level as an explicit
     argument.  This is a little more verbose for logging messages than
     using the log level convenience methods listed above, but this is
     how to log at custom log levels.

*Note getLogger(): 12f4. returns a reference to a logger instance with
the specified name if it is provided, or `root' if not.  The names are
period-separated hierarchical structures.  Multiple calls to *Note
getLogger(): 12f4. with the same name will return a reference to the
same logger object.  Loggers that are further down in the hierarchical
list are children of loggers higher up in the list.  For example, given
a logger with a name of `foo', loggers with names of `foo.bar',
`foo.bar.baz', and `foo.bam' are all descendants of `foo'.

Loggers have a concept of `effective level'. If a level is not
explicitly set on a logger, the level of its parent is used instead as
its effective level.  If the parent has no explicit level set, `its'
parent is examined, and so on - all ancestors are searched until an
explicitly set level is found. The root logger always has an explicit
level set (`WARNING' by default). When deciding whether to process an
event, the effective level of the logger is used to determine whether
the event is passed to the logger’s handlers.

Child loggers propagate messages up to the handlers associated with
their ancestor loggers. Because of this, it is unnecessary to define
and configure handlers for all the loggers an application uses. It is
sufficient to configure handlers for a top-level logger and create
child loggers as needed.  (You can, however, turn off propagation by
setting the `propagate' attribute of a logger to `False'.)


File: python.info,  Node: Handlers,  Next: Formatters,  Prev: Loggers,  Up: Advanced Logging Tutorial

10.7.2.3 Handlers
.................

`Handler' objects are responsible for dispatching the appropriate log
messages (based on the log messages’ severity) to the handler’s
specified destination.  *Note Logger: 1dd. objects can add zero or more
handler objects to themselves with an *Note addHandler(): 1306. method.
As an example scenario, an application may want to send all log
messages to a log file, all log messages of error or higher to stdout,
and all messages of critical to an email address. This scenario
requires three individual handlers where each handler is responsible
for sending messages of a specific severity to a specific location.

The standard library includes quite a few handler types (see *Note
Useful Handlers: 2fbe.); the tutorials use mainly *Note StreamHandler:
1344. and *Note FileHandler: 1367. in its examples.

There are very few methods in a handler for application developers to
concern themselves with.  The only handler methods that seem relevant
for application developers who are using the built-in handler objects
(that is, not creating custom handlers) are the following configuration
methods:

   * The *Note setLevel(): 1313. method, just as in logger objects,
     specifies the lowest severity that will be dispatched to the
     appropriate destination.  Why are there two `setLevel()' methods?
     The level set in the logger determines which severity of messages
     it will pass to its handlers.  The level set in each handler
     determines which messages that handler will send on.

   * *Note setFormatter(): 1314. selects a Formatter object for this
     handler to use.

   * *Note addFilter(): 1315. and *Note removeFilter(): 1316.
     respectively configure and deconfigure filter objects on handlers.

Application code should not directly instantiate and use instances of
`Handler'.  Instead, the `Handler' class is a base class that defines
the interface that all handlers should have and establishes some
default behavior that child classes can use (or override).


File: python.info,  Node: Formatters,  Next: Configuring Logging,  Prev: Handlers,  Up: Advanced Logging Tutorial

10.7.2.4 Formatters
...................

Formatter objects configure the final order, structure, and contents of
the log message.  Unlike the base `logging.Handler' class, application
code may instantiate formatter classes, although you could likely
subclass the formatter if your application needs special behavior.  The
constructor takes two optional arguments – a message format string
and a date format string.

 -- Method: logging.Formatter.__init__ (fmt=None, datefmt=None)

If there is no message format string, the default is to use the raw
message.  If there is no date format string, the default date format is:

    %Y-%m-%d %H:%M:%S

with the milliseconds tacked on at the end.

The message format string uses `%(<dictionary key>)s' styled string
substitution; the possible keys are documented in *Note LogRecord
attributes: 1321.

The following message format string will log the time in a
human-readable format, the severity of the message, and the contents of
the message, in that order:

    '%(asctime)s - %(levelname)s - %(message)s'

Formatters use a user-configurable function to convert the creation
time of a record to a tuple. By default, *Note time.localtime(): b19.
is used; to change this for a particular formatter instance, set the
`converter' attribute of the instance to a function with the same
signature as *Note time.localtime(): b19. or *Note time.gmtime(): b7f.
To change it for all formatters, for example if you want all logging
times to be shown in GMT, set the `converter' attribute in the
Formatter class (to `time.gmtime' for GMT display).


File: python.info,  Node: Configuring Logging,  Next: What happens if no configuration is provided,  Prev: Formatters,  Up: Advanced Logging Tutorial

10.7.2.5 Configuring Logging
............................

Programmers can configure logging in three ways:

  1. Creating loggers, handlers, and formatters explicitly using Python
     code that calls the configuration methods listed above.

  2. Creating a logging config file and reading it using the *Note
     fileConfig(): 134d.  function.

  3. Creating a dictionary of configuration information and passing it
     to the *Note dictConfig(): 134b. function.

For the reference documentation on the last two options, see *Note
Configuration functions: 1d9.  The following example configures a very
simple logger, a console handler, and a simple formatter using Python
code:

    import logging

    # create logger
    logger = logging.getLogger('simple_example')
    logger.setLevel(logging.DEBUG)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)

    # create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # add formatter to ch
    ch.setFormatter(formatter)

    # add ch to logger
    logger.addHandler(ch)

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Running this module from the command line produces the following output:

    $ python simple_logging_module.py
    2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message
    2005-03-19 15:10:26,620 - simple_example - INFO - info message
    2005-03-19 15:10:26,695 - simple_example - WARNING - warn message
    2005-03-19 15:10:26,697 - simple_example - ERROR - error message
    2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message

The following Python module creates a logger, handler, and formatter
nearly identical to those in the example listed above, with the only
difference being the names of the objects:

    import logging
    import logging.config

    logging.config.fileConfig('logging.conf')

    # create logger
    logger = logging.getLogger('simpleExample')

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Here is the logging.conf file:

    [loggers]
    keys=root,simpleExample

    [handlers]
    keys=consoleHandler

    [formatters]
    keys=simpleFormatter

    [logger_root]
    level=DEBUG
    handlers=consoleHandler

    [logger_simpleExample]
    level=DEBUG
    handlers=consoleHandler
    qualname=simpleExample
    propagate=0

    [handler_consoleHandler]
    class=StreamHandler
    level=DEBUG
    formatter=simpleFormatter
    args=(sys.stdout,)

    [formatter_simpleFormatter]
    format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
    datefmt=

The output is nearly identical to that of the non-config-file-based
example:

    $ python simple_logging_config.py
    2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message
    2005-03-19 15:38:55,979 - simpleExample - INFO - info message
    2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message
    2005-03-19 15:38:56,055 - simpleExample - ERROR - error message
    2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message

You can see that the config file approach has a few advantages over the
Python code approach, mainly separation of configuration and code and
the ability of noncoders to easily modify the logging properties.

     Warning: The *Note fileConfig(): 134d. function takes a default
     parameter, `disable_existing_loggers', which defaults to `True'
     for reasons of backward compatibility. This may or may not be what
     you want, since it will cause any loggers existing before the
     *Note fileConfig(): 134d. call to be disabled unless they (or an
     ancestor) are explicitly named in the configuration.  Please refer
     to the reference documentation for more information, and specify
     `False' for this parameter if you wish.

     The dictionary passed to *Note dictConfig(): 134b. can also
     specify a Boolean value with key `disable_existing_loggers', which
     if not specified explicitly in the dictionary also defaults to
     being interpreted as `True'.  This leads to the logger-disabling
     behaviour described above, which may not be what you want - in
     which case, provide the key explicitly with a value of `False'.

Note that the class names referenced in config files need to be either
relative to the logging module, or absolute values which can be
resolved using normal import mechanisms. Thus, you could use either
*Note WatchedFileHandler: 1379. (relative to the logging module) or
`mypackage.mymodule.MyHandler' (for a class defined in package
`mypackage' and module `mymodule', where `mypackage' is available on
the Python import path).

In Python 2.7, a new means of configuring logging has been introduced,
using dictionaries to hold configuration information. This provides a
superset of the functionality of the config-file-based approach
outlined above, and is the recommended configuration method for new
applications and deployments. Because a Python dictionary is used to
hold configuration information, and since you can populate that
dictionary using different means, you have more options for
configuration. For example, you can use a configuration file in JSON
format, or, if you have access to YAML processing functionality, a file
in YAML format, to populate the configuration dictionary. Or, of
course, you can construct the dictionary in Python code, receive it in
pickled form over a socket, or use whatever approach makes sense for
your application.

Here’s an example of the same configuration as above, in YAML format
for the new dictionary-based approach:

    version: 1
    formatters:
      simple:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        level: DEBUG
        formatter: simple
        stream: ext://sys.stdout
    loggers:
      simpleExample:
        level: DEBUG
        handlers: [console]
        propagate: no
    root:
      level: DEBUG
      handlers: [console]

For more information about logging using a dictionary, see *Note
Configuration functions: 1d9.


File: python.info,  Node: What happens if no configuration is provided,  Next: Configuring Logging for a Library,  Prev: Configuring Logging,  Up: Advanced Logging Tutorial

10.7.2.6 What happens if no configuration is provided
.....................................................

If no logging configuration is provided, it is possible to have a
situation where a logging event needs to be output, but no handlers can
be found to output the event. The behaviour of the logging package in
these circumstances is dependent on the Python version.

For Python 2.x, the behaviour is as follows:

   * If `logging.raiseExceptions' is `False' (production mode), the
     event is silently dropped.

   * If `logging.raiseExceptions' is `True' (development mode), a
     message ‘No handlers could be found for logger X.Y.Z’ is
     printed once.


File: python.info,  Node: Configuring Logging for a Library,  Prev: What happens if no configuration is provided,  Up: Advanced Logging Tutorial

10.7.2.7 Configuring Logging for a Library
..........................................

When developing a library which uses logging, you should take care to
document how the library uses logging - for example, the names of
loggers used. Some consideration also needs to be given to its logging
configuration.  If the using application does not configure logging,
and library code makes logging calls, then (as described in the
previous section) an error message will be printed to `sys.stderr'.

If for some reason you `don’t' want this message printed in the
absence of any logging configuration, you can attach a do-nothing
handler to the top-level logger for your library. This avoids the
message being printed, since a handler will be always be found for the
library’s events: it just doesn’t produce any output. If the
library user configures logging for application use, presumably that
configuration will add some handlers, and if levels are suitably
configured then logging calls made in library code will send output to
those handlers, as normal.

A do-nothing handler is included in the logging package: *Note
NullHandler: 1368. (since Python 2.7). An instance of this handler
could be added to the top-level logger of the logging namespace used by
the library (`if' you want to prevent an error message being output to
`sys.stderr' in the absence of logging configuration). If all logging
by a library `foo' is done using loggers with names matching
‘foo.x’, ‘foo.x.y’, etc. then the code:

    import logging
    logging.getLogger('foo').addHandler(logging.NullHandler())

should have the desired effect. If an organisation produces a number of
libraries, then the logger name specified can be ‘orgname.foo’
rather than just ‘foo’.

     Note: It is strongly advised that you `do not add any handlers
     other than' *Note NullHandler: 1368. `to your library’s
     loggers'. This is because the configuration of handlers is the
     prerogative of the application developer who uses your library.
     The application developer knows their target audience and what
     handlers are most appropriate for their application: if you add
     handlers ‘under the hood’, you might well interfere with their
     ability to carry out unit tests and deliver logs which suit their
     requirements.


File: python.info,  Node: Logging Levels<2>,  Next: Useful Handlers,  Prev: Advanced Logging Tutorial,  Up: Logging HOWTO

10.7.3 Logging Levels
---------------------

The numeric values of logging levels are given in the following table.
These are primarily of interest if you want to define your own levels,
and need them to have specific values relative to the predefined
levels. If you define a level with the same numeric value, it
overwrites the predefined value; the predefined name is lost.

Level              Numeric value
--------------------------------------- 
`CRITICAL'         50
`ERROR'            40
`WARNING'          30
`INFO'             20
`DEBUG'            10
`NOTSET'           0

Levels can also be associated with loggers, being set either by the
developer or through loading a saved logging configuration. When a
logging method is called on a logger, the logger compares its own level
with the level associated with the method call. If the logger’s level
is higher than the method call’s, no logging message is actually
generated. This is the basic mechanism controlling the verbosity of
logging output.

Logging messages are encoded as instances of the *Note LogRecord: 130b.
class. When a logger decides to actually log an event, a *Note
LogRecord: 130b. instance is created from the logging message.

Logging messages are subjected to a dispatch mechanism through the use
of `handlers', which are instances of subclasses of the `Handler'
class. Handlers are responsible for ensuring that a logged message (in
the form of a *Note LogRecord: 130b.) ends up in a particular location
(or set of locations) which is useful for the target audience for that
message (such as end users, support desk staff, system administrators,
developers). Handlers are passed *Note LogRecord: 130b. instances
intended for particular destinations. Each logger can have zero, one or
more handlers associated with it (via the *Note addHandler(): 1306.
method of *Note Logger: 1dd.). In addition to any handlers directly
associated with a logger, `all handlers associated with all ancestors
of the logger' are called to dispatch the message (unless the
`propagate' flag for a logger is set to a false value, at which point
the passing to ancestor handlers stops).

Just as for loggers, handlers can have levels associated with them. A
handler’s level acts as a filter in the same way as a logger’s
level does. If a handler decides to actually dispatch an event, the
*Note emit(): 131d. method is used to send the message to its
destination. Most user-defined subclasses of `Handler' will need to
override this *Note emit(): 131d.

* Menu:

* Custom Levels::


File: python.info,  Node: Custom Levels,  Up: Logging Levels<2>

10.7.3.1 Custom Levels
......................

Defining your own levels is possible, but should not be necessary, as
the existing levels have been chosen on the basis of practical
experience.  However, if you are convinced that you need custom levels,
great care should be exercised when doing this, and it is possibly `a
very bad idea to define custom levels if you are developing a library'.
That’s because if multiple library authors all define their own
custom levels, there is a chance that the logging output from such
multiple libraries used together will be difficult for the using
developer to control and/or interpret, because a given numeric value
might mean different things for different libraries.


File: python.info,  Node: Useful Handlers,  Next: Exceptions raised during logging,  Prev: Logging Levels<2>,  Up: Logging HOWTO

10.7.4 Useful Handlers
----------------------

In addition to the base `Handler' class, many useful subclasses are
provided:

  1. *Note StreamHandler: 1344. instances send messages to streams
     (file-like objects).

  2. *Note FileHandler: 1367. instances send messages to disk files.

  3. `BaseRotatingHandler' is the base class for handlers that rotate
     log files at a certain point. It is not meant to be  instantiated
     directly. Instead, use *Note RotatingFileHandler: 1358. or *Note
     TimedRotatingFileHandler: 1381.

  4. *Note RotatingFileHandler: 1358. instances send messages to disk
     files, with support for maximum log file sizes and log file
     rotation.

  5. *Note TimedRotatingFileHandler: 1381. instances send messages to
     disk files, rotating the log file at certain timed intervals.

  6. *Note SocketHandler: 1386. instances send messages to TCP/IP
     sockets.

  7. *Note DatagramHandler: 1390. instances send messages to UDP
     sockets.

  8. *Note SMTPHandler: 13a4. instances send messages to a designated
     email address.

  9. *Note SysLogHandler: 1da. instances send messages to a Unix syslog
     daemon, possibly on a remote machine.

 10. *Note NTEventLogHandler: 139c. instances send messages to a
     Windows NT/2000/XP event log.

 11. *Note MemoryHandler: 1361. instances send messages to a buffer in
     memory, which is flushed whenever specific criteria are met.

 12. *Note HTTPHandler: 13b4. instances send messages to an HTTP server
     using either `GET' or `POST' semantics.

 13. *Note WatchedFileHandler: 1379. instances watch the file they are
     logging to. If the file changes, it is closed and reopened using
     the file name. This handler is only useful on Unix-like systems;
     Windows does not support the underlying mechanism used.

 14. *Note NullHandler: 1368. instances do nothing with error messages.
     They are used by library developers who want to use logging, but
     want to avoid the ‘No handlers could be found for logger XXX’
     message which can be displayed if the library user has not
     configured logging. See *Note Configuring Logging for a Library:
     1376. for more information.

New in version 2.7: The *Note NullHandler: 1368. class.

The *Note NullHandler: 1368, *Note StreamHandler: 1344. and *Note
FileHandler: 1367.  classes are defined in the core logging package.
The other handlers are defined in a sub- module, *Note
logging.handlers: 104. (There is also another sub-module, *Note
logging.config: 103, for configuration functionality.)

Logged messages are formatted for presentation through instances of the
*Note Formatter: 12fb. class. They are initialized with a format string
suitable for use with the % operator and a dictionary.

For formatting multiple messages in a batch, instances of
`BufferingFormatter' can be used. In addition to the format string
(which is applied to each message in the batch), there is provision for
header and trailer format strings.

When filtering based on logger level and/or handler level is not enough,
instances of *Note Filter: 1328. can be added to both *Note Logger:
1dd. and `Handler' instances (through their *Note addFilter(): 1315.
method).  Before deciding to process a message further, both loggers
and handlers consult all their filters for permission. If any filter
returns a false value, the message is not processed further.

The basic *Note Filter: 1328. functionality allows filtering by
specific logger name. If this feature is used, messages sent to the
named logger and its children are allowed through the filter, and all
others dropped.


File: python.info,  Node: Exceptions raised during logging,  Next: Using arbitrary objects as messages,  Prev: Useful Handlers,  Up: Logging HOWTO

10.7.5 Exceptions raised during logging
---------------------------------------

The logging package is designed to swallow exceptions which occur while
logging in production. This is so that errors which occur while
handling logging events - such as logging misconfiguration, network or
other similar errors - do not cause the application using logging to
terminate prematurely.

`SystemExit' and `KeyboardInterrupt' exceptions are never swallowed.
Other exceptions which occur during the *Note emit(): 131d. method of a
`Handler' subclass are passed to its *Note handleError(): 131c.  method.

The default implementation of *Note handleError(): 131c. in `Handler'
checks to see if a module-level variable, `raiseExceptions', is set. If
set, a traceback is printed to *Note sys.stderr: 672. If not set, the
exception is swallowed.

     Note: The default value of `raiseExceptions' is `True'. This is
     because during development, you typically want to be notified of
     any exceptions that occur. It’s advised that you set
     `raiseExceptions' to `False' for production usage.


File: python.info,  Node: Using arbitrary objects as messages,  Next: Optimization,  Prev: Exceptions raised during logging,  Up: Logging HOWTO

10.7.6 Using arbitrary objects as messages
------------------------------------------

In the preceding sections and examples, it has been assumed that the
message passed when logging the event is a string. However, this is not
the only possibility. You can pass an arbitrary object as a message,
and its *Note __str__(): 4b2. method will be called when the logging
system needs to convert it to a string representation. In fact, if you
want to, you can avoid computing a string representation altogether -
for example, the *Note SocketHandler: 1386. emits an event by pickling
it and sending it over the wire.


File: python.info,  Node: Optimization,  Prev: Using arbitrary objects as messages,  Up: Logging HOWTO

10.7.7 Optimization
-------------------

Formatting of message arguments is deferred until it cannot be avoided.
However, computing the arguments passed to the logging method can also
be expensive, and you may want to avoid doing it if the logger will
just throw away your event. To decide what to do, you can call the
*Note isEnabledFor(): 12f8. method which takes a level argument and
returns true if the event would be created by the Logger for that level
of call.  You can write code like this:

    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Message with %s, %s', expensive_func1(),
                                            expensive_func2())

so that if the logger’s threshold is set above `DEBUG', the calls to
`expensive_func1()' and `expensive_func2()' are never made.

     Note: In some cases, *Note isEnabledFor(): 12f8. can itself be more
     expensive than you’d like (e.g. for deeply nested loggers where
     an explicit level is only set high up in the logger hierarchy). In
     such cases (or if you want to avoid calling a method in tight
     loops), you can cache the result of a call to *Note
     isEnabledFor(): 12f8. in a local or instance variable, and use
     that instead of calling the method each time. Such a cached value
     would only need to be recomputed when the logging configuration
     changes dynamically while the application is running (which is not
     all that common).

There are other optimizations which can be made for specific
applications which need more precise control over what logging
information is collected. Here’s a list of things you can do to avoid
processing during logging which you don’t need:

What you don’t want to collect                    How to avoid collecting it
------------------------------------------------------------------------------------------------- 
Information about where calls were made from.       Set `logging._srcfile' to `None'.  This
                                                    avoids calling *Note sys._getframe(): 4ea,
                                                    which may help to speed up your code in
                                                    environments like PyPy (which can’t speed
                                                    up code that uses *Note sys._getframe():
                                                    4ea.).
Threading information.                              Set `logging.logThreads' to `0'.
Process information.                                Set `logging.logProcesses' to `0'.

Also note that the core logging module only includes the basic
handlers. If you don’t import *Note logging.handlers: 104. and *Note
logging.config: 103, they won’t take up any memory.

See also
........

Module *Note logging: 102.
     API reference for the logging module.

Module *Note logging.config: 103.
     Configuration API for the logging module.

Module *Note logging.handlers: 104.
     Useful handlers included with the logging module.

*Note A logging cookbook: 12f1.


File: python.info,  Node: Logging Cookbook,  Next: Regular Expression HOWTO,  Prev: Logging HOWTO,  Up: Python HOWTOs

10.8 Logging Cookbook
=====================

Author: Vinay Sajip <vinay_sajip at red-dove dot com>

This page contains a number of recipes related to logging, which have
been found useful in the past.

* Menu:

* Using logging in multiple modules::
* Logging from multiple threads::
* Multiple handlers and formatters::
* Logging to multiple destinations::
* Configuration server example::
* Sending and receiving logging events across a network::
* Adding contextual information to your logging output::
* Logging to a single file from multiple processes::
* Using file rotation::
* An example dictionary-based configuration::
* Inserting a BOM into messages sent to a SysLogHandler::
* Implementing structured logging::
* Customizing handlers with dictConfig(): Customizing handlers with dictConfig.
* Configuring filters with dictConfig(): Configuring filters with dictConfig.
* Customized exception formatting::
* Speaking logging messages::
* Buffering logging messages and outputting them conditionally::
* Formatting times using UTC (GMT) via configuration: Formatting times using UTC GMT via configuration.
* Using a context manager for selective logging::


File: python.info,  Node: Using logging in multiple modules,  Next: Logging from multiple threads,  Up: Logging Cookbook

10.8.1 Using logging in multiple modules
----------------------------------------

Multiple calls to `logging.getLogger('someLogger')' return a reference
to the same logger object.  This is true not only within the same
module, but also across modules as long as it is in the same Python
interpreter process.  It is true for references to the same object;
additionally, application code can define and configure a parent logger
in one module and create (but not configure) a child logger in a
separate module, and all logger calls to the child will pass up to the
parent.  Here is a main module:

    import logging
    import auxiliary_module

    # create logger with 'spam_application'
    logger = logging.getLogger('spam_application')
    logger.setLevel(logging.DEBUG)
    # create file handler which logs even debug messages
    fh = logging.FileHandler('spam.log')
    fh.setLevel(logging.DEBUG)
    # create console handler with a higher log level
    ch = logging.StreamHandler()
    ch.setLevel(logging.ERROR)
    # create formatter and add it to the handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    # add the handlers to the logger
    logger.addHandler(fh)
    logger.addHandler(ch)

    logger.info('creating an instance of auxiliary_module.Auxiliary')
    a = auxiliary_module.Auxiliary()
    logger.info('created an instance of auxiliary_module.Auxiliary')
    logger.info('calling auxiliary_module.Auxiliary.do_something')
    a.do_something()
    logger.info('finished auxiliary_module.Auxiliary.do_something')
    logger.info('calling auxiliary_module.some_function()')
    auxiliary_module.some_function()
    logger.info('done with auxiliary_module.some_function()')

Here is the auxiliary module:

    import logging

    # create logger
    module_logger = logging.getLogger('spam_application.auxiliary')

    class Auxiliary:
        def __init__(self):
            self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')
            self.logger.info('creating an instance of Auxiliary')

        def do_something(self):
            self.logger.info('doing something')
            a = 1 + 1
            self.logger.info('done doing something')

    def some_function():
        module_logger.info('received a call to "some_function"')

The output looks like this:

    2005-03-23 23:47:11,663 - spam_application - INFO -
       creating an instance of auxiliary_module.Auxiliary
    2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -
       creating an instance of Auxiliary
    2005-03-23 23:47:11,665 - spam_application - INFO -
       created an instance of auxiliary_module.Auxiliary
    2005-03-23 23:47:11,668 - spam_application - INFO -
       calling auxiliary_module.Auxiliary.do_something
    2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -
       doing something
    2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -
       done doing something
    2005-03-23 23:47:11,670 - spam_application - INFO -
       finished auxiliary_module.Auxiliary.do_something
    2005-03-23 23:47:11,671 - spam_application - INFO -
       calling auxiliary_module.some_function()
    2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -
       received a call to 'some_function'
    2005-03-23 23:47:11,673 - spam_application - INFO -
       done with auxiliary_module.some_function()


File: python.info,  Node: Logging from multiple threads,  Next: Multiple handlers and formatters,  Prev: Using logging in multiple modules,  Up: Logging Cookbook

10.8.2 Logging from multiple threads
------------------------------------

Logging from multiple threads requires no special effort. The following
example shows logging from the main (initIal) thread and another thread:

    import logging
    import threading
    import time

    def worker(arg):
        while not arg['stop']:
            logging.debug('Hi from myfunc')
            time.sleep(0.5)

    def main():
        logging.basicConfig(level=logging.DEBUG, format='%(relativeCreated)6d %(threadName)s %(message)s')
        info = {'stop': False}
        thread = threading.Thread(target=worker, args=(info,))
        thread.start()
        while True:
            try:
                logging.debug('Hello from main')
                time.sleep(0.75)
            except KeyboardInterrupt:
                info['stop'] = True
                break
        thread.join()

    if __name__ == '__main__':
        main()

When run, the script should print something like the following:

       0 Thread-1 Hi from myfunc
       3 MainThread Hello from main
     505 Thread-1 Hi from myfunc
     755 MainThread Hello from main
    1007 Thread-1 Hi from myfunc
    1507 MainThread Hello from main
    1508 Thread-1 Hi from myfunc
    2010 Thread-1 Hi from myfunc
    2258 MainThread Hello from main
    2512 Thread-1 Hi from myfunc
    3009 MainThread Hello from main
    3013 Thread-1 Hi from myfunc
    3515 Thread-1 Hi from myfunc
    3761 MainThread Hello from main
    4017 Thread-1 Hi from myfunc
    4513 MainThread Hello from main
    4518 Thread-1 Hi from myfunc

This shows the logging output interspersed as one might expect. This
approach works for more threads than shown here, of course.


File: python.info,  Node: Multiple handlers and formatters,  Next: Logging to multiple destinations,  Prev: Logging from multiple threads,  Up: Logging Cookbook

10.8.3 Multiple handlers and formatters
---------------------------------------

Loggers are plain Python objects.  The *Note addHandler(): 1306. method
has no minimum or maximum quota for the number of handlers you may add.
Sometimes it will be beneficial for an application to log all messages
of all severities to a text file while simultaneously logging errors or
above to the console.  To set this up, simply configure the appropriate
handlers.  The logging calls in the application code will remain
unchanged.  Here is a slight modification to the previous simple
module-based configuration example:

    import logging

    logger = logging.getLogger('simple_example')
    logger.setLevel(logging.DEBUG)
    # create file handler which logs even debug messages
    fh = logging.FileHandler('spam.log')
    fh.setLevel(logging.DEBUG)
    # create console handler with a higher log level
    ch = logging.StreamHandler()
    ch.setLevel(logging.ERROR)
    # create formatter and add it to the handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)
    # add the handlers to logger
    logger.addHandler(ch)
    logger.addHandler(fh)

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Notice that the ‘application’ code does not care about multiple
handlers.  All that changed was the addition and configuration of a new
handler named `fh'.

The ability to create new handlers with higher- or lower-severity
filters can be very helpful when writing and testing an application.
Instead of using many `print' statements for debugging, use
`logger.debug': Unlike the print statements, which you will have to
delete or comment out later, the logger.debug statements can remain
intact in the source code and remain dormant until you need them again.
At that time, the only change that needs to happen is to modify the
severity level of the logger and/or handler to debug.


File: python.info,  Node: Logging to multiple destinations,  Next: Configuration server example,  Prev: Multiple handlers and formatters,  Up: Logging Cookbook

10.8.4 Logging to multiple destinations
---------------------------------------

Let’s say you want to log to console and file with different message
formats and in differing circumstances. Say you want to log messages
with levels of DEBUG and higher to file, and those messages at level
INFO and higher to the console.  Let’s also assume that the file
should contain timestamps, but the console messages should not.
Here’s how you can achieve this:

    import logging

    # set up logging to file - see previous section for more details
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                        datefmt='%m-%d %H:%M',
                        filename='/temp/myapp.log',
                        filemode='w')
    # define a Handler which writes INFO messages or higher to the sys.stderr
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    # set a format which is simpler for console use
    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
    # tell the handler to use this format
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger('').addHandler(console)

    # Now, we can log to the root logger, or any other logger. First the root...
    logging.info('Jackdaws love my big sphinx of quartz.')

    # Now, define a couple of other loggers which might represent areas in your
    # application:

    logger1 = logging.getLogger('myapp.area1')
    logger2 = logging.getLogger('myapp.area2')

    logger1.debug('Quick zephyrs blow, vexing daft Jim.')
    logger1.info('How quickly daft jumping zebras vex.')
    logger2.warning('Jail zesty vixen who grabbed pay from quack.')
    logger2.error('The five boxing wizards jump quickly.')

When you run this, on the console you will see

    root        : INFO     Jackdaws love my big sphinx of quartz.
    myapp.area1 : INFO     How quickly daft jumping zebras vex.
    myapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.
    myapp.area2 : ERROR    The five boxing wizards jump quickly.

and in the file you will see something like

    10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.
    10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.
    10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.
    10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.
    10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.

As you can see, the DEBUG message only shows up in the file. The other
messages are sent to both destinations.

This example uses console and file handlers, but you can use any number
and combination of handlers you choose.


File: python.info,  Node: Configuration server example,  Next: Sending and receiving logging events across a network,  Prev: Logging to multiple destinations,  Up: Logging Cookbook

10.8.5 Configuration server example
-----------------------------------

Here is an example of a module using the logging configuration server:

    import logging
    import logging.config
    import time
    import os

    # read initial config file
    logging.config.fileConfig('logging.conf')

    # create and start listener on port 9999
    t = logging.config.listen(9999)
    t.start()

    logger = logging.getLogger('simpleExample')

    try:
        # loop through logging calls to see the difference
        # new configurations make, until Ctrl+C is pressed
        while True:
            logger.debug('debug message')
            logger.info('info message')
            logger.warn('warn message')
            logger.error('error message')
            logger.critical('critical message')
            time.sleep(5)
    except KeyboardInterrupt:
        # cleanup
        logging.config.stopListening()
        t.join()

And here is a script that takes a filename and sends that file to the
server, properly preceded with the binary-encoded length, as the new
logging configuration:

    #!/usr/bin/env python
    import socket, sys, struct

    with open(sys.argv[1], 'rb') as f:
        data_to_send = f.read()

    HOST = 'localhost'
    PORT = 9999
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    print('connecting...')
    s.connect((HOST, PORT))
    print('sending config...')
    s.send(struct.pack('>L', len(data_to_send)))
    s.send(data_to_send)
    s.close()
    print('complete')


File: python.info,  Node: Sending and receiving logging events across a network,  Next: Adding contextual information to your logging output,  Prev: Configuration server example,  Up: Logging Cookbook

10.8.6 Sending and receiving logging events across a network
------------------------------------------------------------

Let’s say you want to send logging events across a network, and
handle them at the receiving end. A simple way of doing this is
attaching a `SocketHandler' instance to the root logger at the sending
end:

    import logging, logging.handlers

    rootLogger = logging.getLogger('')
    rootLogger.setLevel(logging.DEBUG)
    socketHandler = logging.handlers.SocketHandler('localhost',
                        logging.handlers.DEFAULT_TCP_LOGGING_PORT)
    # don't bother with a formatter, since a socket handler sends the event as
    # an unformatted pickle
    rootLogger.addHandler(socketHandler)

    # Now, we can log to the root logger, or any other logger. First the root...
    logging.info('Jackdaws love my big sphinx of quartz.')

    # Now, define a couple of other loggers which might represent areas in your
    # application:

    logger1 = logging.getLogger('myapp.area1')
    logger2 = logging.getLogger('myapp.area2')

    logger1.debug('Quick zephyrs blow, vexing daft Jim.')
    logger1.info('How quickly daft jumping zebras vex.')
    logger2.warning('Jail zesty vixen who grabbed pay from quack.')
    logger2.error('The five boxing wizards jump quickly.')

At the receiving end, you can set up a receiver using the *Note
SocketServer: 15d.  module. Here is a basic working example:

    import pickle
    import logging
    import logging.handlers
    import SocketServer
    import struct


    class LogRecordStreamHandler(SocketServer.StreamRequestHandler):
        """Handler for a streaming logging request.

        This basically logs the record using whatever logging policy is
        configured locally.
        """

        def handle(self):
            """
            Handle multiple requests - each expected to be a 4-byte length,
            followed by the LogRecord in pickle format. Logs the record
            according to whatever policy is configured locally.
            """
            while True:
                chunk = self.connection.recv(4)
                if len(chunk) < 4:
                    break
                slen = struct.unpack('>L', chunk)[0]
                chunk = self.connection.recv(slen)
                while len(chunk) < slen:
                    chunk = chunk + self.connection.recv(slen - len(chunk))
                obj = self.unPickle(chunk)
                record = logging.makeLogRecord(obj)
                self.handleLogRecord(record)

        def unPickle(self, data):
            return pickle.loads(data)

        def handleLogRecord(self, record):
            # if a name is specified, we use the named logger rather than the one
            # implied by the record.
            if self.server.logname is not None:
                name = self.server.logname
            else:
                name = record.name
            logger = logging.getLogger(name)
            # N.B. EVERY record gets logged. This is because Logger.handle
            # is normally called AFTER logger-level filtering. If you want
            # to do filtering, do it at the client end to save wasting
            # cycles and network bandwidth!
            logger.handle(record)

    class LogRecordSocketReceiver(SocketServer.ThreadingTCPServer):
        """
        Simple TCP socket-based logging receiver suitable for testing.
        """

        allow_reuse_address = 1

        def __init__(self, host='localhost',
                     port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,
                     handler=LogRecordStreamHandler):
            SocketServer.ThreadingTCPServer.__init__(self, (host, port), handler)
            self.abort = 0
            self.timeout = 1
            self.logname = None

        def serve_until_stopped(self):
            import select
            abort = 0
            while not abort:
                rd, wr, ex = select.select([self.socket.fileno()],
                                           [], [],
                                           self.timeout)
                if rd:
                    self.handle_request()
                abort = self.abort

    def main():
        logging.basicConfig(
            format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')
        tcpserver = LogRecordSocketReceiver()
        print('About to start TCP server...')
        tcpserver.serve_until_stopped()

    if __name__ == '__main__':
        main()

First run the server, and then the client. On the client side, nothing
is printed on the console; on the server side, you should see something
like:

    About to start TCP server...
       59 root            INFO     Jackdaws love my big sphinx of quartz.
       59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.
       69 myapp.area1     INFO     How quickly daft jumping zebras vex.
       69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.
       69 myapp.area2     ERROR    The five boxing wizards jump quickly.

Note that there are some security issues with pickle in some scenarios.
If these affect you, you can use an alternative serialization scheme by
overriding the *Note makePickle(): 138b. method and implementing your
alternative there, as well as adapting the above script to use your
alternative serialization.


File: python.info,  Node: Adding contextual information to your logging output,  Next: Logging to a single file from multiple processes,  Prev: Sending and receiving logging events across a network,  Up: Logging Cookbook

10.8.7 Adding contextual information to your logging output
-----------------------------------------------------------

Sometimes you want logging output to contain contextual information in
addition to the parameters passed to the logging call. For example, in a
networked application, it may be desirable to log client-specific
information in the log (e.g. remote client’s username, or IP
address). Although you could use the `extra' parameter to achieve this,
it’s not always convenient to pass the information in this way. While
it might be tempting to create *Note Logger: 1dd. instances on a
per-connection basis, this is not a good idea because these instances
are not garbage collected. While this is not a problem in practice,
when the number of *Note Logger: 1dd. instances is dependent on the
level of granularity you want to use in logging an application, it could
be hard to manage if the number of *Note Logger: 1dd. instances becomes
effectively unbounded.

* Menu:

* Using LoggerAdapters to impart contextual information::
* Using Filters to impart contextual information::


File: python.info,  Node: Using LoggerAdapters to impart contextual information,  Next: Using Filters to impart contextual information,  Up: Adding contextual information to your logging output

10.8.7.1 Using LoggerAdapters to impart contextual information
..............................................................

An easy way in which you can pass contextual information to be output
along with logging event information is to use the *Note LoggerAdapter:
1df. class.  This class is designed to look like a *Note Logger: 1dd,
so that you can call *Note debug(): 12fd, *Note info(): 132a, *Note
warning(): 133a, *Note error(): 133b, *Note exception(): 133d, *Note
critical(): 133c. and *Note log(): 133e. These methods have the same
signatures as their counterparts in *Note Logger: 1dd, so you can use
the two types of instances interchangeably.

When you create an instance of *Note LoggerAdapter: 1df, you pass it a
*Note Logger: 1dd. instance and a dict-like object which contains your
contextual information. When you call one of the logging methods on an
instance of *Note LoggerAdapter: 1df, it delegates the call to the
underlying instance of *Note Logger: 1dd. passed to its constructor,
and arranges to pass the contextual information in the delegated call.
Here’s a snippet from the code of *Note LoggerAdapter: 1df.:

    def debug(self, msg, *args, **kwargs):
        """
        Delegate a debug call to the underlying logger, after adding
        contextual information from this adapter instance.
        """
        msg, kwargs = self.process(msg, kwargs)
        self.logger.debug(msg, *args, **kwargs)

The *Note process(): 1335. method of *Note LoggerAdapter: 1df. is where
the contextual information is added to the logging output. It’s
passed the message and keyword arguments of the logging call, and it
passes back (potentially) modified versions of these to use in the call
to the underlying logger. The default implementation of this method
leaves the message alone, but inserts an ‘extra’ key in the keyword
argument whose value is the dict-like object passed to the constructor.
Of course, if you had passed an ‘extra’ keyword argument in the
call to the adapter, it will be silently overwritten.

The advantage of using ‘extra’ is that the values in the dict-like
object are merged into the *Note LogRecord: 130b. instance’s
__dict__, allowing you to use customized strings with your *Note
Formatter: 12fb. instances which know about the keys of the dict-like
object. If you need a different method, e.g. if you want to prepend or
append the contextual information to the message string, you just need
to subclass *Note LoggerAdapter: 1df. and override *Note process():
1335. to do what you need. Here is a simple example:

    class CustomAdapter(logging.LoggerAdapter):
        """
        This example adapter expects the passed in dict-like object to have a
        'connid' key, whose value in brackets is prepended to the log message.
        """
        def process(self, msg, kwargs):
            return '[%s] %s' % (self.extra['connid'], msg), kwargs

which you can use like this:

    logger = logging.getLogger(__name__)
    adapter = CustomAdapter(logger, {'connid': some_conn_id})

Then any events that you log to the adapter will have the value of
`some_conn_id' prepended to the log messages.

* Menu:

* Using objects other than dicts to pass contextual information::


File: python.info,  Node: Using objects other than dicts to pass contextual information,  Up: Using LoggerAdapters to impart contextual information

10.8.7.2 Using objects other than dicts to pass contextual information
......................................................................

You don’t need to pass an actual dict to a *Note LoggerAdapter: 1df.
- you could pass an instance of a class which implements `__getitem__'
and `__iter__' so that it looks like a dict to logging. This would be
useful if you want to generate values dynamically (whereas the values
in a dict would be constant).


File: python.info,  Node: Using Filters to impart contextual information,  Prev: Using LoggerAdapters to impart contextual information,  Up: Adding contextual information to your logging output

10.8.7.3 Using Filters to impart contextual information
.......................................................

You can also add contextual information to log output using a
user-defined *Note Filter: 1328. `Filter' instances are allowed to
modify the `LogRecords' passed to them, including adding additional
attributes which can then be output using a suitable format string, or
if needed a custom *Note Formatter: 12fb.

For example in a web application, the request being processed (or at
least, the interesting parts of it) can be stored in a threadlocal
(*Note threading.local: 1572.) variable, and then accessed from a
`Filter' to add, say, information from the request - say, the remote IP
address and remote user’s username - to the `LogRecord', using the
attribute names ‘ip’ and ‘user’ as in the `LoggerAdapter'
example above. In that case, the same format string can be used to get
similar output to that shown above. Here’s an example script:

    import logging
    from random import choice

    class ContextFilter(logging.Filter):
        """
        This is a filter which injects contextual information into the log.

        Rather than use actual contextual information, we just use random
        data in this demo.
        """

        USERS = ['jim', 'fred', 'sheila']
        IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']

        def filter(self, record):

            record.ip = choice(ContextFilter.IPS)
            record.user = choice(ContextFilter.USERS)
            return True

    if __name__ == '__main__':
        levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')
        a1 = logging.getLogger('a.b.c')
        a2 = logging.getLogger('d.e.f')

        f = ContextFilter()
        a1.addFilter(f)
        a2.addFilter(f)
        a1.debug('A debug message')
        a1.info('An info message with %s', 'some parameters')
        for x in range(10):
            lvl = choice(levels)
            lvlname = logging.getLevelName(lvl)
            a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')

which, when run, produces something like:

    2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message
    2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters


File: python.info,  Node: Logging to a single file from multiple processes,  Next: Using file rotation,  Prev: Adding contextual information to your logging output,  Up: Logging Cookbook

10.8.8 Logging to a single file from multiple processes
-------------------------------------------------------

Although logging is thread-safe, and logging to a single file from
multiple threads in a single process `is' supported, logging to a
single file from `multiple processes' is `not' supported, because there
is no standard way to serialize access to a single file across multiple
processes in Python. If you need to log to a single file from multiple
processes, one way of doing this is to have all the processes log to a
*Note SocketHandler: 1386, and have a separate process which implements
a socket server which reads from the socket and logs to file. (If you
prefer, you can dedicate one thread in one of the existing processes to
perform this function.)  *Note This section: 2fd3. documents this
approach in more detail and includes a working socket receiver which
can be used as a starting point for you to adapt in your own
applications.

If you are using a recent version of Python which includes the *Note
multiprocessing: 11a. module, you could write your own handler which
uses the *Note Lock: 165b. class from this module to serialize access
to the file from your processes. The existing *Note FileHandler: 1367.
and subclasses do not make use of *Note multiprocessing: 11a. at
present, though they may do so in the future. Note that at present, the
*Note multiprocessing: 11a. module does not provide working lock
functionality on all platforms (see
<https://bugs.python.org/issue3770>).


File: python.info,  Node: Using file rotation,  Next: An example dictionary-based configuration,  Prev: Logging to a single file from multiple processes,  Up: Logging Cookbook

10.8.9 Using file rotation
--------------------------

Sometimes you want to let a log file grow to a certain size, then open
a new file and log to that. You may want to keep a certain number of
these files, and when that many files have been created, rotate the
files so that the number of files and the size of the files both remain
bounded. For this usage pattern, the logging package provides a *Note
RotatingFileHandler: 1358.:

    import glob
    import logging
    import logging.handlers

    LOG_FILENAME = 'logging_rotatingfile_example.out'

    # Set up a specific logger with our desired output level
    my_logger = logging.getLogger('MyLogger')
    my_logger.setLevel(logging.DEBUG)

    # Add the log message handler to the logger
    handler = logging.handlers.RotatingFileHandler(
                  LOG_FILENAME, maxBytes=20, backupCount=5)

    my_logger.addHandler(handler)

    # Log some messages
    for i in range(20):
        my_logger.debug('i = %d' % i)

    # See what files are created
    logfiles = glob.glob('%s*' % LOG_FILENAME)

    for filename in logfiles:
        print(filename)

The result should be 6 separate files, each with part of the log
history for the application:

    logging_rotatingfile_example.out
    logging_rotatingfile_example.out.1
    logging_rotatingfile_example.out.2
    logging_rotatingfile_example.out.3
    logging_rotatingfile_example.out.4
    logging_rotatingfile_example.out.5

The most current file is always `logging_rotatingfile_example.out', and
each time it reaches the size limit it is renamed with the suffix `.1'.
Each of the existing backup files is renamed to increment the suffix
(`.1' becomes `.2', etc.)  and the `.6' file is erased.

Obviously this example sets the log length much too small as an extreme
example.  You would want to set `maxBytes' to an appropriate value.


File: python.info,  Node: An example dictionary-based configuration,  Next: Inserting a BOM into messages sent to a SysLogHandler,  Prev: Using file rotation,  Up: Logging Cookbook

10.8.10 An example dictionary-based configuration
-------------------------------------------------

Below is an example of a logging configuration dictionary - it’s
taken from the documentation on the Django project(1).  This dictionary
is passed to *Note dictConfig(): 134b. to put the configuration into
effect:

    LOGGING = {
        'version': 1,
        'disable_existing_loggers': True,
        'formatters': {
            'verbose': {
                'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'
            },
            'simple': {
                'format': '%(levelname)s %(message)s'
            },
        },
        'filters': {
            'special': {
                '()': 'project.logging.SpecialFilter',
                'foo': 'bar',
            }
        },
        'handlers': {
            'null': {
                'level':'DEBUG',
                'class':'django.utils.log.NullHandler',
            },
            'console':{
                'level':'DEBUG',
                'class':'logging.StreamHandler',
                'formatter': 'simple'
            },
            'mail_admins': {
                'level': 'ERROR',
                'class': 'django.utils.log.AdminEmailHandler',
                'filters': ['special']
            }
        },
        'loggers': {
            'django': {
                'handlers':['null'],
                'propagate': True,
                'level':'INFO',
            },
            'django.request': {
                'handlers': ['mail_admins'],
                'level': 'ERROR',
                'propagate': False,
            },
            'myproject.custom': {
                'handlers': ['console', 'mail_admins'],
                'level': 'INFO',
                'filters': ['special']
            }
        }
    }

For more information about this configuration, you can see the relevant
section(2) of the Django documentation.

---------- Footnotes ----------

(1)
https://docs.djangoproject.com/en/1.9/topics/logging/#configuring-logging

(2)
https://docs.djangoproject.com/en/1.9/topics/logging/#configuring-logging


File: python.info,  Node: Inserting a BOM into messages sent to a SysLogHandler,  Next: Implementing structured logging,  Prev: An example dictionary-based configuration,  Up: Logging Cookbook

10.8.11 Inserting a BOM into messages sent to a SysLogHandler
-------------------------------------------------------------

RFC 5424(1) requires that a Unicode message be sent to a syslog daemon
as a set of bytes which have the following structure: an optional
pure-ASCII component, followed by a UTF-8 Byte Order Mark (BOM),
followed by Unicode encoded using UTF-8. (See the relevant section of
the specification(2).)

In Python 2.6 and 2.7, code was added to *Note SysLogHandler: 1da. to
insert a BOM into the message, but unfortunately, it was implemented
incorrectly, with the BOM appearing at the beginning of the message and
hence not allowing any pure-ASCII component to appear before it.

As this behaviour is broken, the incorrect BOM insertion code is being
removed from Python 2.7.4 and later. However, it is not being replaced,
and if you want to produce RFC 5424-compliant messages which include a
BOM, an optional pure-ASCII sequence before it and arbitrary Unicode
after it, encoded using UTF-8, then you need to do the following:

  1. Attach a *Note Formatter: 12fb. instance to your *Note
     SysLogHandler: 1da. instance, with a format string such as:

         u'ASCII section\ufeffUnicode section'

     The Unicode code point `u'\ufeff'', when encoded using UTF-8, will
     be encoded as a UTF-8 BOM – the byte-string `'\xef\xbb\xbf''.

  2. Replace the ASCII section with whatever placeholders you like, but
     make sure that the data that appears in there after substitution
     is always ASCII (that way, it will remain unchanged after UTF-8
     encoding).

  3. Replace the Unicode section with whatever placeholders you like;
     if the data which appears there after substitution contains
     characters outside the ASCII range, that’s fine – it will be
     encoded using UTF-8.

If the formatted message is Unicode, it `will' be encoded using UTF-8
encoding by `SysLogHandler'. If you follow the above rules, you should
be able to produce RFC 5424-compliant messages. If you don’t, logging
may not complain, but your messages will not be RFC 5424-compliant, and
your syslog daemon may complain.

---------- Footnotes ----------

(1) https://tools.ietf.org/html/rfc5424

(2) https://tools.ietf.org/html/rfc5424#section-6


File: python.info,  Node: Implementing structured logging,  Next: Customizing handlers with dictConfig,  Prev: Inserting a BOM into messages sent to a SysLogHandler,  Up: Logging Cookbook

10.8.12 Implementing structured logging
---------------------------------------

Although most logging messages are intended for reading by humans, and
thus not readily machine-parseable, there might be cirumstances where
you want to output messages in a structured format which `is' capable
of being parsed by a program (without needing complex regular
expressions to parse the log message). This is straightforward to
achieve using the logging package. There are a number of ways in which
this could be achieved, but the following is a simple approach which
uses JSON to serialise the event in a machine-parseable manner:

    import json
    import logging

    class StructuredMessage(object):
        def __init__(self, message, **kwargs):
            self.message = message
            self.kwargs = kwargs

        def __str__(self):
            return '%s >>> %s' % (self.message, json.dumps(self.kwargs))

    _ = StructuredMessage   # optional, to improve readability

    logging.basicConfig(level=logging.INFO, format='%(message)s')
    logging.info(_('message 1', foo='bar', bar='baz', num=123, fnum=123.456))

If the above script is run, it prints:

    message 1 >>> {"fnum": 123.456, "num": 123, "bar": "baz", "foo": "bar"}

Note that the order of items might be different according to the
version of Python used.

If you need more specialised processing, you can use a custom JSON
encoder, as in the following complete example:

    from __future__ import unicode_literals

    import json
    import logging

    # This next bit is to ensure the script runs unchanged on 2.x and 3.x
    try:
        unicode
    except NameError:
        unicode = str

    class Encoder(json.JSONEncoder):
        def default(self, o):
            if isinstance(o, set):
                return tuple(o)
            elif isinstance(o, unicode):
                return o.encode('unicode_escape').decode('ascii')
            return super(Encoder, self).default(o)

    class StructuredMessage(object):
        def __init__(self, message, **kwargs):
            self.message = message
            self.kwargs = kwargs

        def __str__(self):
            s = Encoder().encode(self.kwargs)
            return '%s >>> %s' % (self.message, s)

    _ = StructuredMessage   # optional, to improve readability

    def main():
        logging.basicConfig(level=logging.INFO, format='%(message)s')
        logging.info(_('message 1', set_value=set([1, 2, 3]), snowman='\u2603'))

    if __name__ == '__main__':
        main()

When the above script is run, it prints:

    message 1 >>> {"snowman": "\u2603", "set_value": [1, 2, 3]}

Note that the order of items might be different according to the
version of Python used.


File: python.info,  Node: Customizing handlers with dictConfig,  Next: Configuring filters with dictConfig,  Prev: Implementing structured logging,  Up: Logging Cookbook

10.8.13 Customizing handlers with `dictConfig()'
------------------------------------------------

There are times when you want to customize logging handlers in
particular ways, and if you use *Note dictConfig(): 134b. you may be
able to do this without subclassing. As an example, consider that you
may want to set the ownership of a log file. On POSIX, this is easily
done using *Note os.chown(): 1176, but the file handlers in the stdlib
don’t offer built-in support. You can customize handler creation
using a plain function such as:

    def owned_file_handler(filename, mode='a', encoding=None, owner=None):
        if owner:
            import os, pwd, grp
            # convert user and group names to uid and gid
            uid = pwd.getpwnam(owner[0]).pw_uid
            gid = grp.getgrnam(owner[1]).gr_gid
            owner = (uid, gid)
            if not os.path.exists(filename):
                open(filename, 'a').close()
            os.chown(filename, *owner)
        return logging.FileHandler(filename, mode, encoding)

You can then specify, in a logging configuration passed to *Note
dictConfig(): 134b, that a logging handler be created by calling this
function:

    LOGGING = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'default': {
                'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
            },
        },
        'handlers': {
            'file':{
                # The values below are popped from this dictionary and
                # used to create the handler, set the handler's level and
                # its formatter.
                '()': owned_file_handler,
                'level':'DEBUG',
                'formatter': 'default',
                # The values below are passed to the handler creator callable
                # as keyword arguments.
                'owner': ['pulse', 'pulse'],
                'filename': 'chowntest.log',
                'mode': 'w',
                'encoding': 'utf-8',
            },
        },
        'root': {
            'handlers': ['file'],
            'level': 'DEBUG',
        },
    }

In this example I am setting the ownership using the `pulse' user and
group, just for the purposes of illustration. Putting it together into
a working script, `chowntest.py':

    import logging, logging.config, os, shutil

    def owned_file_handler(filename, mode='a', encoding=None, owner=None):
        if owner:
            if not os.path.exists(filename):
                open(filename, 'a').close()
            shutil.chown(filename, *owner)
        return logging.FileHandler(filename, mode, encoding)

    LOGGING = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'default': {
                'format': '%(asctime)s %(levelname)s %(name)s %(message)s'
            },
        },
        'handlers': {
            'file':{
                # The values below are popped from this dictionary and
                # used to create the handler, set the handler's level and
                # its formatter.
                '()': owned_file_handler,
                'level':'DEBUG',
                'formatter': 'default',
                # The values below are passed to the handler creator callable
                # as keyword arguments.
                'owner': ['pulse', 'pulse'],
                'filename': 'chowntest.log',
                'mode': 'w',
                'encoding': 'utf-8',
            },
        },
        'root': {
            'handlers': ['file'],
            'level': 'DEBUG',
        },
    }

    logging.config.dictConfig(LOGGING)
    logger = logging.getLogger('mylogger')
    logger.debug('A debug message')

To run this, you will probably need to run as `root':

    $ sudo python3.3 chowntest.py
    $ cat chowntest.log
    2013-11-05 09:34:51,128 DEBUG mylogger A debug message
    $ ls -l chowntest.log
    -rw-r--r-- 1 pulse pulse 55 2013-11-05 09:34 chowntest.log

Note that this example uses Python 3.3 because that’s where
`shutil.chown()' makes an appearance. This approach should work with
any Python version that supports *Note dictConfig(): 134b. - namely,
Python 2.7, 3.2 or later. With pre-3.3 versions, you would need to
implement the actual ownership change using e.g.  *Note os.chown():
1176.

In practice, the handler-creating function may be in a utility module
somewhere in your project. Instead of the line in the configuration:

    '()': owned_file_handler,

you could use e.g.:

    '()': 'ext://project.util.owned_file_handler',

where `project.util' can be replaced with the actual name of the package
where the function resides. In the above working script, using
`'ext://__main__.owned_file_handler'' should work. Here, the actual
callable is resolved by *Note dictConfig(): 134b. from the `ext://'
specification.

This example hopefully also points the way to how you could implement
other types of file change - e.g. setting specific POSIX permission
bits - in the same way, using *Note os.chmod(): e57.

Of course, the approach could also be extended to types of handler
other than a *Note FileHandler: 1367. - for example, one of the
rotating file handlers, or a different type of handler altogether.


File: python.info,  Node: Configuring filters with dictConfig,  Next: Customized exception formatting,  Prev: Customizing handlers with dictConfig,  Up: Logging Cookbook

10.8.14 Configuring filters with `dictConfig()'
-----------------------------------------------

You `can' configure filters using *Note dictConfig(): 134b, though it
might not be obvious at first glance how to do it (hence this recipe).
Since *Note Filter: 1328. is the only filter class included in the
standard library, and it is unlikely to cater to many requirements
(it’s only there as a base class), you will typically need to define
your own *Note Filter: 1328.  subclass with an overridden *Note
filter(): 1329. method. To do this, specify the `()' key in the
configuration dictionary for the filter, specifying a callable which
will be used to create the filter (a class is the most obvious, but you
can provide any callable which returns a *Note Filter: 1328. instance).
Here is a complete example:

    import logging
    import logging.config
    import sys

    class MyFilter(logging.Filter):
        def __init__(self, param=None):
            self.param = param

        def filter(self, record):
            if self.param is None:
                allow = True
            else:
                allow = self.param not in record.msg
            if allow:
                record.msg = 'changed: ' + record.msg
            return allow

    LOGGING = {
        'version': 1,
        'filters': {
            'myfilter': {
                '()': MyFilter,
                'param': 'noshow',
            }
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'filters': ['myfilter']
            }
        },
        'root': {
            'level': 'DEBUG',
            'handlers': ['console']
        },
    }

    if __name__ == '__main__':
        logging.config.dictConfig(LOGGING)
        logging.debug('hello')
        logging.debug('hello - noshow')

This example shows how you can pass configuration data to the callable
which constructs the instance, in the form of keyword parameters. When
run, the above script will print:

    changed: hello

which shows that the filter is working as configured.

A couple of extra points to note:

   * If you can’t refer to the callable directly in the configuration
     (e.g. if it lives in a different module, and you can’t import it
     directly where the configuration dictionary is), you can use the
     form `ext://...' as described in *Note Access to external objects:
     135e. For example, you could have used the text
     `'ext://__main__.MyFilter'' instead of `MyFilter' in the above
     example.

   * As well as for filters, this technique can also be used to
     configure custom handlers and formatters. See *Note User-defined
     objects: 1357. for more information on how logging supports using
     user-defined objects in its configuration, and see the other
     cookbook recipe *Note Customizing handlers with dictConfig():
     2fe0. above.


File: python.info,  Node: Customized exception formatting,  Next: Speaking logging messages,  Prev: Configuring filters with dictConfig,  Up: Logging Cookbook

10.8.15 Customized exception formatting
---------------------------------------

There might be times when you want to do customized exception
formatting - for argument’s sake, let’s say you want exactly one
line per logged event, even when exception information is present. You
can do this with a custom formatter class, as shown in the following
example:

    import logging

    class OneLineExceptionFormatter(logging.Formatter):
        def formatException(self, exc_info):
            """
            Format an exception so that it prints on a single line.
            """
            result = super(OneLineExceptionFormatter, self).formatException(exc_info)
            return repr(result) # or format into one line however you want to

        def format(self, record):
            s = super(OneLineExceptionFormatter, self).format(record)
            if record.exc_text:
                s = s.replace('\n', '') + '|'
            return s

    def configure_logging():
        fh = logging.FileHandler('output.txt', 'w')
        f = OneLineExceptionFormatter('%(asctime)s|%(levelname)s|%(message)s|',
                                      '%d/%m/%Y %H:%M:%S')
        fh.setFormatter(f)
        root = logging.getLogger()
        root.setLevel(logging.DEBUG)
        root.addHandler(fh)

    def main():
        configure_logging()
        logging.info('Sample message')
        try:
            x = 1 / 0
        except ZeroDivisionError as e:
            logging.exception('ZeroDivisionError: %s', e)

    if __name__ == '__main__':
        main()

When run, this produces a file with exactly two lines:

    28/01/2015 07:21:23|INFO|Sample message|
    28/01/2015 07:21:23|ERROR|ZeroDivisionError: integer division or modulo by zero|'Traceback (most recent call last):\n  File "logtest7.py", line 30, in main\n    x = 1 / 0\nZeroDivisionError: integer division or modulo by zero'|

While the above treatment is simplistic, it points the way to how
exception information can be formatted to your liking. The *Note
traceback: 181. module may be helpful for more specialized needs.


File: python.info,  Node: Speaking logging messages,  Next: Buffering logging messages and outputting them conditionally,  Prev: Customized exception formatting,  Up: Logging Cookbook

10.8.16 Speaking logging messages
---------------------------------

There might be situations when it is desirable to have logging messages
rendered in an audible rather than a visible format. This is easy to do
if you have text- to-speech (TTS) functionality available in your
system, even if it doesn’t have a Python binding. Most TTS systems
have a command line program you can run, and this can be invoked from a
handler using *Note subprocess: 167. It’s assumed here that TTS
command line programs won’t expect to interact with users or take a
long time to complete, and that the frequency of logged messages will
be not so high as to swamp the user with messages, and that it’s
acceptable to have the messages spoken one at a time rather than
concurrently, The example implementation below waits for one message to
be spoken before the next is processed, and this might cause other
handlers to be kept waiting. Here is a short example showing the
approach, which assumes that the `espeak' TTS package is available:

    import logging
    import subprocess
    import sys

    class TTSHandler(logging.Handler):
        def emit(self, record):
            msg = self.format(record)
            # Speak slowly in a female English voice
            cmd = ['espeak', '-s150', '-ven+f3', msg]
            p = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT)
            # wait for the program to finish
            p.communicate()

    def configure_logging():
        h = TTSHandler()
        root = logging.getLogger()
        root.addHandler(h)
        # the default formatter just returns the message
        root.setLevel(logging.DEBUG)

    def main():
        logging.info('Hello')
        logging.debug('Goodbye')

    if __name__ == '__main__':
        configure_logging()
        sys.exit(main())

When run, this script should say “Hello” and then “Goodbye” in
a female voice.

The above approach can, of course, be adapted to other TTS systems and
even other systems altogether which can process messages via external
programs run from a command line.


File: python.info,  Node: Buffering logging messages and outputting them conditionally,  Next: Formatting times using UTC GMT via configuration,  Prev: Speaking logging messages,  Up: Logging Cookbook

10.8.17 Buffering logging messages and outputting them conditionally
--------------------------------------------------------------------

There might be situations where you want to log messages in a temporary
area and only output them if a certain condition occurs. For example,
you may want to start logging debug events in a function, and if the
function completes without errors, you don’t want to clutter the log
with the collected debug information, but if there is an error, you
want all the debug information to be output as well as the error.

Here is an example which shows how you could do this using a decorator
for your functions where you want logging to behave this way. It makes
use of the *Note logging.handlers.MemoryHandler: 1361, which allows
buffering of logged events until some condition occurs, at which point
the buffered events are `flushed' - passed to another handler (the
`target' handler) for processing. By default, the `MemoryHandler'
flushed when its buffer gets filled up or an event whose level is
greater than or equal to a specified threshold is seen. You can use this
recipe with a more specialised subclass of `MemoryHandler' if you want
custom flushing behavior.

The example script has a simple function, `foo', which just cycles
through all the logging levels, writing to `sys.stderr' to say what
level it’s about to log at, and then actually logging a message at
that level. You can pass a parameter to `foo' which, if true, will log
at ERROR and CRITICAL levels - otherwise, it only logs at DEBUG, INFO
and WARNING levels.

The script just arranges to decorate `foo' with a decorator which will
do the conditional logging that’s required. The decorator takes a
logger as a parameter and attaches a memory handler for the duration of
the call to the decorated function. The decorator can be additionally
parameterised using a target handler, a level at which flushing should
occur, and a capacity for the buffer. These default to a *Note
StreamHandler: 1344. which writes to `sys.stderr', `logging.ERROR' and
`100' respectively.

Here’s the script:

    import logging
    from logging.handlers import MemoryHandler
    import sys

    logger = logging.getLogger(__name__)
    logger.addHandler(logging.NullHandler())

    def log_if_errors(logger, target_handler=None, flush_level=None, capacity=None):
        if target_handler is None:
            target_handler = logging.StreamHandler()
        if flush_level is None:
            flush_level = logging.ERROR
        if capacity is None:
            capacity = 100
        handler = MemoryHandler(capacity, flushLevel=flush_level, target=target_handler)

        def decorator(fn):
            def wrapper(*args, **kwargs):
                logger.addHandler(handler)
                try:
                    return fn(*args, **kwargs)
                except Exception:
                    logger.exception('call failed')
                    raise
                finally:
                    super(MemoryHandler, handler).flush()
                    logger.removeHandler(handler)
            return wrapper

        return decorator

    def write_line(s):
        sys.stderr.write('%s\n' % s)

    def foo(fail=False):
        write_line('about to log at DEBUG ...')
        logger.debug('Actually logged at DEBUG')
        write_line('about to log at INFO ...')
        logger.info('Actually logged at INFO')
        write_line('about to log at WARNING ...')
        logger.warning('Actually logged at WARNING')
        if fail:
            write_line('about to log at ERROR ...')
            logger.error('Actually logged at ERROR')
            write_line('about to log at CRITICAL ...')
            logger.critical('Actually logged at CRITICAL')
        return fail

    decorated_foo = log_if_errors(logger)(foo)

    if __name__ == '__main__':
        logger.setLevel(logging.DEBUG)
        write_line('Calling undecorated foo with False')
        assert not foo(False)
        write_line('Calling undecorated foo with True')
        assert foo(True)
        write_line('Calling decorated foo with False')
        assert not decorated_foo(False)
        write_line('Calling decorated foo with True')
        assert decorated_foo(True)

When this script is run, the following output should be observed:

    Calling undecorated foo with False
    about to log at DEBUG ...
    about to log at INFO ...
    about to log at WARNING ...
    Calling undecorated foo with True
    about to log at DEBUG ...
    about to log at INFO ...
    about to log at WARNING ...
    about to log at ERROR ...
    about to log at CRITICAL ...
    Calling decorated foo with False
    about to log at DEBUG ...
    about to log at INFO ...
    about to log at WARNING ...
    Calling decorated foo with True
    about to log at DEBUG ...
    about to log at INFO ...
    about to log at WARNING ...
    about to log at ERROR ...
    Actually logged at DEBUG
    Actually logged at INFO
    Actually logged at WARNING
    Actually logged at ERROR
    about to log at CRITICAL ...
    Actually logged at CRITICAL

As you can see, actual logging output only occurs when an event is
logged whose severity is ERROR or greater, but in that case, any
previous events at lower severities are also logged.

You can of course use the conventional means of decoration:

    @log_if_errors(logger)
    def foo(fail=False):
        ...


File: python.info,  Node: Formatting times using UTC GMT via configuration,  Next: Using a context manager for selective logging,  Prev: Buffering logging messages and outputting them conditionally,  Up: Logging Cookbook

10.8.18 Formatting times using UTC (GMT) via configuration
----------------------------------------------------------

Sometimes you want to format times using UTC, which can be done using a
class such as `UTCFormatter', shown below:

    import logging
    import time

    class UTCFormatter(logging.Formatter):
        converter = time.gmtime

and you can then use the `UTCFormatter' in your code instead of *Note
Formatter: 12fb. If you want to do that via configuration, you can use
the *Note dictConfig(): 134b. API with an approach illustrated by the
following complete example:

    import logging
    import logging.config
    import time

    class UTCFormatter(logging.Formatter):
        converter = time.gmtime

    LOGGING = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'utc': {
                '()': UTCFormatter,
                'format': '%(asctime)s %(message)s',
            },
            'local': {
                'format': '%(asctime)s %(message)s',
            }
        },
        'handlers': {
            'console1': {
                'class': 'logging.StreamHandler',
                'formatter': 'utc',
            },
            'console2': {
                'class': 'logging.StreamHandler',
                'formatter': 'local',
            },
        },
        'root': {
            'handlers': ['console1', 'console2'],
       }
    }

    if __name__ == '__main__':
        logging.config.dictConfig(LOGGING)
        logging.warning('The local time is %s', time.asctime())

When this script is run, it should print something like:

    2015-10-17 12:53:29,501 The local time is Sat Oct 17 13:53:29 2015
    2015-10-17 13:53:29,501 The local time is Sat Oct 17 13:53:29 2015

showing how the time is formatted both as local time and UTC, one for
each handler.


File: python.info,  Node: Using a context manager for selective logging,  Prev: Formatting times using UTC GMT via configuration,  Up: Logging Cookbook

10.8.19 Using a context manager for selective logging
-----------------------------------------------------

There are times when it would be useful to temporarily change the
logging configuration and revert it back after doing something. For
this, a context manager is the most obvious way of saving and restoring
the logging context.  Here is a simple example of such a context
manager, which allows you to optionally change the logging level and
add a logging handler purely in the scope of the context manager:

    import logging
    import sys

    class LoggingContext(object):
        def __init__(self, logger, level=None, handler=None, close=True):
            self.logger = logger
            self.level = level
            self.handler = handler
            self.close = close

        def __enter__(self):
            if self.level is not None:
                self.old_level = self.logger.level
                self.logger.setLevel(self.level)
            if self.handler:
                self.logger.addHandler(self.handler)

        def __exit__(self, et, ev, tb):
            if self.level is not None:
                self.logger.setLevel(self.old_level)
            if self.handler:
                self.logger.removeHandler(self.handler)
            if self.handler and self.close:
                self.handler.close()
            # implicit return of None => don't swallow exceptions

If you specify a level value, the logger’s level is set to that value
in the scope of the with block covered by the context manager. If you
specify a handler, it is added to the logger on entry to the block and
removed on exit from the block. You can also ask the manager to close
the handler for you on block exit - you could do this if you don’t
need the handler any more.

To illustrate how it works, we can add the following block of code to
the above:

    if __name__ == '__main__':
        logger = logging.getLogger('foo')
        logger.addHandler(logging.StreamHandler())
        logger.setLevel(logging.INFO)
        logger.info('1. This should appear just once on stderr.')
        logger.debug('2. This should not appear.')
        with LoggingContext(logger, level=logging.DEBUG):
            logger.debug('3. This should appear once on stderr.')
        logger.debug('4. This should not appear.')
        h = logging.StreamHandler(sys.stdout)
        with LoggingContext(logger, level=logging.DEBUG, handler=h, close=True):
            logger.debug('5. This should appear twice - once on stderr and once on stdout.')
        logger.info('6. This should appear just once on stderr.')
        logger.debug('7. This should not appear.')

We initially set the logger’s level to `INFO', so message #1 appears
and message #2 doesn’t. We then change the level to `DEBUG'
temporarily in the following `with' block, and so message #3 appears.
After the block exits, the logger’s level is restored to `INFO' and
so message #4 doesn’t appear. In the next `with' block, we set the
level to `DEBUG' again but also add a handler writing to `sys.stdout'.
Thus, message #5 appears twice on the console (once via `stderr' and
once via `stdout'). After the `with' statement’s completion, the
status is as it was before so message #6 appears (like message #1)
whereas message #7 doesn’t (just like message #2).

If we run the resulting script, the result is as follows:

    $ python logctx.py
    1. This should appear just once on stderr.
    3. This should appear once on stderr.
    5. This should appear twice - once on stderr and once on stdout.
    5. This should appear twice - once on stderr and once on stdout.
    6. This should appear just once on stderr.

If we run it again, but pipe `stderr' to `/dev/null', we see the
following, which is the only message written to `stdout':

    $ python logctx.py 2>/dev/null
    5. This should appear twice - once on stderr and once on stdout.

Once again, but piping `stdout' to `/dev/null', we get:

    $ python logctx.py >/dev/null
    1. This should appear just once on stderr.
    3. This should appear once on stderr.
    5. This should appear twice - once on stderr and once on stdout.
    6. This should appear just once on stderr.

In this case, the message #5 printed to `stdout' doesn’t appear, as
expected.

Of course, the approach described here can be generalised, for example
to attach logging filters temporarily. Note that the above code works
in Python 2 as well as Python 3.


File: python.info,  Node: Regular Expression HOWTO,  Next: Socket Programming HOWTO,  Prev: Logging Cookbook,  Up: Python HOWTOs

10.9 Regular Expression HOWTO
=============================

Author: A.M. Kuchling <<amk@amk.ca>>

Abstract
........

This document is an introductory tutorial to using regular expressions
in Python with the *Note re: 144. module.  It provides a gentler
introduction than the corresponding section in the Library Reference.

* Menu:

* Introduction: Introduction<12>.
* Simple Patterns::
* Using Regular Expressions::
* More Pattern Power::
* Modifying Strings::
* Common Problems::
* Feedback::


File: python.info,  Node: Introduction<12>,  Next: Simple Patterns,  Up: Regular Expression HOWTO

10.9.1 Introduction
-------------------

The *Note re: 144. module was added in Python 1.5, and provides
Perl-style regular expression patterns.  Earlier versions of Python
came with the `regex' module, which provided Emacs-style patterns.  The
`regex' module was removed completely in Python 2.5.

Regular expressions (called REs, or regexes, or regex patterns) are
essentially a tiny, highly specialized programming language embedded
inside Python and made available through the *Note re: 144. module.
Using this little language, you specify the rules for the set of
possible strings that you want to match; this set might contain English
sentences, or e-mail addresses, or TeX commands, or anything you like.
You can then ask questions such as “Does this string match the
pattern?”, or “Is there a match for the pattern anywhere in this
string?”.  You can also use REs to modify a string or to split it
apart in various ways.

Regular expression patterns are compiled into a series of bytecodes
which are then executed by a matching engine written in C.  For
advanced use, it may be necessary to pay careful attention to how the
engine will execute a given RE, and write the RE in a certain way in
order to produce bytecode that runs faster.  Optimization isn’t
covered in this document, because it requires that you have a good
understanding of the matching engine’s internals.

The regular expression language is relatively small and restricted, so
not all possible string processing tasks can be done using regular
expressions.  There are also tasks that `can' be done with regular
expressions, but the expressions turn out to be very complicated.  In
these cases, you may be better off writing Python code to do the
processing; while Python code will be slower than an elaborate regular
expression, it will also probably be more understandable.


File: python.info,  Node: Simple Patterns,  Next: Using Regular Expressions,  Prev: Introduction<12>,  Up: Regular Expression HOWTO

10.9.2 Simple Patterns
----------------------

We’ll start by learning about the simplest possible regular
expressions.  Since regular expressions are used to operate on strings,
we’ll begin with the most common task: matching characters.

For a detailed explanation of the computer science underlying regular
expressions (deterministic and non-deterministic finite automata), you
can refer to almost any textbook on writing compilers.

* Menu:

* Matching Characters::
* Repeating Things::


File: python.info,  Node: Matching Characters,  Next: Repeating Things,  Up: Simple Patterns

10.9.2.1 Matching Characters
............................

Most letters and characters will simply match themselves.  For example,
the regular expression `test' will match the string `test' exactly.
(You can enable a case-insensitive mode that would let this RE match
`Test' or `TEST' as well; more about this later.)

There are exceptions to this rule; some characters are special
`metacharacters', and don’t match themselves.  Instead, they signal
that some out-of-the-ordinary thing should be matched, or they affect
other portions of the RE by repeating them or changing their meaning.
Much of this document is devoted to discussing various metacharacters
and what they do.

Here’s a complete list of the metacharacters; their meanings will be
discussed in the rest of this HOWTO.

    . ^ $ * + ? { } [ ] \ | ( )

The first metacharacters we’ll look at are `[' and `]'. They’re
used for specifying a character class, which is a set of characters
that you wish to match.  Characters can be listed individually, or a
range of characters can be indicated by giving two characters and
separating them by a `'-''.  For example, `[abc]' will match any of the
characters `a', `b', or `c'; this is the same as `[a-c]', which uses a
range to express the same set of characters.  If you wanted to match
only lowercase letters, your RE would be `[a-z]'.

Metacharacters are not active inside classes.  For example, `[akm$]'
will match any of the characters `'a'', `'k'', `'m'', or `'$''; `'$'' is
usually a metacharacter, but inside a character class it’s stripped
of its special nature.

You can match the characters not listed within the class by
`complementing' the set.  This is indicated by including a `'^'' as the
first character of the class; `'^'' outside a character class will
simply match the `'^'' character.  For example, `[^5]' will match any
character except `'5''.

Perhaps the most important metacharacter is the backslash, `\'.   As in
Python string literals, the backslash can be followed by various
characters to signal various special sequences.  It’s also used to
escape all the metacharacters so you can still match them in patterns;
for example, if you need to match a `[' or  `\', you can precede them
with a backslash to remove their special meaning: `\[' or `\\'.

Some of the special sequences beginning with `'\'' represent predefined
sets of characters that are often useful, such as the set of digits,
the set of letters, or the set of anything that isn’t whitespace.
The following predefined special sequences are a subset of those
available. The equivalent classes are for byte string patterns. For a
complete list of sequences and expanded class definitions for Unicode
string patterns, see the last part of *Note Regular Expression Syntax:
9dd.

`\d'
     Matches any decimal digit; this is equivalent to the class `[0-9]'.

`\D'
     Matches any non-digit character; this is equivalent to the class
     `[^0-9]'.

`\s'
     Matches any whitespace character; this is equivalent to the class
     `[ \t\n\r\f\v]'.

`\S'
     Matches any non-whitespace character; this is equivalent to the
     class `[^ \t\n\r\f\v]'.

`\w'
     Matches any alphanumeric character; this is equivalent to the class
     `[a-zA-Z0-9_]'.

`\W'
     Matches any non-alphanumeric character; this is equivalent to the
     class `[^a-zA-Z0-9_]'.

These sequences can be included inside a character class.  For example,
`[\s,.]' is a character class that will match any whitespace character,
or `','' or `'.''.

The final metacharacter in this section is `.'.  It matches anything
except a newline character, and there’s an alternate mode
(`re.DOTALL') where it will match even a newline.  `'.'' is often used
where you want to match “any character”.


File: python.info,  Node: Repeating Things,  Prev: Matching Characters,  Up: Simple Patterns

10.9.2.2 Repeating Things
.........................

Being able to match varying sets of characters is the first thing
regular expressions can do that isn’t already possible with the
methods available on strings.  However, if that was the only additional
capability of regexes, they wouldn’t be much of an advance. Another
capability is that you can specify that portions of the RE must be
repeated a certain number of times.

The first metacharacter for repeating things that we’ll look at is
`*'.  `*' doesn’t match the literal character `*'; instead, it
specifies that the previous character can be matched zero or more
times, instead of exactly once.

For example, `ca*t' will match `ct' (0 `a' characters), `cat' (1 `a'),
`caaat' (3 `a' characters), and so forth.  The RE engine has various
internal limitations stemming from the size of C’s `int' type that
will prevent it from matching over 2 billion `a' characters; you
probably don’t have enough memory to construct a string that large,
so you shouldn’t run into that limit.

Repetitions such as `*' are `greedy'; when repeating a RE, the matching
engine will try to repeat it as many times as possible. If later
portions of the pattern don’t match, the matching engine will then
back up and try again with fewer repetitions.

A step-by-step example will make this more obvious.  Let’s consider
the expression `a[bcd]*b'.  This matches the letter `'a'', zero or more
letters from the class `[bcd]', and finally ends with a `'b''.  Now
imagine matching this RE against the string `abcbd'.

Step       Matched         Explanation
----------------------------------------------------------------- 
1          `a'             The `a' in the RE matches.
2          `abcbd'         The engine matches `[bcd]*', going
                           as far as it can, which is to the
                           end of the string.
3          `Failure'       The engine tries to match `b', but
                           the current position is at the end
                           of the string, so it fails.
4          `abcb'          Back up, so that  `[bcd]*' matches
                           one less character.
5          `Failure'       Try `b' again, but the current
                           position is at the last character,
                           which is a `'d''.
6          `abc'           Back up again, so that `[bcd]*' is
                           only matching `bc'.
6          `abcb'          Try `b' again.  This time the
                           character at the current position is
                           `'b'', so it succeeds.

The end of the RE has now been reached, and it has matched `abcb'.  This
demonstrates how the matching engine goes as far as it can at first,
and if no match is found it will then progressively back up and retry
the rest of the RE again and again.  It will back up until it has tried
zero matches for `[bcd]*', and if that subsequently fails, the engine
will conclude that the string doesn’t match the RE at all.

Another repeating metacharacter is `+', which matches one or more
times.  Pay careful attention to the difference between `*' and `+';
`*' matches `zero' or more times, so whatever’s being repeated may
not be present at all, while `+' requires at least `one' occurrence.
To use a similar example, `ca+t' will match `cat' (1 `a'), `caaat' (3
`a'’s), but won’t match `ct'.

There are two more repeating qualifiers.  The question mark character,
`?', matches either once or zero times; you can think of it as marking
something as being optional.  For example, `home-?brew' matches either
`homebrew' or `home-brew'.

The most complicated repeated qualifier is `{m,n}', where `m' and `n'
are decimal integers.  This qualifier means there must be at least `m'
repetitions, and at most `n'.  For example, `a/{1,3}b' will match
`a/b', `a//b', and `a///b'.  It won’t match `ab', which has no
slashes, or `a////b', which has four.

You can omit either `m' or `n'; in that case, a reasonable value is
assumed for the missing value.  Omitting `m' is interpreted as a lower
limit of 0, while omitting `n' results in an upper bound of infinity
— actually, the upper bound is the 2-billion limit mentioned earlier,
but that might as well be infinity.

Readers of a reductionist bent may notice that the three other
qualifiers can all be expressed using this notation.  `{0,}' is the
same as `*', `{1,}' is equivalent to `+', and `{0,1}' is the same as
`?'.  It’s better to use `*', `+', or `?' when you can, simply
because they’re shorter and easier to read.


File: python.info,  Node: Using Regular Expressions,  Next: More Pattern Power,  Prev: Simple Patterns,  Up: Regular Expression HOWTO

10.9.3 Using Regular Expressions
--------------------------------

Now that we’ve looked at some simple regular expressions, how do we
actually use them in Python?  The *Note re: 144. module provides an
interface to the regular expression engine, allowing you to compile REs
into objects and then perform matches with them.

* Menu:

* Compiling Regular Expressions::
* The Backslash Plague::
* Performing Matches::
* Module-Level Functions: Module-Level Functions<2>.
* Compilation Flags::


File: python.info,  Node: Compiling Regular Expressions,  Next: The Backslash Plague,  Up: Using Regular Expressions

10.9.3.1 Compiling Regular Expressions
......................................

Regular expressions are compiled into pattern objects, which have
methods for various operations such as searching for pattern matches or
performing string substitutions.

    >>> import re
    >>> p = re.compile('ab*')
    >>> p  #doctest: +ELLIPSIS
    <_sre.SRE_Pattern object at 0x...>

*Note re.compile(): 9ea. also accepts an optional `flags' argument,
used to enable various special features and syntax variations.  We’ll
go over the available settings later, but for now a single example will
do:

    >>> p = re.compile('ab*', re.IGNORECASE)

The RE is passed to *Note re.compile(): 9ea. as a string.  REs are
handled as strings because regular expressions aren’t part of the
core Python language, and no special syntax was created for expressing
them.  (There are applications that don’t need REs at all, so
there’s no need to bloat the language specification by including
them.) Instead, the *Note re: 144. module is simply a C extension module
included with Python, just like the *Note socket: 15c. or *Note zlib:
1ad. modules.

Putting REs in strings keeps the Python language simpler, but has one
disadvantage which is the topic of the next section.


File: python.info,  Node: The Backslash Plague,  Next: Performing Matches,  Prev: Compiling Regular Expressions,  Up: Using Regular Expressions

10.9.3.2 The Backslash Plague
.............................

As stated earlier, regular expressions use the backslash character
(`'\'') to indicate special forms or to allow special characters to be
used without invoking their special meaning. This conflicts with
Python’s usage of the same character for the same purpose in string
literals.

Let’s say you want to write a RE that matches the string `\section',
which might be found in a LaTeX file.  To figure out what to write in
the program code, start with the desired string to be matched.  Next,
you must escape any backslashes and other metacharacters by preceding
them with a backslash, resulting in the string `\\section'.  The
resulting string that must be passed to *Note re.compile(): 9ea. must
be `\\section'.  However, to express this as a Python string literal,
both backslashes must be escaped `again'.

Characters              Stage
----------------------------------------------------------------------- 
`\section'              Text string to be matched
`\\section'             Escaped backslash for *Note re.compile(): 9ea.
`"\\\\section"'         Escaped backslashes for a string literal

In short, to match a literal backslash, one has to write `'\\\\'' as
the RE string, because the regular expression must be `\\', and each
backslash must be expressed as `\\' inside a regular Python string
literal.  In REs that feature backslashes repeatedly, this leads to
lots of repeated backslashes and makes the resulting strings difficult
to understand.

The solution is to use Python’s raw string notation for regular
expressions; backslashes are not handled in any special way in a string
literal prefixed with `'r'', so `r"\n"' is a two-character string
containing `'\'' and `'n'', while `"\n"' is a one-character string
containing a newline. Regular expressions will often be written in
Python code using this raw string notation.

Regular String          Raw string
----------------------------------------------- 
`"ab*"'                 `r"ab*"'
`"\\\\section"'         `r"\\section"'
`"\\w+\\s+\\1"'         `r"\w+\s+\1"'


File: python.info,  Node: Performing Matches,  Next: Module-Level Functions<2>,  Prev: The Backslash Plague,  Up: Using Regular Expressions

10.9.3.3 Performing Matches
...........................

Once you have an object representing a compiled regular expression,
what do you do with it?  Pattern objects have several methods and
attributes.  Only the most significant ones will be covered here;
consult the *Note re: 144. docs for a complete listing.

Method/Attribute       Purpose
--------------------------------------------------------------------------- 
`match()'              Determine if the RE matches at the beginning of
                       the string.
`search()'             Scan through a string, looking for any location
                       where this RE matches.
`findall()'            Find all substrings where the RE matches, and
                       returns them as a list.
`finditer()'           Find all substrings where the RE matches, and
                       returns them as an *Note iterator: 8a8.

`match()' and `search()' return `None' if no match can be found.  If
they’re successful, a *Note match object: a04. instance is returned,
containing information about the match: where it starts and ends, the
substring it matched, and more.

You can learn about this by interactively experimenting with the *Note
re: 144.  module.  If you have Tkinter available, you may also want to
look at Tools/scripts/redemo.py(1), a demonstration program included
with the Python distribution.  It allows you to enter REs and strings,
and displays whether the RE matches or fails. `redemo.py' can be quite
useful when trying to debug a complicated RE.  Phil Schwartz’s
Kodos(2) is also an interactive tool for developing and testing RE
patterns.

This HOWTO uses the standard Python interpreter for its examples.
First, run the Python interpreter, import the *Note re: 144. module,
and compile a RE:

    Python 2.2.2 (#1, Feb 10 2003, 12:57:01)
    >>> import re
    >>> p = re.compile('[a-z]+')
    >>> p  #doctest: +ELLIPSIS
    <_sre.SRE_Pattern object at 0x...>

Now, you can try matching various strings against the RE `[a-z]+'.  An
empty string shouldn’t match at all, since `+' means ‘one or more
repetitions’.  `match()' should return `None' in this case, which
will cause the interpreter to print no output.  You can explicitly
print the result of `match()' to make this clear.

    >>> p.match("")
    >>> print p.match("")
    None

Now, let’s try it on a string that it should match, such as `tempo'.
In this case, `match()' will return a *Note match object: a04, so you
should store the result in a variable for later use.

    >>> m = p.match('tempo')
    >>> m  #doctest: +ELLIPSIS
    <_sre.SRE_Match object at 0x...>

Now you can query the *Note match object: a04. for information about
the matching string.  *Note match object: a04. instances also have
several methods and attributes; the most important ones are:

Method/Attribute       Purpose
------------------------------------------------------------------------ 
`group()'              Return the string matched by the RE
`start()'              Return the starting position of the match
`end()'                Return the ending position of the match
`span()'               Return a tuple containing the (start, end)
                       positions  of the match

Trying these methods will soon clarify their meaning:

    >>> m.group()
    'tempo'
    >>> m.start(), m.end()
    (0, 5)
    >>> m.span()
    (0, 5)

`group()' returns the substring that was matched by the RE.  `start()'
and `end()' return the starting and ending index of the match. `span()'
returns both start and end indexes in a single tuple.  Since the
`match()' method only checks if the RE matches at the start of a
string, `start()' will always be zero.  However, the `search()' method
of patterns scans through the string, so  the match may not start at
zero in that case.

    >>> print p.match('::: message')
    None
    >>> m = p.search('::: message'); print m  #doctest: +ELLIPSIS
    <_sre.SRE_Match object at 0x...>
    >>> m.group()
    'message'
    >>> m.span()
    (4, 11)

In actual programs, the most common style is to store the *Note match
object: a04. in a variable, and then check if it was `None'.  This
usually looks like:

    p = re.compile( ... )
    m = p.match( 'string goes here' )
    if m:
        print 'Match found: ', m.group()
    else:
        print 'No match'

Two pattern methods return all of the matches for a pattern.
`findall()' returns a list of matching strings:

    >>> p = re.compile('\d+')
    >>> p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')
    ['12', '11', '10']

`findall()' has to create the entire list before it can be returned as
the result.  The `finditer()' method returns a sequence of *Note match
object: a04. instances as an *Note iterator: 8a8. (3)

    >>> iterator = p.finditer('12 drummers drumming, 11 ... 10 ...')
    >>> iterator  #doctest: +ELLIPSIS
    <callable-iterator object at 0x...>
    >>> for match in iterator:
    ...     print match.span()
    ...
    (0, 2)
    (22, 24)
    (29, 31)

---------- Footnotes ----------

(1) https://hg.python.org/cpython/file/2.7/Tools/scripts/redemo.py

(2) http://kodos.sourceforge.net/

(3) Introduced in Python 2.2.2.


File: python.info,  Node: Module-Level Functions<2>,  Next: Compilation Flags,  Prev: Performing Matches,  Up: Using Regular Expressions

10.9.3.4 Module-Level Functions
...............................

You don’t have to create a pattern object and call its methods; the
*Note re: 144. module also provides top-level functions called
`match()', `search()', `findall()', `sub()', and so forth.  These
functions take the same arguments as the corresponding pattern method,
with the RE string added as the first argument, and still return either
`None' or a *Note match object: a04. instance.

    >>> print re.match(r'From\s+', 'Fromage amk')
    None
    >>> re.match(r'From\s+', 'From amk Thu May 14 19:12:10 1998')  #doctest: +ELLIPSIS
    <_sre.SRE_Match object at 0x...>

Under the hood, these functions simply create a pattern object for you
and call the appropriate method on it.  They also store the compiled
object in a cache, so future calls using the same RE are faster.

Should you use these module-level functions, or should you get the
pattern and call its methods yourself?  That choice depends on how
frequently the RE will be used, and on your personal coding style.  If
the RE is being used at only one point in the code, then the module
functions are probably more convenient.  If a program contains a lot of
regular expressions, or re-uses the same ones in several locations,
then it might be worthwhile to collect all the definitions in one
place, in a section of code that compiles all the REs ahead of time.
To take an example from the standard library, here’s an extract from
the deprecated `xmllib' module:

    ref = re.compile( ... )
    entityref = re.compile( ... )
    charref = re.compile( ... )
    starttagopen = re.compile( ... )

I generally prefer to work with the compiled object, even for one-time
uses, but few people will be as much of a purist about this as I am.


File: python.info,  Node: Compilation Flags,  Prev: Module-Level Functions<2>,  Up: Using Regular Expressions

10.9.3.5 Compilation Flags
..........................

Compilation flags let you modify some aspects of how regular
expressions work.  Flags are available in the *Note re: 144. module
under two names, a long name such as `IGNORECASE' and a short,
one-letter form such as `I'.  (If you’re familiar with Perl’s
pattern modifiers, the one-letter forms use the same letters; the short
form of *Note re.VERBOSE: 9f2. is *Note re.X: 9e8, for example.)
Multiple flags can be specified by bitwise OR-ing them; `re.I | re.M'
sets both the `I' and `M' flags, for example.

Here’s a table of the available flags, followed by a more detailed
explanation of each one.

Flag                                  Meaning
--------------------------------------------------------------------------------------- 
`DOTALL', `S'                         Make `.' match any character, including newlines
`IGNORECASE', `I'                     Do case-insensitive matches
`LOCALE', `L'                         Do a locale-aware match
`MULTILINE', `M'                      Multi-line matching, affecting `^' and `$'
`VERBOSE', `X'                        Enable verbose REs, which can be organized more
                                      cleanly and understandably.
`UNICODE', `U'                        Makes several escapes like `\w', `\b', `\s' and
                                      `\d' dependent on the Unicode character
                                      database.

 -- Data: I
 -- Data: IGNORECASE
     Perform case-insensitive matching; character class and literal
     strings will match letters by ignoring case.  For example, `[A-Z]'
     will match lowercase letters, too, and `Spam' will match `Spam',
     `spam', or `spAM'. This lowercasing doesn’t take the current
     locale into account; it will if you also set the `LOCALE' flag.

 -- Data: L
 -- Data: LOCALE
     Make `\w', `\W', `\b', and `\B', dependent on the current locale.

     Locales are a feature of the C library intended to help in writing
     programs that take account of language differences.  For example,
     if you’re processing French text, you’d want to be able to
     write `\w+' to match words, but `\w' only matches the character
     class `[A-Za-z]'; it won’t match `'é'' or `'ç''.  If your
     system is configured properly and a French locale is selected,
     certain C functions will tell the program that `'é'' should also
     be considered a letter.  Setting the `LOCALE' flag when compiling
     a regular expression will cause the resulting compiled object to
     use these C functions for `\w'; this is slower, but also enables
     `\w+' to match French words as you’d expect.

 -- Data: M
 -- Data: MULTILINE
     (`^' and `$' haven’t been explained yet;  they’ll be
     introduced in section *Note More Metacharacters: 2ff9.)

     Usually `^' matches only at the beginning of the string, and `$'
     matches only at the end of the string and immediately before the
     newline (if any) at the end of the string. When this flag is
     specified, `^' matches at the beginning of the string and at the
     beginning of each line within the string, immediately following
     each newline.  Similarly, the `$' metacharacter matches either at
     the end of the string and at the end of each line (immediately
     preceding each newline).

 -- Data: S
 -- Data: DOTALL
     Makes the `'.'' special character match any character at all,
     including a newline; without this flag, `'.'' will match anything
     `except' a newline.

 -- Data: U
 -- Data: UNICODE
     Make `\w', `\W', `\b', `\B', `\d', `\D', `\s' and `\S' dependent
     on the Unicode character properties database.

 -- Data: X
 -- Data: VERBOSE
     This flag allows you to write regular expressions that are more
     readable by granting you more flexibility in how you can format
     them.  When this flag has been specified, whitespace within the RE
     string is ignored, except when the whitespace is in a character
     class or preceded by an unescaped backslash; this lets you
     organize and indent the RE more clearly.  This flag also lets you
     put comments within a RE that will be ignored by the engine;
     comments are marked by a `'#'' that’s neither in a character
     class or preceded by an unescaped backslash.

     For example, here’s a RE that uses *Note re.VERBOSE: 9f2.; see
     how much easier it is to read?

         charref = re.compile(r"""
          &[#]                # Start of a numeric entity reference
          (
              0[0-7]+         # Octal form
            | [0-9]+          # Decimal form
            | x[0-9a-fA-F]+   # Hexadecimal form
          )
          ;                   # Trailing semicolon
         """, re.VERBOSE)

     Without the verbose setting, the RE would look like this:

         charref = re.compile("&#(0[0-7]+"
                              "|[0-9]+"
                              "|x[0-9a-fA-F]+);")

     In the above example, Python’s automatic concatenation of string
     literals has been used to break up the RE into smaller pieces, but
     it’s still more difficult to understand than the version using
     *Note re.VERBOSE: 9f2.


File: python.info,  Node: More Pattern Power,  Next: Modifying Strings,  Prev: Using Regular Expressions,  Up: Regular Expression HOWTO

10.9.4 More Pattern Power
-------------------------

So far we’ve only covered a part of the features of regular
expressions.  In this section, we’ll cover some new metacharacters,
and how to use groups to retrieve portions of the text that was matched.

* Menu:

* More Metacharacters::
* Grouping::
* Non-capturing and Named Groups::
* Lookahead Assertions::


File: python.info,  Node: More Metacharacters,  Next: Grouping,  Up: More Pattern Power

10.9.4.1 More Metacharacters
............................

There are some metacharacters that we haven’t covered yet.  Most of
them will be covered in this section.

Some of the remaining metacharacters to be discussed are `zero-width
assertions'.  They don’t cause the engine to advance through the
string; instead, they consume no characters at all, and simply succeed
or fail.  For example, `\b' is an assertion that the current position
is located at a word boundary; the position isn’t changed by the `\b'
at all.  This means that zero-width assertions should never be
repeated, because if they match once at a given location, they can
obviously be matched an infinite number of times.

`|'
     Alternation, or the “or” operator.   If A and B are regular
     expressions, `A|B' will match any string that matches either `A'
     or `B'. `|' has very low precedence in order to make it work
     reasonably when you’re alternating multi-character strings.
     `Crow|Servo' will match either `Crow' or `Servo', not `Cro', a
     `'w'' or an `'S'', and `ervo'.

     To match a literal `'|'', use `\|', or enclose it inside a
     character class, as in `[|]'.

`^'
     Matches at the beginning of lines.  Unless the `MULTILINE' flag
     has been set, this will only match at the beginning of the string.
     In `MULTILINE' mode, this also matches immediately after each
     newline within the string.

     For example, if you wish to match the word `From' only at the
     beginning of a line, the RE to use is `^From'.

         >>> print re.search('^From', 'From Here to Eternity')  #doctest: +ELLIPSIS
         <_sre.SRE_Match object at 0x...>
         >>> print re.search('^From', 'Reciting From Memory')
         None

`$'
     Matches at the end of a line, which is defined as either the end
     of the string, or any location followed by a newline character.

         >>> print re.search('}$', '{block}')  #doctest: +ELLIPSIS
         <_sre.SRE_Match object at 0x...>
         >>> print re.search('}$', '{block} ')
         None
         >>> print re.search('}$', '{block}\n')  #doctest: +ELLIPSIS
         <_sre.SRE_Match object at 0x...>

     To match a literal `'$'', use `\$' or enclose it inside a
     character class, as in  `[$]'.

`\A'
     Matches only at the start of the string.  When not in `MULTILINE'
     mode, `\A' and `^' are effectively the same.  In `MULTILINE' mode,
     they’re different: `\A' still matches only at the beginning of
     the string, but `^' may match at any location inside the string
     that follows a newline character.

`\Z'
     Matches only at the end of the string.

`\b'
     Word boundary.  This is a zero-width assertion that matches only
     at the beginning or end of a word.  A word is defined as a
     sequence of alphanumeric characters, so the end of a word is
     indicated by whitespace or a non-alphanumeric character.

     The following example matches `class' only when it’s a complete
     word; it won’t match when it’s contained inside another word.

         >>> p = re.compile(r'\bclass\b')
         >>> print p.search('no class at all')  #doctest: +ELLIPSIS
         <_sre.SRE_Match object at 0x...>
         >>> print p.search('the declassified algorithm')
         None
         >>> print p.search('one subclass is')
         None

     There are two subtleties you should remember when using this
     special sequence.  First, this is the worst collision between
     Python’s string literals and regular expression sequences.  In
     Python’s string literals, `\b' is the backspace character, ASCII
     value 8.  If you’re not using raw strings, then Python will
     convert the `\b' to a backspace, and your RE won’t match as you
     expect it to.  The following example looks the same as our
     previous RE, but omits the `'r'' in front of the RE string.

         >>> p = re.compile('\bclass\b')
         >>> print p.search('no class at all')
         None
         >>> print p.search('\b' + 'class' + '\b')  #doctest: +ELLIPSIS
         <_sre.SRE_Match object at 0x...>

     Second, inside a character class, where there’s no use for this
     assertion, `\b' represents the backspace character, for
     compatibility with Python’s string literals.

`\B'
     Another zero-width assertion, this is the opposite of `\b', only
     matching when the current position is not at a word boundary.


File: python.info,  Node: Grouping,  Next: Non-capturing and Named Groups,  Prev: More Metacharacters,  Up: More Pattern Power

10.9.4.2 Grouping
.................

Frequently you need to obtain more information than just whether the RE
matched or not.  Regular expressions are often used to dissect strings
by writing a RE divided into several subgroups which match different
components of interest.  For example, an RFC-822 header line is divided
into a header name and a value, separated by a `':'', like this:

    From: author@example.com
    User-Agent: Thunderbird 1.5.0.9 (X11/20061227)
    MIME-Version: 1.0
    To: editor@example.com

This can be handled by writing a regular expression which matches an
entire header line, and has one group which matches the header name,
and another group which matches the header’s value.

Groups are marked by the `'('', `')'' metacharacters. `'('' and `')''
have much the same meaning as they do in mathematical expressions; they
group together the expressions contained inside them, and you can
repeat the contents of a group with a repeating qualifier, such as `*',
`+', `?', or `{m,n}'.  For example, `(ab)*' will match zero or more
repetitions of `ab'.

    >>> p = re.compile('(ab)*')
    >>> print p.match('ababababab').span()
    (0, 10)

Groups indicated with `'('', `')'' also capture the starting and ending
index of the text that they match; this can be retrieved by passing an
argument to `group()', `start()', `end()', and `span()'.  Groups are
numbered starting with 0.  Group 0 is always present; it’s the whole
RE, so *Note match object: a04. methods all have group 0 as their
default argument.  Later we’ll see how to express groups that don’t
capture the span of text that they match.

    >>> p = re.compile('(a)b')
    >>> m = p.match('ab')
    >>> m.group()
    'ab'
    >>> m.group(0)
    'ab'

Subgroups are numbered from left to right, from 1 upward.  Groups can
be nested; to determine the number, just count the opening parenthesis
characters, going from left to right.

    >>> p = re.compile('(a(b)c)d')
    >>> m = p.match('abcd')
    >>> m.group(0)
    'abcd'
    >>> m.group(1)
    'abc'
    >>> m.group(2)
    'b'

`group()' can be passed multiple group numbers at a time, in which case
it will return a tuple containing the corresponding values for those
groups.

    >>> m.group(2,1,2)
    ('b', 'abc', 'b')

The `groups()' method returns a tuple containing the strings for all the
subgroups, from 1 up to however many there are.

    >>> m.groups()
    ('abc', 'b')

Backreferences in a pattern allow you to specify that the contents of
an earlier capturing group must also be found at the current location
in the string.  For example, `\1' will succeed if the exact contents of
group 1 can be found at the current position, and fails otherwise.
Remember that Python’s string literals also use a backslash followed
by numbers to allow including arbitrary characters in a string, so be
sure to use a raw string when incorporating backreferences in a RE.

For example, the following RE detects doubled words in a string.

    >>> p = re.compile(r'(\b\w+)\s+\1')
    >>> p.search('Paris in the the spring').group()
    'the the'

Backreferences like this aren’t often useful for just searching
through a string — there are few text formats which repeat data in
this way — but you’ll soon find out that they’re `very' useful
when performing string substitutions.


File: python.info,  Node: Non-capturing and Named Groups,  Next: Lookahead Assertions,  Prev: Grouping,  Up: More Pattern Power

10.9.4.3 Non-capturing and Named Groups
.......................................

Elaborate REs may use many groups, both to capture substrings of
interest, and to group and structure the RE itself.  In complex REs, it
becomes difficult to keep track of the group numbers.  There are two
features which help with this problem.  Both of them use a common
syntax for regular expression extensions, so we’ll look at that first.

Perl 5 added several additional features to standard regular
expressions, and the Python *Note re: 144. module supports most of
them.   It would have been difficult to choose new single-keystroke
metacharacters or new special sequences beginning with `\' to represent
the new features without making Perl’s regular expressions
confusingly different from standard REs.  If you chose `&' as a new
metacharacter, for example, old expressions would be assuming that `&'
was a regular character and wouldn’t have escaped it by writing `\&'
or `[&]'.

The solution chosen by the Perl developers was to use `(?...)' as the
extension syntax.  `?' immediately after a parenthesis was a syntax
error because the `?' would have nothing to repeat, so this didn’t
introduce any compatibility problems.  The characters immediately after
the `?'  indicate what extension is being used, so `(?=foo)' is one
thing (a positive lookahead assertion) and `(?:foo)' is something else
(a non-capturing group containing the subexpression `foo').

Python adds an extension syntax to Perl’s extension syntax.  If the
first character after the question mark is a `P', you know that it’s
an extension that’s specific to Python.  Currently there are two such
extensions: `(?P<name>...)' defines a named group, and `(?P=name)' is a
backreference to a named group.  If future versions of Perl 5 add
similar features using a different syntax, the *Note re: 144. module
will be changed to support the new syntax, while preserving the
Python-specific syntax for compatibility’s sake.

Now that we’ve looked at the general extension syntax, we can return
to the features that simplify working with groups in complex REs. Since
groups are numbered from left to right and a complex expression may use
many groups, it can become difficult to keep track of the correct
numbering.  Modifying such a complex RE is annoying, too: insert a new
group near the beginning and you change the numbers of everything that
follows it.

Sometimes you’ll want to use a group to collect a part of a regular
expression, but aren’t interested in retrieving the group’s
contents. You can make this fact explicit by using a non-capturing
group: `(?:...)', where you can replace the `...' with any other
regular expression.

    >>> m = re.match("([abc])+", "abc")
    >>> m.groups()
    ('c',)
    >>> m = re.match("(?:[abc])+", "abc")
    >>> m.groups()
    ()

Except for the fact that you can’t retrieve the contents of what the
group matched, a non-capturing group behaves exactly the same as a
capturing group; you can put anything inside it, repeat it with a
repetition metacharacter such as `*', and nest it within other groups
(capturing or non-capturing).  `(?:...)' is particularly useful when
modifying an existing pattern, since you can add new groups without
changing how all the other groups are numbered.  It should be mentioned
that there’s no performance difference in searching between capturing
and non-capturing groups; neither form is any faster than the other.

A more significant feature is named groups: instead of referring to
them by numbers, groups can be referenced by a name.

The syntax for a named group is one of the Python-specific extensions:
`(?P<name>...)'.  `name' is, obviously, the name of the group.  Named
groups also behave exactly like capturing groups, and additionally
associate a name with a group.  The *Note match object: a04. methods
that deal with capturing groups all accept either integers that refer
to the group by number or strings that contain the desired group’s
name.  Named groups are still given numbers, so you can retrieve
information about a group in two ways:

    >>> p = re.compile(r'(?P<word>\b\w+\b)')
    >>> m = p.search( '(((( Lots of punctuation )))' )
    >>> m.group('word')
    'Lots'
    >>> m.group(1)
    'Lots'

Named groups are handy because they let you use easily-remembered
names, instead of having to remember numbers.  Here’s an example RE
from the *Note imaplib: f3.  module:

    InternalDate = re.compile(r'INTERNALDATE "'
            r'(?P<day>[ 123][0-9])-(?P<mon>[A-Z][a-z][a-z])-'
            r'(?P<year>[0-9][0-9][0-9][0-9])'
            r' (?P<hour>[0-9][0-9]):(?P<min>[0-9][0-9]):(?P<sec>[0-9][0-9])'
            r' (?P<zonen>[-+])(?P<zoneh>[0-9][0-9])(?P<zonem>[0-9][0-9])'
            r'"')

It’s obviously much easier to retrieve `m.group('zonem')', instead of
having to remember to retrieve group 9.

The syntax for backreferences in an expression such as `(...)\1' refers
to the number of the group.  There’s naturally a variant that uses
the group name instead of the number. This is another Python extension:
`(?P=name)' indicates that the contents of the group called `name'
should again be matched at the current point.  The regular expression
for finding doubled words, `(\b\w+)\s+\1' can also be written as
`(?P<word>\b\w+)\s+(?P=word)':

    >>> p = re.compile(r'(?P<word>\b\w+)\s+(?P=word)')
    >>> p.search('Paris in the the spring').group()
    'the the'


File: python.info,  Node: Lookahead Assertions,  Prev: Non-capturing and Named Groups,  Up: More Pattern Power

10.9.4.4 Lookahead Assertions
.............................

Another zero-width assertion is the lookahead assertion.  Lookahead
assertions are available in both positive and negative form, and  look
like this:

`(?=...)'
     Positive lookahead assertion.  This succeeds if the contained
     regular expression, represented here by `...', successfully
     matches at the current location, and fails otherwise. But, once
     the contained expression has been tried, the matching engine
     doesn’t advance at all; the rest of the pattern is tried right
     where the assertion started.

`(?!...)'
     Negative lookahead assertion.  This is the opposite of the
     positive assertion; it succeeds if the contained expression
     `doesn’t' match at the current position in the string.

To make this concrete, let’s look at a case where a lookahead is
useful.  Consider a simple pattern to match a filename and split it
apart into a base name and an extension, separated by a `.'.  For
example, in `news.rc', `news' is the base name, and `rc' is the
filename’s extension.

The pattern to match this is quite simple:

`.*[.].*$'

Notice that the `.' needs to be treated specially because it’s a
metacharacter; I’ve put it inside a character class.  Also notice the
trailing `$'; this is added to ensure that all the rest of the string
must be included in the extension.  This regular expression matches
`foo.bar' and `autoexec.bat' and `sendmail.cf' and `printers.conf'.

Now, consider complicating the problem a bit; what if you want to match
filenames where the extension is not `bat'? Some incorrect attempts:

`.*[.][^b].*$'  The first attempt above tries to exclude `bat' by
requiring that the first character of the extension is not a `b'.  This
is wrong, because the pattern also doesn’t match `foo.bar'.

`.*[.]([^b]..|.[^a].|..[^t])$'

The expression gets messier when you try to patch up the first solution
by requiring one of the following cases to match: the first character
of the extension isn’t `b'; the second character isn’t `a'; or the
third character isn’t `t'.  This accepts `foo.bar' and rejects
`autoexec.bat', but it requires a three-letter extension and won’t
accept a filename with a two-letter extension such as `sendmail.cf'.
We’ll complicate the pattern again in an effort to fix it.

`.*[.]([^b].?.?|.[^a]?.?|..?[^t]?)$'

In the third attempt, the second and third letters are all made
optional in order to allow matching extensions shorter than three
characters, such as `sendmail.cf'.

The pattern’s getting really complicated now, which makes it hard to
read and understand.  Worse, if the problem changes and you want to
exclude both `bat' and `exe' as extensions, the pattern would get even
more complicated and confusing.

A negative lookahead cuts through all this confusion:

`.*[.](?!bat$)[^.]*$'  The negative lookahead means: if the expression
`bat' doesn’t match at this point, try the rest of the pattern; if
`bat$' does match, the whole pattern will fail.  The trailing `$' is
required to ensure that something like `sample.batch', where the
extension only starts with `bat', will be allowed.  The `[^.]*' makes
sure that the pattern works when there are multiple dots in the
filename.

Excluding another filename extension is now easy; simply add it as an
alternative inside the assertion.  The following pattern excludes
filenames that end in either `bat' or `exe':

`.*[.](?!bat$|exe$)[^.]*$'


File: python.info,  Node: Modifying Strings,  Next: Common Problems,  Prev: More Pattern Power,  Up: Regular Expression HOWTO

10.9.5 Modifying Strings
------------------------

Up to this point, we’ve simply performed searches against a static
string.  Regular expressions are also commonly used to modify strings
in various ways, using the following pattern methods:

Method/Attribute       Purpose
--------------------------------------------------------------------------- 
`split()'              Split the string into a list, splitting it
                       wherever the RE matches
`sub()'                Find all substrings where the RE matches, and
                       replace them with a different string
`subn()'               Does the same thing as `sub()',  but returns the
                       new string and the number of replacements

* Menu:

* Splitting Strings::
* Search and Replace::


File: python.info,  Node: Splitting Strings,  Next: Search and Replace,  Up: Modifying Strings

10.9.5.1 Splitting Strings
..........................

The `split()' method of a pattern splits a string apart wherever the RE
matches, returning a list of the pieces. It’s similar to the
`split()' method of strings but provides much more generality in the
delimiters that you can split by; `split()' only supports splitting by
whitespace or by a fixed string.  As you’d expect, there’s a
module-level *Note re.split(): 247. function, too.

 -- Method: .split (string[, maxsplit=0])
     Split `string' by the matches of the regular expression.  If
     capturing parentheses are used in the RE, then their contents will
     also be returned as part of the resulting list.  If `maxsplit' is
     nonzero, at most `maxsplit' splits are performed.

You can limit the number of splits made, by passing a value for
`maxsplit'.  When `maxsplit' is nonzero, at most `maxsplit' splits will
be made, and the remainder of the string is returned as the final
element of the list.  In the following example, the delimiter is any
sequence of non-alphanumeric characters.

    >>> p = re.compile(r'\W+')
    >>> p.split('This is a test, short and sweet, of split().')
    ['This', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']
    >>> p.split('This is a test, short and sweet, of split().', 3)
    ['This', 'is', 'a', 'test, short and sweet, of split().']

Sometimes you’re not only interested in what the text between
delimiters is, but also need to know what the delimiter was.  If
capturing parentheses are used in the RE, then their values are also
returned as part of the list.  Compare the following calls:

    >>> p = re.compile(r'\W+')
    >>> p2 = re.compile(r'(\W+)')
    >>> p.split('This... is a test.')
    ['This', 'is', 'a', 'test', '']
    >>> p2.split('This... is a test.')
    ['This', '... ', 'is', ' ', 'a', ' ', 'test', '.', '']

The module-level function *Note re.split(): 247. adds the RE to be used
as the first argument, but is otherwise the same.

    >>> re.split('[\W]+', 'Words, words, words.')
    ['Words', 'words', 'words', '']
    >>> re.split('([\W]+)', 'Words, words, words.')
    ['Words', ', ', 'words', ', ', 'words', '.', '']
    >>> re.split('[\W]+', 'Words, words, words.', 1)
    ['Words', 'words, words.']


File: python.info,  Node: Search and Replace,  Prev: Splitting Strings,  Up: Modifying Strings

10.9.5.2 Search and Replace
...........................

Another common task is to find all the matches for a pattern, and
replace them with a different string.  The `sub()' method takes a
replacement value, which can be either a string or a function, and the
string to be processed.

 -- Method: .sub (replacement, string[, count=0])
     Returns the string obtained by replacing the leftmost
     non-overlapping occurrences of the RE in `string' by the
     replacement `replacement'.  If the pattern isn’t found, `string'
     is returned unchanged.

     The optional argument `count' is the maximum number of pattern
     occurrences to be replaced; `count' must be a non-negative
     integer.  The default value of 0 means to replace all occurrences.

Here’s a simple example of using the `sub()' method.  It replaces
colour names with the word `colour':

    >>> p = re.compile('(blue|white|red)')
    >>> p.sub('colour', 'blue socks and red shoes')
    'colour socks and colour shoes'
    >>> p.sub('colour', 'blue socks and red shoes', count=1)
    'colour socks and red shoes'

The `subn()' method does the same work, but returns a 2-tuple
containing the new string value and the number of replacements  that
were performed:

    >>> p = re.compile('(blue|white|red)')
    >>> p.subn('colour', 'blue socks and red shoes')
    ('colour socks and colour shoes', 2)
    >>> p.subn('colour', 'no colours at all')
    ('no colours at all', 0)

Empty matches are replaced only when they’re not adjacent to a
previous match.

    >>> p = re.compile('x*')
    >>> p.sub('-', 'abxd')
    '-a-b-d-'

If `replacement' is a string, any backslash escapes in it are
processed.  That is, `\n' is converted to a single newline character,
`\r' is converted to a carriage return, and so forth. Unknown escapes
such as `\j' are left alone.  Backreferences, such as `\6', are
replaced with the substring matched by the corresponding group in the
RE.  This lets you incorporate portions of the original text in the
resulting replacement string.

This example matches the word `section' followed by a string enclosed in
`{', `}', and changes `section' to `subsection':

    >>> p = re.compile('section{ ( [^}]* ) }', re.VERBOSE)
    >>> p.sub(r'subsection{\1}','section{First} section{second}')
    'subsection{First} subsection{second}'

There’s also a syntax for referring to named groups as defined by the
`(?P<name>...)' syntax.  `\g<name>' will use the substring matched by
the group named `name', and  `\g<number>'  uses the corresponding group
number.  `\g<2>' is therefore equivalent to `\2',  but isn’t
ambiguous in a replacement string such as `\g<2>0'.  (`\20' would be
interpreted as a reference to group 20, not a reference to group 2
followed by the literal character `'0''.)  The following substitutions
are all equivalent, but use all three variations of the replacement
string.

    >>> p = re.compile('section{ (?P<name> [^}]* ) }', re.VERBOSE)
    >>> p.sub(r'subsection{\1}','section{First}')
    'subsection{First}'
    >>> p.sub(r'subsection{\g<1>}','section{First}')
    'subsection{First}'
    >>> p.sub(r'subsection{\g<name>}','section{First}')
    'subsection{First}'

`replacement' can also be a function, which gives you even more
control.  If `replacement' is a function, the function is called for
every non-overlapping occurrence of `pattern'.  On each call, the
function is passed a *Note match object: a04. argument for the match
and can use this information to compute the desired replacement string
and return it.

In the following example, the replacement function translates decimals
into hexadecimal:

    >>> def hexrepl(match):
    ...     "Return the hex string for a decimal number"
    ...     value = int(match.group())
    ...     return hex(value)
    ...
    >>> p = re.compile(r'\d+')
    >>> p.sub(hexrepl, 'Call 65490 for printing, 49152 for user code.')
    'Call 0xffd2 for printing, 0xc000 for user code.'

When using the module-level *Note re.sub(): 248. function, the pattern
is passed as the first argument.  The pattern may be provided as an
object or as a string; if you need to specify regular expression flags,
you must either use a pattern object as the first parameter, or use
embedded modifiers in the pattern string, e.g. `sub("(?i)b+", "x",
"bbbb BBBB")' returns `'x x''.


File: python.info,  Node: Common Problems,  Next: Feedback,  Prev: Modifying Strings,  Up: Regular Expression HOWTO

10.9.6 Common Problems
----------------------

Regular expressions are a powerful tool for some applications, but in
some ways their behaviour isn’t intuitive and at times they don’t
behave the way you may expect them to.  This section will point out
some of the most common pitfalls.

* Menu:

* Use String Methods::
* match() versus search(): match versus search.
* Greedy versus Non-Greedy::
* Using re.VERBOSE: Using re VERBOSE.


File: python.info,  Node: Use String Methods,  Next: match versus search,  Up: Common Problems

10.9.6.1 Use String Methods
...........................

Sometimes using the *Note re: 144. module is a mistake.  If you’re
matching a fixed string, or a single character class, and you’re not
using any *Note re: 144. features such as the `IGNORECASE' flag, then
the full power of regular expressions may not be required. Strings have
several methods for performing operations with fixed strings and
they’re usually much faster, because the implementation is a single
small C loop that’s been optimized for the purpose, instead of the
large, more generalized regular expression engine.

One example might be replacing a single fixed string with another one;
for example, you might replace `word' with `deed'.  `re.sub()' seems
like the function to use for this, but consider the `replace()' method.
Note that `replace()' will also replace `word' inside words, turning
`swordfish' into `sdeedfish', but the  naive RE `word' would have done
that, too.  (To avoid performing the substitution on parts of words,
the pattern would have to be `\bword\b', in order to require that
`word' have a word boundary on either side.  This takes the job beyond
`replace()'’s abilities.)

Another common task is deleting every occurrence of a single character
from a string or replacing it with another single character.  You might
do this with something like `re.sub('\n', ' ', S)', but `translate()'
is capable of doing both tasks and will be faster than any regular
expression operation can be.

In short, before turning to the *Note re: 144. module, consider whether
your problem can be solved with a faster and simpler string method.


File: python.info,  Node: match versus search,  Next: Greedy versus Non-Greedy,  Prev: Use String Methods,  Up: Common Problems

10.9.6.2 match() versus search()
................................

The `match()' function only checks if the RE matches at the beginning
of the string while `search()' will scan forward through the string for
a match.  It’s important to keep this distinction in mind.  Remember,
`match()' will only report a successful match which will start at 0;
if the match wouldn’t start at zero,  `match()' will `not' report it.

    >>> print re.match('super', 'superstition').span()
    (0, 5)
    >>> print re.match('super', 'insuperable')
    None

On the other hand, `search()' will scan forward through the string,
reporting the first match it finds.

    >>> print re.search('super', 'superstition').span()
    (0, 5)
    >>> print re.search('super', 'insuperable').span()
    (2, 7)

Sometimes you’ll be tempted to keep using *Note re.match(): 9ec, and
just add `.*' to the front of your RE.  Resist this temptation and use
*Note re.search(): 9eb.  instead.  The regular expression compiler does
some analysis of REs in order to speed up the process of looking for a
match.  One such analysis figures out what the first character of a
match must be; for example, a pattern starting with `Crow' must match
starting with a `'C''.  The analysis lets the engine quickly scan
through the string looking for the starting character, only trying the
full match if a `'C'' is found.

Adding `.*' defeats this optimization, requiring scanning to the end of
the string and then backtracking to find a match for the rest of the
RE.  Use *Note re.search(): 9eb. instead.


File: python.info,  Node: Greedy versus Non-Greedy,  Next: Using re VERBOSE,  Prev: match versus search,  Up: Common Problems

10.9.6.3 Greedy versus Non-Greedy
.................................

When repeating a regular expression, as in `a*', the resulting action
is to consume as much of the pattern as possible.  This fact often
bites you when you’re trying to match a pair of balanced delimiters,
such as the angle brackets surrounding an HTML tag.  The naive pattern
for matching a single HTML tag doesn’t work because of the greedy
nature of `.*'.

    >>> s = '<html><head><title>Title</title>'
    >>> len(s)
    32
    >>> print re.match('<.*>', s).span()
    (0, 32)
    >>> print re.match('<.*>', s).group()
    <html><head><title>Title</title>

The RE matches the `'<'' in `<html>', and the `.*' consumes the rest of
the string.  There’s still more left in the RE, though, and the `>'
can’t match at the end of the string, so the regular expression
engine has to backtrack character by character until it finds a match
for the `>'.   The final match extends from the `'<'' in `<html>' to
the `'>'' in `</title>', which isn’t what you want.

In this case, the solution is to use the non-greedy qualifiers `*?',
`+?', `??', or `{m,n}?', which match as `little' text as possible.  In
the above example, the `'>'' is tried immediately after the first `'<''
matches, and when it fails, the engine advances a character at a time,
retrying the `'>'' at every step.  This produces just the right result:

    >>> print re.match('<.*?>', s).group()
    <html>

(Note that parsing HTML or XML with regular expressions is painful.
Quick-and-dirty patterns will handle common cases, but HTML and XML
have special cases that will break the obvious regular expression; by
the time you’ve written a regular expression that handles all of the
possible cases, the patterns will be `very' complicated.  Use an HTML
or XML parser module for such tasks.)


File: python.info,  Node: Using re VERBOSE,  Prev: Greedy versus Non-Greedy,  Up: Common Problems

10.9.6.4 Using re.VERBOSE
.........................

By now you’ve probably noticed that regular expressions are a very
compact notation, but they’re not terribly readable.  REs of moderate
complexity can become lengthy collections of backslashes, parentheses,
and metacharacters, making them difficult to read and understand.

For such REs, specifying the `re.VERBOSE' flag when compiling the
regular expression can be helpful, because it allows you to format the
regular expression more clearly.

The `re.VERBOSE' flag has several effects.  Whitespace in the regular
expression that `isn’t' inside a character class is ignored.  This
means that an expression such as `dog | cat' is equivalent to the less
readable `dog|cat', but `[a b]' will still match the characters `'a'',
`'b'', or a space.  In addition, you can also put comments inside a RE;
comments extend from a `#' character to the next newline.  When used
with triple-quoted strings, this enables REs to be formatted more
neatly:

    pat = re.compile(r"""
     \s*                 # Skip leading whitespace
     (?P<header>[^:]+)   # Header name
     \s* :               # Whitespace, and a colon
     (?P<value>.*?)      # The header's value -- *? used to
                         # lose the following trailing whitespace
     \s*$                # Trailing whitespace to end-of-line
    """, re.VERBOSE)

This is far more readable than:

    pat = re.compile(r"\s*(?P<header>[^:]+)\s*:(?P<value>.*?)\s*$")


File: python.info,  Node: Feedback,  Prev: Common Problems,  Up: Regular Expression HOWTO

10.9.7 Feedback
---------------

Regular expressions are a complicated topic.  Did this document help you
understand them?  Were there parts that were unclear, or Problems you
encountered that weren’t covered here?  If so, please send
suggestions for improvements to the author.

The most complete book on regular expressions is almost certainly
Jeffrey Friedl’s Mastering Regular Expressions, published by
O’Reilly.  Unfortunately, it exclusively concentrates on Perl and
Java’s flavours of regular expressions, and doesn’t contain any
Python material at all, so it won’t be useful as a reference for
programming in Python.  (The first edition covered Python’s
now-removed `regex' module, which won’t help you much.)  Consider
checking it out from your library.


File: python.info,  Node: Socket Programming HOWTO,  Next: Sorting HOW TO,  Prev: Regular Expression HOWTO,  Up: Python HOWTOs

10.10 Socket Programming HOWTO
==============================

Author: Gordon McMillan

Abstract
........

Sockets are used nearly everywhere, but are one of the most severely
misunderstood technologies around. This is a 10,000 foot overview of
sockets.  It’s not really a tutorial - you’ll still have work to do
in getting things operational. It doesn’t cover the fine points (and
there are a lot of them), but I hope it will give you enough background
to begin using them decently.

* Menu:

* Sockets::
* Creating a Socket::
* Using a Socket::
* Disconnecting::
* Non-blocking Sockets::


File: python.info,  Node: Sockets,  Next: Creating a Socket,  Up: Socket Programming HOWTO

10.10.1 Sockets
---------------

I’m only going to talk about INET sockets, but they account for at
least 99% of the sockets in use. And I’ll only talk about STREAM
sockets - unless you really know what you’re doing (in which case
this HOWTO isn’t for you!), you’ll get better behavior and
performance from a STREAM socket than anything else. I will try to
clear up the mystery of what a socket is, as well as some hints on how
to work with blocking and non-blocking sockets. But I’ll start by
talking about blocking sockets. You’ll need to know how they work
before dealing with non-blocking sockets.

Part of the trouble with understanding these things is that
“socket” can mean a number of subtly different things, depending on
context. So first, let’s make a distinction between a “client”
socket - an endpoint of a conversation, and a “server” socket,
which is more like a switchboard operator. The client application (your
browser, for example) uses “client” sockets exclusively; the web
server it’s talking to uses both “server” sockets and
“client” sockets.

* Menu:

* History::


File: python.info,  Node: History,  Up: Sockets

10.10.1.1 History
.................

Of the various forms of IPC (Inter Process Communication), sockets are
by far the most popular.  On any given platform, there are likely to be
other forms of IPC that are faster, but for cross-platform
communication, sockets are about the only game in town.

They were invented in Berkeley as part of the BSD flavor of Unix. They
spread like wildfire with the Internet. With good reason — the
combination of sockets with INET makes talking to arbitrary machines
around the world unbelievably easy (at least compared to other schemes).


File: python.info,  Node: Creating a Socket,  Next: Using a Socket,  Prev: Sockets,  Up: Socket Programming HOWTO

10.10.2 Creating a Socket
-------------------------

Roughly speaking, when you clicked on the link that brought you to this
page, your browser did something like the following:

    #create an INET, STREAMing socket
    s = socket.socket(
        socket.AF_INET, socket.SOCK_STREAM)
    #now connect to the web server on port 80
    # - the normal http port
    s.connect(("www.mcmillan-inc.com", 80))

When the `connect' completes, the socket `s' can be used to send in a
request for the text of the page. The same socket will read the reply,
and then be destroyed. That’s right, destroyed. Client sockets are
normally only used for one exchange (or a small set of sequential
exchanges).

What happens in the web server is a bit more complex. First, the web
server creates a “server socket”:

    #create an INET, STREAMing socket
    serversocket = socket.socket(
        socket.AF_INET, socket.SOCK_STREAM)
    #bind the socket to a public host,
    # and a well-known port
    serversocket.bind((socket.gethostname(), 80))
    #become a server socket
    serversocket.listen(5)

A couple things to notice: we used `socket.gethostname()' so that the
socket would be visible to the outside world.  If we had used
`s.bind(('localhost', 80))' or `s.bind(('127.0.0.1', 80))' we would
still have a “server” socket, but one that was only visible within
the same machine.  `s.bind(('', 80))' specifies that the socket is
reachable by any address the machine happens to have.

A second thing to note: low number ports are usually reserved for
“well known” services (HTTP, SNMP etc). If you’re playing around,
use a nice high number (4 digits).

Finally, the argument to `listen' tells the socket library that we want
it to queue up as many as 5 connect requests (the normal max) before
refusing outside connections. If the rest of the code is written
properly, that should be plenty.

Now that we have a “server” socket, listening on port 80, we can
enter the mainloop of the web server:

    while 1:
        #accept connections from outside
        (clientsocket, address) = serversocket.accept()
        #now do something with the clientsocket
        #in this case, we'll pretend this is a threaded server
        ct = client_thread(clientsocket)
        ct.run()

There’s actually 3 general ways in which this loop could work -
dispatching a thread to handle `clientsocket', create a new process to
handle `clientsocket', or restructure this app to use non-blocking
sockets, and multiplex between our “server” socket and any active
`clientsocket's using `select'. More about that later. The important
thing to understand now is this: this is `all' a “server” socket
does. It doesn’t send any data. It doesn’t receive any data. It
just produces “client” sockets. Each `clientsocket' is created in
response to some `other' “client” socket doing a `connect()' to the
host and port we’re bound to. As soon as we’ve created that
`clientsocket', we go back to listening for more connections. The two
“clients” are free to chat it up - they are using some dynamically
allocated port which will be recycled when the conversation ends.

* Menu:

* IPC::

